[
  {
    "objectID": "dev/short_bio.html",
    "href": "dev/short_bio.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "i! I’m Kundan Kumar, a Ph.D. candidate and researcher focused on building intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work bridges deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), safe and explainable AI, and computer vision, with real-world applications in smart grids, autonomous vehicles, and critical infrastructure.\nMy Ph.D. research centers on physics-informed and safety-critical DRL frameworks that embed domain knowledge, safety constraints, and uncertainty into the learning process—enabling agents to make robust and interpretable decisions in dynamic, complex environments. My research within DRL focuses on techniques such as transfer learning, uncertainty quantification, and adversarial resilience to improve generalization, safety, and reliability across diverse tasks and environments.\nI also develop LLM-integrated simulation frameworks for robotics and autonomous systems, combining vision-based perception, trajectory planning, and natural language reasoning to support high-level control and human-AI collaboration.\nBeyond research, I enjoy sharing my insights through educational content on Substack and YouTube. Outside of work, I love cooking and Ice skating 🛼."
  },
  {
    "objectID": "teaching/coms3620/index.html",
    "href": "teaching/coms3620/index.html",
    "title": "COMS 3620",
    "section": "",
    "text": "COMS 3620 is a project-based course focused on object-oriented requirements analysis and system design. Students learn to apply Unified Modeling Language (UML) and design patterns to build robust and evolvable software systems. The course emphasizes teamwork, iterative development, and the communication of design decisions through technical documentation and presentations.\n\n\n\n\nAs a Teaching Assistant for COMS 3620, I provided critical support to students in both design and implementation phases of large-scale object-oriented projects. My contributions included:\n\nUML & Design Mentorship: Assisted students in modeling system requirements using UML (use case diagrams, class diagrams, sequence diagrams).\nProject Guidance: Advised student teams during the full development cycle—from requirement analysis and domain modeling to implementation and testing.\nDesign Critique & Feedback: Evaluated and gave feedback on design homeworks, helping students strengthen their reasoning with design principles and patterns.\nTechnical Assistance: Supported Java implementation of OO designs, clarifying concepts like inheritance, polymorphism, abstraction, and class hierarchies.\nPresentation Coaching: Helped teams prepare and deliver clear, well-structured final project presentations and reports.\n\nThis role enhanced my skills in software architecture, peer instruction, and coaching students on both technical correctness and design clarity.\n\n\n\n\nUpon completing the course, students are expected to:\n\nAnalyze software system requirements and model problem domains\n\nDesign robust object-oriented systems using design principles and heuristics\n\nApply and justify the use of design patterns\n\nProduce UML-based design documentation\n\nImplement object-oriented solutions in Java using abstraction, inheritance, and polymorphism\n\nPresent and explain team design decisions in written and oral formats\n\nCollaborate effectively in team-based development projects\n\n\n\n\n\n\nProcedural & Data Abstraction\n\nModularity, Objects, State\n\nUnified Modeling Language (UML)\n\nDesign Principles & Patterns\n\nMetalinguistic Abstraction\n\nProject Presentations & Evaluation\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms3620/index.html#coms-3620-object-oriented-analysis-and-design",
    "href": "teaching/coms3620/index.html#coms-3620-object-oriented-analysis-and-design",
    "title": "COMS 3620",
    "section": "",
    "text": "COMS 3620 is a project-based course focused on object-oriented requirements analysis and system design. Students learn to apply Unified Modeling Language (UML) and design patterns to build robust and evolvable software systems. The course emphasizes teamwork, iterative development, and the communication of design decisions through technical documentation and presentations.\n\n\n\n\nAs a Teaching Assistant for COMS 3620, I provided critical support to students in both design and implementation phases of large-scale object-oriented projects. My contributions included:\n\nUML & Design Mentorship: Assisted students in modeling system requirements using UML (use case diagrams, class diagrams, sequence diagrams).\nProject Guidance: Advised student teams during the full development cycle—from requirement analysis and domain modeling to implementation and testing.\nDesign Critique & Feedback: Evaluated and gave feedback on design homeworks, helping students strengthen their reasoning with design principles and patterns.\nTechnical Assistance: Supported Java implementation of OO designs, clarifying concepts like inheritance, polymorphism, abstraction, and class hierarchies.\nPresentation Coaching: Helped teams prepare and deliver clear, well-structured final project presentations and reports.\n\nThis role enhanced my skills in software architecture, peer instruction, and coaching students on both technical correctness and design clarity.\n\n\n\n\nUpon completing the course, students are expected to:\n\nAnalyze software system requirements and model problem domains\n\nDesign robust object-oriented systems using design principles and heuristics\n\nApply and justify the use of design patterns\n\nProduce UML-based design documentation\n\nImplement object-oriented solutions in Java using abstraction, inheritance, and polymorphism\n\nPresent and explain team design decisions in written and oral formats\n\nCollaborate effectively in team-based development projects\n\n\n\n\n\n\nProcedural & Data Abstraction\n\nModularity, Objects, State\n\nUnified Modeling Language (UML)\n\nDesign Principles & Patterns\n\nMetalinguistic Abstraction\n\nProject Presentations & Evaluation\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms3090/index.html",
    "href": "teaching/coms3090/index.html",
    "title": "COMS 3090",
    "section": "",
    "text": "This course provides a practical introduction to software engineering principles including process models, requirements engineering, system design, testing, and project management. It emphasizes collaborative software development through a semester-long group project, focusing on teamwork, accountability, and real-world delivery practices.\n\n\n\n\nStudents completing this course will:\n\nGain practical exposure to the software development lifecycle, from requirements to implementation and testing.\nLearn to apply project management techniques including cost estimation, scheduling, and risk analysis.\nCollaborate in teams to design, develop, and deliver a working software product.\nUse software engineering artifacts and tools such as source control, UML, SQL schemas, and versioning systems.\nUnderstand software ethics, legal responsibilities, and industry best practices.\nPractice client/server programming concepts including socket communication and APIs.\n\n\n\n\n\n\nSoftware Process Models and Lifecycle\n\nRequirements Analysis & Specification\n\nArchitecture and System Design\n\nProject Scheduling, Risk Management, and Metrics\n\nTesting, Inspections, and Code Reviews\n\nSoftware Configuration & Source Control\n\nDatabase Schema Design and SQL\n\nObject-Oriented Concepts and Modeling\n\nNetworking & Client-Server Programming (HTTP, DNS, Sockets)\n\nDNS, IP, URI\nIntroduction to the client/server model\nHttp\nSocket APIs\nClient/server APIs\n\nProfessional Ethics in Computing\n\n\n\n\n\nAs a Teaching Assistant for COMS 3090, I supported over 150 students in mastering software development practices through lectures, labs, and team projects. My responsibilities included:\n\nMentoring Project Teams: Guided multiple student groups through requirements gathering, iterative design, and implementation milestones for their semester-long projects.\nLab Instruction: Facilitated labs covering Git, object-oriented modeling, system architecture, and backend programming fundamentals.\nTechnical Support: Helped students with debugging, API integrations, testing techniques, and version control during weekly office hours.\nEvaluation & Feedback: Assisted in grading project deliverables, presentations, and code quality, providing constructive feedback for improvement.\nProcess Coaching: Reinforced agile principles, milestone planning, documentation quality, and collaborative communication.\n\nThis role allowed me to blend technical mentorship with project coordination, while reinforcing industry practices and promoting engineering discipline among future software professionals.\n\nCourse Catalog | Email"
  },
  {
    "objectID": "teaching/coms3090/index.html#coms-3090-software-development-practices",
    "href": "teaching/coms3090/index.html#coms-3090-software-development-practices",
    "title": "COMS 3090",
    "section": "",
    "text": "This course provides a practical introduction to software engineering principles including process models, requirements engineering, system design, testing, and project management. It emphasizes collaborative software development through a semester-long group project, focusing on teamwork, accountability, and real-world delivery practices.\n\n\n\n\nStudents completing this course will:\n\nGain practical exposure to the software development lifecycle, from requirements to implementation and testing.\nLearn to apply project management techniques including cost estimation, scheduling, and risk analysis.\nCollaborate in teams to design, develop, and deliver a working software product.\nUse software engineering artifacts and tools such as source control, UML, SQL schemas, and versioning systems.\nUnderstand software ethics, legal responsibilities, and industry best practices.\nPractice client/server programming concepts including socket communication and APIs.\n\n\n\n\n\n\nSoftware Process Models and Lifecycle\n\nRequirements Analysis & Specification\n\nArchitecture and System Design\n\nProject Scheduling, Risk Management, and Metrics\n\nTesting, Inspections, and Code Reviews\n\nSoftware Configuration & Source Control\n\nDatabase Schema Design and SQL\n\nObject-Oriented Concepts and Modeling\n\nNetworking & Client-Server Programming (HTTP, DNS, Sockets)\n\nDNS, IP, URI\nIntroduction to the client/server model\nHttp\nSocket APIs\nClient/server APIs\n\nProfessional Ethics in Computing\n\n\n\n\n\nAs a Teaching Assistant for COMS 3090, I supported over 150 students in mastering software development practices through lectures, labs, and team projects. My responsibilities included:\n\nMentoring Project Teams: Guided multiple student groups through requirements gathering, iterative design, and implementation milestones for their semester-long projects.\nLab Instruction: Facilitated labs covering Git, object-oriented modeling, system architecture, and backend programming fundamentals.\nTechnical Support: Helped students with debugging, API integrations, testing techniques, and version control during weekly office hours.\nEvaluation & Feedback: Assisted in grading project deliverables, presentations, and code quality, providing constructive feedback for improvement.\nProcess Coaching: Reinforced agile principles, milestone planning, documentation quality, and collaborative communication.\n\nThis role allowed me to blend technical mentorship with project coordination, while reinforcing industry practices and promoting engineering discipline among future software professionals.\n\nCourse Catalog | Email"
  },
  {
    "objectID": "teaching/cheatsheet_pkgdev/index.html",
    "href": "teaching/cheatsheet_pkgdev/index.html",
    "title": "Pkg Dev at CSIDS",
    "section": "",
    "text": "Rstat"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "As a Teaching Assistant at Iowa State University, I am dedicated to creating an inclusive and dynamic learning environment that enables students to excel in computer science. My teaching approach emphasizes practical application of theoretical concepts, encouraging students to develop hands-on experience in software development, user interface design, and database management. I believe in preparing students for real-world challenges in the technology industry through project-based learning and personalized support.\nCourses I have served as a Teaching Assistant or Mentor in:"
  },
  {
    "objectID": "rpkg/nowcast/index.html",
    "href": "rpkg/nowcast/index.html",
    "title": "nowcast",
    "section": "",
    "text": "https://github.com/csids/nowcast"
  },
  {
    "objectID": "rpkg/ggehr/index.html",
    "href": "rpkg/ggehr/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr"
  },
  {
    "objectID": "rpkg/noreden/index.html",
    "href": "rpkg/noreden/index.html",
    "title": "noreden",
    "section": "",
    "text": "This package provides tools to facilitate sustainable diet discovery.\nCode"
  },
  {
    "objectID": "rpkg/csalert/index.html",
    "href": "rpkg/csalert/index.html",
    "title": "csalert",
    "section": "",
    "text": "https://github.com/csids/csalert"
  },
  {
    "objectID": "rpkg/rpkg.html",
    "href": "rpkg/rpkg.html",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg.html#research-vision",
    "href": "rpkg/rpkg.html#research-vision",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg.html#research-areas",
    "href": "rpkg/rpkg.html#research-areas",
    "title": "Research",
    "section": "Research Areas",
    "text": "Research Areas\n\n🔬 Safe Reinforcement Learning\n\nPhysics-Informed RL with embedded domain constraints\nAdversarial robustness & uncertainty quantification\nSafe exploration and constrained policy optimization\n\n\n\n🎯 Transfer Learning & Meta-Learning\n\nPolicy transfer across varied network topologies\nSimulation-to-real transfer in safety-critical domains\nCross-domain generalization across environments\n\n\n\n🔎 Vision-Based Simulation\n\nPerception and control using CARLA simulator\nComputer vision for object detection, scene understanding\nVision-based control pipelines for autonomous systems\n\n\n\n🔗 LLM-Augmented Control Reasoning\n\nIntegrating large language models for high-level decision-making\nContext-aware planning and explainable reasoning\nHuman-AI collaboration in control and robotics\n\n\n\n⚡ Core Application Domains\n\nSmart Energy Systems (Volt-VAR Control, Grid Resilience)\nAutonomous Vehicles & Robotics\nAI Safety in Cyber-Physical Infrastructure"
  },
  {
    "objectID": "rpkg/rpkg.html#publications",
    "href": "rpkg/rpkg.html#publications",
    "title": "Research",
    "section": "📚 Publications",
    "text": "📚 Publications\n\n📝 Journal Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster"
  },
  {
    "objectID": "rpkg/medicode/index.html",
    "href": "rpkg/medicode/index.html",
    "title": "medicode",
    "section": "",
    "text": "This package provides metadata and tools for medical classification and clinical coding.\nCode\nIt is especially useful for English and Norwegian (Bokmål) languages.\nPlanned content:\n\nICD-10\nICPC-2\nEuropean shortlist for Cause of Death"
  },
  {
    "objectID": "rpkg/cstime/index.html",
    "href": "rpkg/cstime/index.html",
    "title": "cstime",
    "section": "",
    "text": "cstime provides convenient and consistent conversion between\n\nisoyear\nisoweek\ncalyear\nseason week (used in influenza surveillance)\n\nCode"
  },
  {
    "objectID": "blog/technotes_20230205_clinreport_part1/index.html",
    "href": "blog/technotes_20230205_clinreport_part1/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "href": "blog/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Introduction to clinical trial",
    "text": "Introduction to clinical trial\nClinical trial: aim to demonstrate that drug is safe and effective (safety, efficacy)\n\nphase 1: 10-20 people, focus on safety (healthy volunteers)\nphase 2: 100, study of side effects, determine best dose\nphase 3: 1000, demonstrate drug efficacy, fuller safety profile (common across multiple regions, ethnicities)\n\ncollecting data from different hospitals, hence important to ensure standards are being followed\n\nevidence must be submitted to health authorities (FDA, EMA European medicines agency)\nhealth authorities determine whether the drug is submitted to market\n\nSubmit the analysis plan in advance"
  },
  {
    "objectID": "blog/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "href": "blog/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Why share data",
    "text": "Why share data\n\nregulatory requirements\nscientific community interest\ncompany internal research interest\nmarketing materials\n\n\nData and results sharing\n\nRegulatory req (e.g. EMA require sharing clinical trial results to gain marketing authorization for pharma products, FDA require sharing data)\nscientific community (peer review check accuracy, perform additional analyses, derive new hypothesis)\nCDISC standards\n\nCDASH (clinical data acquisition standards harmonization)\nSDTM (study data tabulation model)\n\nformat for ‘raw’ data, define datasets, structures, contents, variable attributes\n\nSEND (standard for exchange of non clinical data)\nADaM (analysis data model)\n\ndata format for data processed for analysis (e.g. converted, imputed, derived)\n\n\nDictionary\n\ne.g. nose congestion, stuffy nose, … need to be standardized\nMedDRA: standard dictionary for medical conditions, events and procedures\nWHO drug dictionary (for pharma agents)\n\nSAP statistical analysis plan\n\nbased on study protocol, focus on statistical methodology, is regulated\n\nProgramming specification\n\nbased on SAP, provides additional details on datasets and tables, listing and figures (TLFs) required for statistical analysis. focus on programming details. Not regulated\n\n\n\n\nQuality assurance\n\nGood clinical practice (GCP), issued by ICH\npurpose: prevent mistakes, reduce inefficiencies/waste in a process, increase reliability/trustworthiness of the product of a process\nClinical monitoring: performed by a clinical research assistant (CRA) at investigator sites, checks that study protocols are executed as intended, and site processes result in accurate data capture. Focus on trial subjects’ safety. Traditional goal: 100% source data verification\nData quality checks (more relevant for data scientists). Checks data for technical conformance, and data plausibility. Focus on data quality. Traditional goal: 100% accurate and format compliant data\nCode review\nDouble programming\n\n\n\nData access restrictions\n\nreasons\n\ndata collected is very sensitive (health data), need data protection\nclinical trial data is a key asset and revenue predictor for pharma companies, high confidentiality levels\nscientific validity, data is ideally double blinded, no-one should know whether a subjecttreatment is as long as the data is still being collected\n\npseudonymization: data de-identification\n\nuse pseudonym (ID), link is recorded to allow re-identification\n\nanonymization: limit the risk of re-identification\n\nremove variables, remove values, replace more precise values with more general categories, replace personal identifiers with random identifiers\n\nFSP, CRO (out-sourcing), personnels require data access at different levels\nUnblinding"
  },
  {
    "objectID": "blog/readnotes_20240218_open_source/index.html",
    "href": "blog/readnotes_20240218_open_source/index.html",
    "title": "Working in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal",
    "section": "",
    "text": "Github as a platform\n\nOn contribution\nNearly half of all contributors only contributed once; which accounted for less than 2% of total commits.\nThe pattern that one or a few developers do most of the work, followed by many casual contributors and even more passive users is the norm, not exception in open source.\nOn casual contributors: they primarily see themselves as users of the project, rather than a part of a contributor community.\nChallenge for maintainers: not how to get more contributors, but how to manage high volume of frequent, low-touch interactions (directing air traffic)\n\nGithub’s open source developers have more in common with solo creators on Twitter, Instagram, YouTube or Twitch.\n\nComparing early internet and social platform nowadays: the early online communities have mailing lists, online forums, membership groups, operated like villages that have their own culture, history and norms. Nowadays creators have much bigger potential audience but the relationship is one-sided, and can be overwhelming.\n\n\nOn free software and hacker\n“Free” means you are able to do what you want with the software, rather than the cost. Libre rather than gratis. At least at the beginning.\nBravado, showmanship, mischievousness, deep mistrust of authority. This culture in the 1980s and 90s was closely linked to the early open source software.\n“Bazaar”: highly participatory, versus “Cathedral”: restricted to a smaller group\nToday’s developer hardly even notice “open source” as a concept anymore, they just want to write and publish their code. They prioritize convenience over freedom or openness.\n\n\nOn licensing\nThe widespread use of permissive licensing is popularized by GH.\nCopyleft licensing (e.g. GNU General Public License GPL) is not commercial friendly as it requires companies to license their software that depend on open source GPL software to have the same license. However GPL gives developers more control over how others use their code in the long run.\n\nAs with any other online content today, sharing is the default.\n\n\n\n\nThe structure of an open source software\n\nOn how projects evolve\nCreate -&gt; Promote and distribute -&gt; Grow\nProjects are promoted like a founder would promote a startup: share on the relevant channels online, give talks at conference and meetups, encourage others to write and talk about it\nA sign that the software is used widely: when the maintainer starts doing more non-code (triage issues, review pull requests) rather than code work.\n\n\nContributor and users\nDepends on technical scope (whether there is much to do), support required (code and admin work), ease of participation (whether on Github) and user adoption (potential contributor base).\nFour types of projects\n\nhigh user growth, high contributor growth: federations. Rare, impactful, the ‘ideal’ of open source project. Roughly 3% of open source projects. Examples: Rust, Node.js, Linux\nhigh user growth, low contributor growth: stadiums: powered by one or a few developers. Centralized.\nlow user growth, high contributor growth: club. Similar to meetup or hobby groups, do not have a wide reach but are loved and built by enthusiasts.\nlow user growth, low contributor growth: toys. Personal project, isn’t trying to grow its user base. Projects on Github with less than 10 stars. Authors do not expect to receive contributions nor do they care about whether people are watching.\n\nDecentralized communities (clubs and federations) have the potential for high user growth - recruit new contributors, reduce contribution friction.\nCentralized communities (stadium) depends on the creators to manage user demand - automation, elimination of noise\n\n\n\nRoles, incentives and relationships\n\nFirms or communities\nFirms (companies, organizations): centralized resources; from a coordination standpoint, managing resources would be more efficient within the same organization - which does not explain why open source developers make software together without formal contracts and financial compensation.\n\n\nThe commons and peer production\nTragedy of the commons: resources depleted by people acting in their own self-interest rather than in the collective interest.\n(One of the 8 design principles by Ostrom on) successful commons:\n\nThose who are affecteed by the rules can participate in modifying them.\n\nStrong sense of group identity maks rules, dispute resolution more meaningful.\nCoordination cost is lower when self-organized based on who wants to do the work most, anyone can do the advertised work and volunteer.\nIn contrast, in companies - solicit, evaluate, hire, manage employees; only employees can do the work limited by their job functions.\nPeople collaborating online for no obvious reason beyond personal satisfaction (intrinsic motivation)\nModular and granular tasks: how tasks are organized, and how big each task is.\nLow coordination costs: quality control over thee modules, integrate the contributions into the finished product\n\n\nContribution beyond code\nSome users do not consider them a contributor, but do actually contribute by education, spreading the word, support (forum), bug reports and more.\nThese active users are similar to contributors but operate independently from project’s contributor community."
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html",
    "href": "blog/blog_20241105_positconf2024/index.html",
    "title": "Personal Highlights: Positconf 2024",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html#quarto",
    "href": "blog/blog_20241105_positconf2024/index.html#quarto",
    "title": "Personal Highlights: Positconf 2024",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html#python",
    "href": "blog/blog_20241105_positconf2024/index.html#python",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Python",
    "text": "Python\nEmily Riederer: Python Rgonomics\nPython alternatives to R. Worth rewatching!"
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html#teaching-and-education",
    "href": "blog/blog_20241105_positconf2024/index.html#teaching-and-education",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Teaching and education",
    "text": "Teaching and education\nAndrew Gard: Teaching and learning data science in the era of AI\nStudents don’t know enough to be able to edit the prompt to reach a sensible code chunk, AI guessed and guessed wrong. We should not expect AI to guess information that we do not provide!\nStudents should still learn to code, and teachers should ask better questions - instead of asking for the final result (create a bar plot), ask students to critically think: why doesn’t the AI-generated code work? what information is missing? how do you improve the prompt?\nJames Wade: Posit Academy in the Age of Generative AI - Lessons from the Frontlines\nchattr, gptstudio, github copilot\nPosit Academy learners (over half) give AI code assistants 2 star rating or less\nRewarding, high-growth period. Threshold concepts: once understood, transforms your perception and approach of a discipline, and these must be encountered not told.\nTC in DS:\n\ntidy data enables efficient analysis\nmodular code enhances re-usablity and clarity\nvisualization as a tool for exploration and communication\n\nHow to incorporate AI code assistants (in DS class)\n\nearly stage: explain this code piece by piece\nmid stage: add a roxygen skeleton to my code\nlate stage: try code assistants in the IDE\n\nTC for code assistants:\n\ndrive faster but don’t forget to steer\nprompting matters, learning how to use these tools is a skill"
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html#statistics",
    "href": "blog/blog_20241105_positconf2024/index.html#statistics",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Statistics",
    "text": "Statistics\nHannah Frick: tidymodels for time-to-event data\nMax Kuhn: Evaluating time-to-event models is hard\nDemetri Pananos - Making sense of marginal effects"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html",
    "href": "blog/technotes_20230301_clinreport_part4/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Open source packages",
    "text": "Open source packages\nExmample:\n\nsurvival: 8 developers, &gt;18 years\nadmiral: 25 developers, &gt;1 year\ntern: 77 developers, 5 years\nrtables: 21 developers, 4 years\n\nEngagement across these packages is different, some receive more issues and comments, some receive more code contributions.\nStale: stable? abandoned?\nContribution is highly skewed, a few contributors write the majority of the code.\nR package life cycles (indicative, not guaranteed)\n\nexperimental (ready to use?)\nstable (safe to use?)\ndeprecated, no longer maintained\nsuperseded, something better exists\n&lt;1.0: big changes likely; &gt;=v1.0: is it safe to use?"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Risk mitigation for R packages",
    "text": "Risk mitigation for R packages\nCombine external and internal packages (CI/CD release)\n-&gt; automated package data collection\n-&gt; automated quality checks: if not pass, assess\n-&gt; package repo integration tests\n-&gt; publish to package repo, generate package validation report"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Assess external packages for statistical methods",
    "text": "Assess external packages for statistical methods\nDoes it provide the required functionality?\n\nCorrect statistical method?\nCould you extend it?\nCorrect results? (compare with another software)\nDo you understand the method? (check the paper linked with package)\n\nDoes it work reliably?\n\nPublished? (e.g. on CRAN)\nDifferent inputs?\nFast?\nDo other people use it? (downloads)\nDoes other software use it? (reverse dependencies)\n\nDoes the code look robust and well tested?\n\nHow are the functions implemented\nIs the source code readable\nCoverage with unit tests\nMature package?\n\nIs it well documented?\n\nDocumented functions?\nVignettes?\nPublished?\nInformative NEWS entry?\n\nWho are the authors, are they responsive?\n\nDid they publish statistics papers on this topic\nIs a github site with issues available"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#tools",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Tools",
    "text": "Tools\ncovr and unit tests\nriskmetric and the R Validation Hub\npharmaverse.org, with end-to-end examples"
  },
  {
    "objectID": "blog/blog_20230904_cen2023/index.html",
    "href": "blog/blog_20230904_cen2023/index.html",
    "title": "Personal Highlights: CEN2023",
    "section": "",
    "text": "The IBS (International Biometric Society) conference of the Central European Network, CEN2023 has been a great opportunity to keep myself up to date with the latest development of biostatistics, both in academia and industry. Thanks to the great effort made by the organizing committee and almighty Google Meet/Zoom, I have been able to follow the talks without any issue, and have definitely learned a lot.\nGiven my background, I paid more attention on talks and workshops on\n\nStatistical software, R programming and simulation\nCausal inference\n\nThere were also two topics that drew my attention: one is on statistical education towards medial professionals, the other is on a Data Challenge using RCT data.\n\nStatistical software\n\nSoftware Engineering Working Group (SWE WG), MMRM\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nDaniel Sabanes Bove (Roche). First year of the Software Engineering working group - working together across organizations\nGonzalo Duran-Pacheco (Roche). Comparing R libraries with SAS’s PROC MIXED for the analysis of longitudinal continuous endpoints using MMRM\n\n\n\n\nThe ASA Biopharmaceutical Section (BIOP) Software Engineering Working Group SWE WG was established in 2022. Currently they have 3 work streams:\n\nmmrm implements MMRM (mixed models with repeated measures)\nbrms.mmrm, the Bayesian version of MMRM\nHealth Technology Assessment with R\n\nAt a later talk, mmrm was compared with SAS’s PROC MIXED and R’s nlme, glmmTMB for analyses of longitudinal continuous endpoints. In terms of speed and convergence, mmrm is superior than others; while the estimate prodouced by mmrm is very close to PROC MIXED and glmmTMB.\nThis looks like a very interesting tool to try out! Vignette\n\n\nSimulation tools and RWD\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMichael Kammer (Medical University of Vienna). An overview of R software tools to support simulation studies: towards standardizing coding practices.\n\n\n\n\nKammer and colleagues did a review on R packages for simulation, and selected 14 top simulation packages, including simstudy, simdata, synthpop, bigsimr and others. The full list is made available here.\nA real-world dataset, NHANES was also introduced here. The data can be accessed with R package nhanesA.\n\n\n\nCausal Inference\n\n\n\n\n\n\nInformation\n\n\n\n\n\nWorkshop: Implementing the estimand framework in global drug development: Application of causal inference approaches (Mouna Akacha, Björn Bornkamp, Alex Ocampo, Jiawei Wei at Novartis)\nKeynote: Ruth Keogh (LSHTM). Causal inference with observational data: A survival guide\n\n\n\nThese two workshop / talk cover slightly different scenarios: one in RCT, one for observational data. It deserves a whole article or more to elaborate on this topic, so I’m only putting some resources here.\nCausal inference is definitely gaining traction in recent years in both academia and industry. Techniques such as g-computation, IPW and doubly robust estimation are starting to become mainstream. It is fascinating that these techniques themselves are not bound to a fixed model.\nResources:\n\nWorkshop repository Causal-inference-in-RCTs\nBook: Causal Inference: What If by Hernán and Robins (2020)\nPrincipal stratum strategy, Bornkamp et al. (2021)\nTime-dependent covariates, Keogh et al. (2023)\nTarget Trial Emulation (TTE), Hernán and Robins (2016)\n\n\n\nCovariate adjustment and data challenge with RCT data\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nKelly Van Lancker (Ghent University). Improving Power in Randomized Trials by Leveraging Baseline Variables\nDominic Magirr (Novartis). Organizing a Data Challenge on Covariate Adjustment in RCTs\nCraig Wang (Novartis). Participating in a Data Challenge on Covariate Adjustment in RCTs\n\nPanel discussion: Jonathan Bartlett (LSHTM)\n\n\n\n23 teams at Novartis participated in a Data Challenge on Covariate Adjustment. They were given a fixed outcome model, and 5 prior studies trial data, and their task was to create the design matrices that improve the precision compared to unadjusted data.\nIf I were to select talks based on the category titles, I would probably missed the whole session. However, it is surprisingly similar to using not trial, but real-world data (such as EHR) to make predictions. The conclusion were similar as well: using “supercovariates” created by ML isn’t gaining much compared to simple models such as ANCOVA. Possible reasons:\n\nsmall to moderate data size\nlinear relationship between covariates and outcome\ngood enough prognostic variables\n\nIt was also mentioned that the winning team did some trick to reduce the variance among the covariates. Would be interesting to read about it.\nSome resources:\n\nLancker et al. The use of covariate adjustment in randomized controlled trials: an overview link\nCovariate adjustment tutorial, link\n\n\n\nStatistical education\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMaren Vens et al (University of Lübeck). Biostatistics/Biometrics for physicians – essential or unnecessary? How do practicing physicians and dentists evaluate biostatistics? A cross-sectional survey\n\n\n\n\nStatistical education to students / professionals who are not used to working with data has always been tricky. Students generally think statistics is difficult, and need help from a statistician. However there are only limited number of statisticians. The talk by Vens and colleagues confirms what practicing statisticians know, but can’t do much about: most (87%) physicians and dentists in the survey need a statistician to help with their work.\nHow to improve the statistical competency is an important and relevant topic for discussion, and might require systematic changes in how it is taught. Use of modern technology can help, yet it’s only helpful when students start to not fear, or not find math and technology boring."
  },
  {
    "objectID": "blog/blog_20230301_ds_clinreport/index.html",
    "href": "blog/blog_20230301_ds_clinreport/index.html",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera (course link). It is not necessary to have a paid coursera membership to view the course, everyone could access it.\nIt is a 4 part course released one month ago (Jan/Feb 2023), and it seems that a follow-up will be released in the future.\nOverall I think it strikes a good balance between high-level introduction of the good practices, and examples with how they are implemented. Even though the course focuses on clinical reporting in the pharmaceutical industry, the practices are highly relevant in other sectors as well (e.g. public health, academia, other industries that use open-source software).\nSpecific statistical methods, packages are introduced only at a high-level; which means the course is not for learning how to use this or that packages; but good practice guidelines.\nIn my opinion,\n\nit would be useful if the learner has some experience with software development and/or statistics; otherwise learners might not know how to practice them.\nmost of the examples are related to R packages (understandable), so some experience with R package (use or develop) is useful.\nit could be a very good study material for university students in related subjects.\n\n\n\n\nModule 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software\n\n\n\n\nI have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog/blog_20230301_ds_clinreport/index.html#each-module",
    "href": "blog/blog_20230301_ds_clinreport/index.html#each-module",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "Module 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software"
  },
  {
    "objectID": "blog/blog_20230301_ds_clinreport/index.html#highlight",
    "href": "blog/blog_20230301_ds_clinreport/index.html#highlight",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "I have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog/blog_20240923_quartofriends/index.html",
    "href": "blog/blog_20240923_quartofriends/index.html",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "",
    "text": "I wrote a blog back in early 2023 when I first switched from blogdown to Quarto on my initial impression (read here), and this is a two-year follow-up on my journey since I started using Quarto, for my personal website, teaching, scientific works and collaborative community projects."
  },
  {
    "objectID": "blog/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "href": "blog/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a teaching tool",
    "text": "Quarto as a teaching tool\n\nFrom personal to workshop website\nI switched from blogdown to Quarto in late 2022, right after my PhD. It was initially a cure for a severe burnout from a combination of work-related stressors, when I desperately needed something other than research. My mental state was like the famous painting by Norwegian artist Edvard Munch:\n\n\n\n\n\nThe experience of the switch was explained in the previously mentioned blog. Briefly, it was light like a feather. Since I was quite satisfied, I thought, why don’t I make a workshop website? So I did.\nThe result was quite good, I made the (as far as I knew) first quarto workshop website at University of Oslo for the Oslo Bioinformatics Workshop Week 2022. Feedback from students were positive, and the instructor team thought it hosts the material in a more organized way.\n\n\nSingle day workshop -&gt; two week course\nI was greatly encouraged by the experience, so when I got a 50% position at University of Oslo as biostatistics lecturer, I thought, why don’t we have the same thing for the course?\n\n\n\n\n\nOh well, the workload is crushing. There were a few key differences:\n\nR scripts and material were unavailable since the course was originally in STATA. Everything need to be done from scratch, for at least 12 lab sessions;\nThe students generally have little IT skills, which means more effort need to be done to guide them through the ‘get started’ part.\n\n\n\n\n\n\nIt took one month to create the first version of the website. More details about the experience can be read here.\n\n\nAdding WebR to the course\nOne year later, as technology advances, we added new content to some parts of the website. Most notably is the interactivity achieved through WebR. For example, I made this page on randomness and statistical distribution where students can interactively modify code chunks in a web browser."
  },
  {
    "objectID": "blog/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "href": "blog/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a collaboration tool",
    "text": "Quarto as a collaboration tool\nA static (or even interactive) website is not exactly what you call ‘collaborative tool’. However, if you work as a group towards something cool, Quarto might just be the tool you need. Check out the CAMIS project to find out what I mean by this!"
  },
  {
    "objectID": "blog/blog_20240923_quartofriends/index.html#what-else",
    "href": "blog/blog_20240923_quartofriends/index.html#what-else",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "What else?",
    "text": "What else?\nThe associated talk is available on YouTube, check it out!"
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html",
    "href": "blog/blog_20230921_positconf2023/index.html",
    "title": "Personal Highlights: Positconf 2023",
    "section": "",
    "text": "The yearly party of Positconf (formerly Rstudio conf) has come to an end. I joined the virtual experience at home, it is of course not the same as attending in-person, yet the atmosphere in discord was still great!\nIt’s hard to choose which talks to watch since multiple were scheduled at the same time, so one has to prioritize. I definitely will re-visit some of the talks at a later point, so this blog acts as a placeholder for links so that I can find them in the future."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#make-interactive-things",
    "href": "blog/blog_20230921_positconf2023/index.html#make-interactive-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make interactive things",
    "text": "Make interactive things\nWebDev is definitely a big thing at this year’s positconf. If I’m learning one thing from the conference, I’d check out webR.\nI still remember when R was mainly for statistical analysis and computing back when I learned it. Now it’s become much more fun! Strictly speaking, webRand quarto are not R per se. However, they’ve become the gateway drugs for R programmers to dabble in WebDev. With web assembly (wasm), now one can execute R code in a browser and even run shiny app.\nUnlock the power of dataViz animation and interactivity in quarto by Deepsha Menghani used a super fun example (F-bomb) to demonstrate how to add interactivity to your barplot (or other plots) with Crosstalk. Check out the talk here. The presentation was as interactive as the quarto slides, good job Deepsha!\nRunning shiny without a server by Joe Cheng (repo): this was a big announcement. I used shiny at work, but for my own projects or smaller teaching projects I tried to stay away from shiny - I was concerned about the fee. This looks like a promising thing to try out once it’s stable, although I’d probably do webR first."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#make-pretty-things",
    "href": "blog/blog_20230921_positconf2023/index.html#make-pretty-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make pretty things",
    "text": "Make pretty things\nIt is fascinating to see so many organizations and individual R developers make their own themes for better branding, recognition and storytelling. More and more peple have realized that making beautiful plots is important, and totally possible as well. Work on layout, color, font and sizes!\n\nThemes\nAdding a touch of glitr: Developing a package of themes on top of ggplot by Aaron Chafetz, Karishma Srikanth and colleagues at USAID. repo\n\n\nTables\nMaking tables with gt has been on my to-do list for a while now. It is very inspiring to see so many cool tables that makes you wonder, “is it really JUST a table?” For example, check out this gallery by Posit community.\nThe book Creating beautiful tables in R with gt by Albert Rapp would be a good place to learn how to make nice tables. Actually the reason why I wanted to use gt is that it seems to be the mainsteam in clinical reporting in pharma. I bumped into this blog post some time ago, and this would be my starting point.\n\n\nQuarto\nIf you want to go one step further and start making your quarto project pretty, there are a few things to try out.\nAlbert Rapp in his talk HTML and CSS for R Users stated that quarto is a gateway drug to WebDev. It reminds me of my very first presentation at my local R users community (2019) was about building a website with blogdown, and when I really spent a lot of time to make my markdown documentation colorful with span style - and that was about everything I knew.\nNow I want more. Learning HTML and CSS can make your dataviz, tables, slides and dashboards look not only professional but also special. I’m going to check out the scss variables in quarto which defines the theme, theme_file.scss. Emil Hvitfeldt (Styling and templating quarto documents) showed us how to make really pretty and animated (!) quarto sldies themes, and shared this template with us, quarto-revealjs-earth. I really like how revealjs slides look like, just that the MacOS Keynote (or MS ppt) drag-and-drop seems more flexible to me (?) Guess it’s something I should get used to over time.\nRichard Iannone (Extending quarto) introduced quarto shortcode extensions to add a bunch of fancy-looking icons to quarto files. To create extensions in general: https://quarto.org/docs/extensions/creating. This is for more pro-users since you needs to learn lua."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#quarto-updates",
    "href": "blog/blog_20230921_positconf2023/index.html#quarto-updates",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Quarto updates",
    "text": "Quarto updates\nQuarto is definitely one of the most discussed topics in the year 2022-2023 in the R community. For good reasons. I need to catch up the the latest developments annd use-cases:\n\nWhat’s new in quarto? by Charlotte Wickham\nReproducible manuscripts with Quarto by Mine Çetinkaya-Rundel\nParametrized quarto reports improves understanding of soil health by Jadey Ryan\n\nand so many more. I couldn’t follow all the talks and I’m sure there are lots of great examples of how quarto is better than traditional ways of reporting."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "href": "blog/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "title": "Personal Highlights: Positconf 2023",
    "section": "A few other things to check out",
    "text": "A few other things to check out\nBeyond the web and quarto topics, I think there are some existing and new tools that can be useful for my work. For example,\n\nI should review Hadley and Jenny’s R package book (2e).\nthis package targets for pipeline automation and management look like something that can be used for my analysis\n…\n\nIt will take a while to digest the latest developments. But little by little, we’ll get there! People in the R community are doing great things."
  },
  {
    "objectID": "blog/technotes_20230220_pkgdown/index.html",
    "href": "blog/technotes_20230220_pkgdown/index.html",
    "title": "R package website with pkgdown",
    "section": "",
    "text": "1. Create the website skeleton.\nBefore editing the details, we need to create the skeleton for the website. It can be done with usethis and pkgdown packages.\nIn R, run this:\nusethis::use_pkgdown()\nThis creates the _pkgdown.yml file, which is the place you configure your site.\nTo view the initial package website, use the following command:\npkgdown::build_site()\nThis creates docs/ directory containing a website\n\nREADME.md becomes the homepage,\ndocumentation in man/ generates a function reference,\nvignettes are rendered into articles/.\n\n\n\n2. Edit the vignette documentation\nMake sure that the vignette index is consistent with Title, otherwise it will not render.\n\n\n3. Build and preview your site\nNow check if the site looks good, and contents are correctly positioned.\npkgdown::preview_site()\npkgdown::build_site()\nYou can also do this to build the site.\npkgdown::build_site_github_pages()\n\n\n4. Deploy site with GitHub Pages\nThere seems to be two options:\n\nusethis::use_pkgdown_github_pages(), this function should take care of everything after pushing changes to GH.\nif you used pkgdown::build_site_github_pages() and pushed everything to GitHub, it might not automatically deploy your site to GH pages. I tried to go to Settings -&gt; Pages -&gt; Deploy from a branch -&gt; main -&gt; /docs, this makes Action deploy your site from the docs folder.\n\ndouble check if you have .nojekyll file\nif a website does not show, check whether you have docs in the .gitignore file; since you are deploying from that folder."
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html",
    "href": "blog/technotes_20230225_shinyappsio/index.html",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "",
    "text": "Useful references:"
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#considerations",
    "href": "blog/technotes_20230225_shinyappsio/index.html#considerations",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Considerations",
    "text": "Considerations\nA few ways to do it: Shiny Server (free), shinyapps.io (free and premium), and professional Rstudio Connect (paid).\nI choose to test out the second option, since it allows more possibilities compared to the free open-source Shiny Server.\nThe free option should allow me to create 5 apps, which is more than enough for personal use. It also allows 25 active hours per month; a note on that at the end."
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#configuration",
    "href": "blog/technotes_20230225_shinyappsio/index.html#configuration",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Configuration",
    "text": "Configuration\nSign up with GitHub account; or something else. It is possible to change account name afterwards.\nIn Rstudio,\n\nfirst install.packages('rsconnect')\nthen, configure the account. It can be done with rsconnect::setAccountInfo() with information provided in your own shinyapps.io page.\n\nBefore the last step, it is necessary to have an app to deploy!"
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "href": "blog/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Create my first shiny project",
    "text": "Create my first shiny project\nHere I use my usual workflow of creating a new R project:\n\nCreate a new repo on GitHub;\nClone the repo locally, by opening a new R project with version control.\n\nNow copy the two R scripts from the demo example:\n\nserver.R\nui.R\n\nTest locally by running shiny::runApp(). This should render the app."
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "href": "blog/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Deploy to shinyapps.io",
    "text": "Deploy to shinyapps.io\nrsconnect::deployApp() will deploy the app, with an automatically generated url that links to your account.\nThe demo app is deployed here.\n\nNote on active hours\nAfter deployment, the site seems to be active until you shut it down manually; or timeout. The default timeout is 15 minutes, which can be reduced to 5 minutes.\n25 hours per month suggests that I can open the site for 300 times (without manually shuting it down). It might be necessary to start using the paid options, if I have more than one site, or multiple users want to access it …"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html",
    "href": "blog/technotes_20250703_research_guide/index.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#overview",
    "href": "blog/technotes_20250703_research_guide/index.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "Overview",
    "text": "Overview\nThis guide provides a practical framework to prepare for Research Scientist roles in academia, industry research labs (FAANG, OpenAI, DeepMind, Anthropic, NVIDIA), and national labs. It blends personal experience with hiring-manager expectations and common evaluation rubrics.\nGitHub repository: Research Scientist Preparation Guide\nData Science Notes: Data Science Intro\nStatistical Learning Notes: Statistical Analysis LLM : Statistical Analysis —"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#key-interview-components",
    "href": "blog/technotes_20250703_research_guide/index.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1. Research Portfolio Deep Dive\n\nBe able to explain your core research contributions clearly and concisely.\nClearly define the problem, highlight novelty, describe methods, show results, and explain real-world relevance.\nPractice different versions of your explanation (5-min, 15-min, 30-min).\nShow awareness of how your work connects to broader trends and open challenges.\n\n\n\n2. Technical Machine Learning Knowledge\n\nReinforcement Learning: Policy gradients, actor-critic, safe RL, off-policy and on-policy methods.\nDeep Learning: Architecture design, optimization, transformers, generalization.\nStatistical Learning: Model selection, regularization (L1/L2), kernel methods, bias-variance tradeoff. (See Notes)\nProbabilistic Modeling: Bayesian inference, uncertainty quantification, latent variable models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: Scaling laws, fine-tuning, instruction tuning, prompting, RAG architectures.\nComputer Vision: Object detection, segmentation, multimodal learning.\nData Science Foundations: Data pipelines, EDA, data cleaning, transformation. (See Notes)\n\n\n\n3. System Design / Applied ML Problems\n\nUnderstand how to build ML systems end-to-end:\n\nFeature engineering and data preprocessing\nModel training, hyperparameter tuning, and validation\nDeployment (latency, scalability, monitoring)\nAddressing data drift, noisy labels, and real-world constraints\n\n\n\n\n4. Coding and Algorithmic Skills\n\nModerate-level data structures and algorithms:\n\nArrays, strings, trees, graphs, dynamic programming\n\nML coding:\n\nData manipulation using pandas/numpy\nPrototyping in PyTorch, TensorFlow, or JAX\nBasic SQL queries and joins\n\n\n\n\n5. Behavioral and Collaboration Skills\n\nPractice behavioral questions:\n\n“Tell me about a time you resolved a conflict”\n“How do you approach ambiguous research questions?”\n\nDemonstrate collaboration across disciplines and teams\nEmphasize your ability to communicate technical ideas to non-experts"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#recommended-preparation-resources",
    "href": "blog/technotes_20250703_research_guide/index.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nKey Papers: NeurIPS, ICLR, ICML, CVPR, ACL\nBooks:\n\nDesigning Machine Learning Systems – Chip Huyen\nDeep Learning – Ian Goodfellow\nThe Elements of Statistical Learning – Hastie, Tibshirani, Friedman\nBayesian Reasoning and Machine Learning – David Barber\n\nCoding Practice: Leetcode (focus on Medium problems)\nSystem Design Practice: ML interview prep platforms, YouTube walkthroughs\nMock Interviews: Conduct with peers, mentors, or online platforms"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#example-interview-questions",
    "href": "blog/technotes_20250703_research_guide/index.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nWhat are the main contributions of your most recent paper?\nHow does your approach compare to existing methods?\nHow would you adapt your method if data was scarce or noisy?\nWhat assumptions underlie your models, and how would you validate them?\nHow do you handle uncertainty and interpretability?\nHow would you apply your method to a new domain or product?"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#my-personal-advice",
    "href": "blog/technotes_20250703_research_guide/index.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "My Personal Advice",
    "text": "My Personal Advice\n\nClarity beats complexity — explain ideas simply.\nBe enthusiastic about your work and its impact.\nConnect your strengths to the job’s mission.\nShow your ability to collaborate and iterate."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#mentorship",
    "href": "blog/technotes_20250703_research_guide/index.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’d like feedback on your talk, paper deep dive, or a full mock onsite, reach me at cs.kundann@gmail.com."
  },
  {
    "objectID": "blog/blog_20230112_roche_opensource/index.html",
    "href": "blog/blog_20230112_roche_opensource/index.html",
    "title": "Open source reporting with R: clinical, public health, RSE and embrace the change",
    "section": "",
    "text": "Two days ago (Jan 11 2023) I watched a presentation by data scientists at Roche about why they are making their clinical trials in 2023 open source with R. As someone who uses R for most of the time and has done similar works (not in pharma, but in public health surveillance and reporting: watch my talk, slides to find out what we do), I watched the presentation with great interest. Here are my notes, combined with some thoughts on open-source in the industry, public sector and academia.\n\nThree reasons for why I am writing this blog\n\nNote down some of the technology which points towards the future of the field\nRelate to my experience of open-source applied in public health, specifically public health reporting\nShare some thoughts in statistical education of applied students/researchers (e.g. medicine), and training Research Software Engineers\n\n\n\nMy experience with statistical software\nTo put my opinions in perspective,\n\nI do not have experience with SAS or pharma, so I do not have first-hand knowledge on the functionality, ease-of-use or the popularity of commercial softwares in the industry.\nI did my MSc and PhD in statistics/biostatistics/medical informatics and R had always been a default choice.\nI worked in public health for a few years, where Excel is possibly the most common tool, and STATA and R are scarcely used (statisticians, epidemiologists, bioinformaticians).\nIn the past few years, my university has made the switch from SPSS to STATA for intro statistics for medical students (while students at higher level, or doing advanced analyses might use R/python), and a test-run with R might be in motion.\n\n\n\n\nClinical Reporting\nIn drug development at pharmaceutical companies (and/or research institutes and hospitals), these data related tasks are very common:\n\nsummarise safety and efficacy data\nprovide accurate picture of trial outcomes\nmanage data collection across different sites\n\nCompleting these tasks in a correct, efficient and reproducible manner is crucial for patient safety. However, these tasks are also highly resource intensive: highly trained scientist, statisticians and technincians must be involved in the process. Historically, pharma use commercial software such as SAS.\n\nRegulation and exploration needs\nThere are requirements for clinical reporting: both regulartory and exploratory. From the regulatory side, there exist industry standards (CDISC) in the clinical research process, such as SDTM (Model for Tabulation of Study Data) and ADaM (Analysis Data Model). Statistical analyses, tables, listings and graphs (TLGs) also fall into this cateogory.\nFrom the exploratory side, clinical data are highly context dependent, and new formats of data such as imaging are more and more used in prediction modeling and drug development.\nIn addition, it is not hard to imagine that the technical competency of employees differ, especially in large organizations. Enabling people with less experience to analyse trial data in a reproducible manner is helpful for not only the learning and growth of employees, but also the productivity of organizations.\nThe existing commercial tools are not able to adapt to the rapid changes in the field.\n\n\nTransition into Open-Source\nIn this talk, Dr Kieran Martin at Roche introduced that they started using R as their core data science tool, aiming to move their codebase to having a core R. In the future, they plan to have something that is lanugage agnostic: meaning that python, Stan, C++, Julia and beyond can be used for different tasks.\nI only noted down a few of the things they mentioned on the infrastructure side:\n\nOCEAN - a lanugage agnostic computing platform on AWS (docker)\nGit, Gitlab for version control and collaboration\nRstudio connect server\nSnakemake for orchestrate production\n\n\n\nR and Shiny\nThere are obvious benefits of using R. It is convenient to install and use (if you used python and R, you’d probably agree), and the latest development in Shiny made it very easy to develop interactive visualizations, suitable for exploration. Package development is critical for reproducibility and distributing works - which R does it very well. A few packages developed by pharma are Teal and admiral: the ADaM in R, which I intend to check out at one point.\nR has deep roots in academia which means the newest statistical methods are well covered; which also affects the skill sets that talents own - fresh graduates probably already learned it at university. R being open source means that collaboration with external partners is much more efficient, and transparent. Strong community support is another positive thing that encourages beginners to enter the field and learn.\n\n\n\n\nOpen Sourcing Public Health\n\nSurveillance and reporting\nOne key functionality of public health (PH) authorities is stay informed and inform. They collect data from labs, hospitals and clinics across the country, summarize into useful statistics in tables and graphs, make reports, then inform the policy makers to make decisions (such as vaccination campaigns).\nCompared to clinical reporting (in my understanding), there are many similarities - we make TLG (tables, listings and graphs). There are also features that make reporting in public health unique:\n\nPH surveillance and reporting are dynamic and real-time, which can change in a matter of days. That is because the situation of different infectious diseases can evolve rapidly, so PH authorities need to make appropriate adjustments.\nTime and location (spatial-temporal) are important. Different time granularity (daily, weekly) and geographical units (nation, county, municipality, city districts) are typically required for reporting.\n\n\n\nScale up and automate with open source tools\nTraditionally, these reports are made manually - one location, one graph per time on a certain disease. When a global pandemic hits, this is definitely not fast enough. At my team (Sykdomspulsen team at the Norwegian Institute of Public Health), we tried a different approach. Details of what we did can be found in this talk(slides), but to make it brief:\n\nWe developed a fully automated pipeline that connects 15 registries (vaccination, lab, hospital and intensive care and many more). The data is gathered, censored, cleaned and pre-processed for down-stream analysis\nStatistical analysis, tables, graphs and maps are made for all locations in Norway for various outcomes of interest, such as Covid, influenza, respiratory and gastrointestinal infections\nOver 1000 customized reports with over 30 graphs and tables are produced daily and sent to local PH officials, where we also had a shiny website (Kommunehelsetjenester for Kommunelege) for over 300 PH officials to get most up-to-date information about their own municipality\n\nBy automation, every year Sykdomspulsen can save 700 000 NOK (roughly 70k USD) while making 400 times more real-time reports for public health. Even better, with reproducibility and quality control.\n\n\nToolbox\nSykdomspulsen is a small team (8 people, 3 are statisticians and 1 engineer), and our infrastructure was built upon R packages, which we call splverse. Our infrastructure is not fundamentally different from the one Roche introduced, basically:\n\nR does the task planning and project organization. On top of this, the data cleaning, statistical analysis are implemented. Graphs, tables and maps are made with appropriate R packages\nRmarkdown does automated reporting into .docx and .xlsx. Some reports are also in .html tables to be embedded into customized emails\nRstudio Workbench and GitHub help with teamwork\n\nDocker, GoCD and Airflow do the CI/CD and orchestration\n\n\n\n\nEmbrace the transition\n\nCulture change needed\nUnfortunately, not all organizations are eager to abandon the old way. Even at our own institute where researchers are the majority, open source and modern day programming is hardly practiced (by my observation). Even worse, under the budget cuts in 2023-24, a large number of younger employees who have the technical skills have left - which left the public health surveillance even more vulnerable now that Covid is far from over.\nIn my opinion, public health needs open-source and good programming even more than pharmaceutical companies. Both save lifes - and PH has less money to invest in softwares, infrastructures and talents. In this situation, resources should be spent in fields that are critical and most cost-effective; yet in reality this is often not the case.\nThe slow culture change at big organizations can happen, but only if there is a sufficient amount of employees who are willing to embrace the new technology. In the talk by Roche they about about their training strategy. It is not possible to train all users, and not everyone has the same needs at the same time. Therefore, self study with certain study paths is encouraged and supported.\n\n\nTeach programming to students in various fields\nBased on my experience in the UK and Norway, students (myself included) learn R programming in one of the two ways\n\nLearning by Googling (self-taught): a university degree needs to use it: provides a short introduction, then students learn by using. This is how I learned R at my MSc Statistics degree, and this is probably the most common way\nWorkshops at university: organizations such as the Carpentries provide course material and teaching a few times per year, where interested students (usually from subjects such as biology) come and learn. These classes are quite popular, and usually have a long waiting list.\n\nFrom learning by googling to some organized teaching - that is already some good progress. However, if not, can we improve?\nIn my experience with statistical advising with the university hospital, clinical researchers and medical students are enthusastic to get their statistics done, some are also eager to do some analysis themselves. That is good. Yet, there is generally lack of capacity - either knowledge or software skills. Once the statistician who helps with the project stops, the project ends. There is the need to have in-house statistical capacity. To this end, open-source softwares such as R, and good programming practice (reproducibility for example) can help a lot: the license doesn’t end, and everything is documented so that the next person can continue the work.\nI’m glad that my university has made some transitional efforts in this regard: STATA instead of SPSS is being taught to medical students as part of their statistics course. There might be a test-run in R soon, which is very exciting (since I’ll be involved in the teaching)!\n\n\nStatistical engineering and RSEs\nThat was the capacity building to get beginners more independent. On the other side, there is also the need for better programming practice for researchers at more advanced level. Research Software Engineering (RSE) is starting to get more and more attention, because it is not only relevant for research (i.e. getting papers published), but in broader applications.\nFor example, in the talk by Roche, they mentioned that “RSE teams need to accelerate adoption of new statistical methods and biomarker data analysis”, and the implementation with R packages and templates is at its core. In the future more languages would be included such as Python, Stan, C++ and Julia.\nHowever, RSE as a job title or career path is still a new thing. I know two RSEs at my university, and RSE is definitely not your typical academic faculty position: only departments that think it’s important makes positions, often not permanent. To get any new methods actually used in either industry or the public sector outside research, translating methods into tools is must-do. In the future I hope RSE becomes a stable and common career path, and more exciting things can happen."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html",
    "href": "blog/technotes_20230519_pkgcran/index.html",
    "title": "R package workflow",
    "section": "",
    "text": "This checklist is being updated over time. Mostly for my own use; but great if it helps you as well!\nFor a complete treatment, please refer to R Packages (2e) by Hadley Wickham and Jennifer Bryan."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "href": "blog/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "title": "R package workflow",
    "section": "Initialize the project",
    "text": "Initialize the project\n\nusethis::create_package('path_to_pkg/pkgname') \n\nIt opens a new R project (directory) named pkgname, with the following items:\n\nDESCRIPTION\nNAMESPACE\ndirectory R/\n.Rbuildignore and .gitignore\nand the project icon, pkgname.Rproj.\n\nIf you have an existing R project but wish to build a package there, copy everything but pkgname.Rproj, and modify the files in your existing pkg directory. Pay extra attention to the hidden files like .Rbuildignore.\n\nusethis::use_mit_license() # modify name to yours\nusethis::use_readme_md() # if you do not have this already\nusethis::use_news_md()\nusethis::use_test()\n\n# create a folder for future data documentation\nx &lt;- 1 \nusethis::use_data() \n\nIn addition, URL and bug reports should be added in the DESCRIPTION."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#planning",
    "href": "blog/technotes_20230519_pkgcran/index.html#planning",
    "title": "R package workflow",
    "section": "Planning",
    "text": "Planning\nIt is good practice to start with planning the package, rather than directly start coding.\nCreate a folder called dev. To prevent it from being built, add the following line in .Rbuildignore"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "href": "blog/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "title": "R package workflow",
    "section": "Write, test and document",
    "text": "Write, test and document\nCreate exported functions in R/, development code in script/ (or somewhere else, such as dev/).\n\nData: raw and processed\nNeed to be clear in mind where the data files go. There are a few data related folders:\n\nraw data files, in the format of excel sheets or csv. Usually placed as inst/data_name.csv\nR scripts to process the raw data so that we create data object inside the package, put inside data-raw\ndata objects that can be called as pkg::data_name, are placed in data. These files are usually directly generated by executing write.rda().\ndata documentation, usually placed in R/data_documentation.R. These are Roxygen2 documents for the data.\n\n\n\nDocumentation\nYou need to configure the Build tools.\nThese three things should be done:\n\nFunction documentation\nCreate a function f1, and put your cursor on it. Go to Code -&gt; Insert Roxygen Skeleton to create the template.\nAlternatively, use #' to start.\n\n#' A simple placehold function \n#'\n#' @param x a numeric value\n#'\n#' @return a value 3 greater than the input\n#' @export\n#'\n#' @examples \n#' f1(5)\nf1 &lt;- function(x){\n  x+3\n}\n\n\n\nData documentation\nIt can be beneficial to create a separate file to document data only, say data_documentation.R under the R/ directory.\n\n#' Placeholder data x\n#'\n#' This dataset contains one value, x\n#'\n#' @format\n#' \\describe{\n#' \\item{x}{The placeholder data x}\n#' }\n#' @examples\n#' print(x)\n\"x\"\n\n\n\nVignette documentation\n\nusethis::use_vignette('your_vignette')\n\n\n\nDeploy to pkgdown\nCheck this reference here"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "href": "blog/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "title": "R package workflow",
    "section": "Build package and check",
    "text": "Build package and check\nIt is possible that your checks don’t pass on the first try.\n\nWhat to ignore when build?\n^.*\\.Rproj$\n^\\.Rproj\\.user$\n^dev$\n^_pkgdown\\.yml$\n^license\\.md$\nMakefile\ndata-raw\ncran-comments.md\n^\\.github$"
  },
  {
    "objectID": "talks/rstats_20230721_teaching/index.html",
    "href": "talks/rstats_20230721_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Time and place: July 21, 2023 10AM. Roche office, Basel, Switzerland\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "talks/rstats_20230721_teaching/index.html#about-the-topic",
    "href": "talks/rstats_20230721_teaching/index.html#about-the-topic",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "About the topic",
    "text": "About the topic\nThe 8 day introductory statistics course (MF9130) at the Faculty of Medicine, University of Oslo is designed for PhD students in medicine, biology, psychology and other health related fields. Similar to other conventional teaching methods, the course has been focusing largely on theory and hand calculation. The software has been Stata and SPSS, and data analysis was mostly left for the students to figure out on their own.\nThis year, we made an attempt to transform the course with R, and aimed to teach more practical data analysis skills. We added one session per day where the instructor guide students on R and project management, importing data , basic manipulation and statistical methods. The IT skills of the students vary greatly, and therefore we used the ‘sticky notes’ help system borrowed from the Carpentries to make sure everyone could get help in the first days. We have created a course website using Quarto, where all the material and R exercises (with rendered solution) are available for self-study. We have witnessed amazing progress - by the end of the first week, students with the least computer / data skills were able to work on dataframes, make basic plots and do a chi-squared test. This helps build students confidence in data and statistics, and as a result, they can start to work on their own datasets using the skills immediately."
  },
  {
    "objectID": "talks/rstats_20190402_blogdown/index.html",
    "href": "talks/rstats_20190402_blogdown/index.html",
    "title": "Building Website in R: Step by Step Introduction to blogdown",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "talks/community_20240921_quartofriends/index.html",
    "href": "talks/community_20240921_quartofriends/index.html",
    "title": "Use Quarto, Make Friends",
    "section": "",
    "text": "It has been two years since Quarto became the most popular reproducible publication tool in data science and R community. However Quarto is so much more than just a publication tool! I started using it since late 2022, and it has helped me become more organized, productive and connected with people in the data science community.\nIn this talk I will not focus on the technical aspects on ‘how’ to use this tool. In the first part of the talk, I would like to report the latest news and trends seen in the useR conference and Posit conf, the two biggest global R events. In the second part, I will share my own experience in using Quarto for my career: from learning new skills, collaborating with co-workers, teaching university courses to networking and building a community (CAMIS collaboration). It is a powerful tool to share your work, and make new connections - both for work and for fun! I hope this talk will provide you with some new ideas on how to use this fantastic technology to fulfill your goals."
  },
  {
    "objectID": "talks/ehr_20210218_biday/index.html",
    "href": "talks/ehr_20210218_biday/index.html",
    "title": "Network Analysis of Hospital EHR data",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "talks/talks.html",
    "href": "talks/talks.html",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "talks/talks.html#upcoming",
    "href": "talks/talks.html#upcoming",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "talks/talks.html#selected-previous-talks",
    "href": "talks/talks.html#selected-previous-talks",
    "title": "Talks",
    "section": "Selected previous talks",
    "text": "Selected previous talks\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\nDate\n\n\n\n\n\n\nUse Quarto, Make Friends\n\n\nKolkata UseR meetup\n\n\n2024-09-21\n\n\n\n\nOne step closer to better Electronic Health Records data\n\n\nPHUSE Single Day Event Basel\n\n\n2024-09-18\n\n\n\n\nCAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n\n\nUseR! 2024 Salzburg\n\n\n2024-07-10\n\n\n\n\nA one year recap on teaching statistcis to medical students: how can R and Quarto help?\n\n\nR/Medicine 2024 - Online\n\n\n2024-06-13\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\nBasel R meeting\n\n\n2023-07-21\n\n\n\n\nSykdomspulsen: An automated public health surveillance platform\n\n\nOslo UseR meetup \n\n\n2022-06-16\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/ph_20220616_splverse/index.html",
    "href": "talks/ph_20220616_splverse/index.html",
    "title": "Transfer Learning in Deep Reinforcement Learning for Scalable VVC in Smart Grids",
    "section": "",
    "text": "About the talk\nWatch the talk on YouTube\nSykdomspulsen is a real-time analysis and disease surveillance system designed at developed at the Norwegian Institute of Public Health (FHI). Sykdomspulsen processes new data collected from 15 data sources (e.g., covid-19 cases), runs 1000.000+ statistical analysis automatically for all locations (nation, county, municipality) in Norway, produces 1000+ reports and alerts for public health authorities and shares data to the public on GitHub.\nSykdomspulsen runs on a collection of R packages, the {splverse}. {splverse} is an ecosystem for infectious disease surveillance, from analysis planning, statistical analysis to reporting via visualization, shiny website and Rmarkdown generated reports. In this talk, Chi will present how Sykdomspulsen does public health real-time surveillance during the pandemic using R. Chi will introduce some of the core packages and illustrate how they work together, with an example using real surveillance data published daily on GitHub.\n\n\nAbout the speaker\nChi is currently working at the Sykdomspulsen team as a researcher and R developer, at the Norwegian Institute of Public Health. Before she joined Sykdomspulsen in the middle of the pandemic (2020), she was a PhD student at the Department of Biostatistics at University of Oslo (OCBE), working on hospital EHR data."
  },
  {
    "objectID": "projects/cv.html",
    "href": "projects/cv.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects/cv.html."
  },
  {
    "objectID": "projects/phuse/index.html",
    "href": "projects/phuse/index.html",
    "title": "PHUSE - CAMIS",
    "section": "",
    "text": "I am contributing to two working groups at PHUSE: CAMIS, and RWD - Real World Data Guideline (early stage).\n\nCAMIS: Comparing Analysis Method Implementations in Software\nCAMIS is a cross-industry group formed of members from PHUSE, PSI and ASA.\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comparison and comprehensive explanations.\n\n\nRWD Working Group\nThis is a newly formed working group, working on statistical programming guidelines while working on RWD (read world data)."
  },
  {
    "objectID": "projects/sykdomspulsen/index.html",
    "href": "projects/sykdomspulsen/index.html",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "projects/sykdomspulsen/index.html#overview",
    "href": "projects/sykdomspulsen/index.html#overview",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Projects.” https://kundan-kumarr.github.io/projects/projects.html."
  },
  {
    "objectID": "projects/robo.html",
    "href": "projects/robo.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/os_teaching/index.html",
    "href": "projects/os_teaching/index.html",
    "title": "Teach in R and Quarto",
    "section": "",
    "text": "MF9130E - Introductory course in statistics\n8-day intensive course on introductory statistics. April 2023 we made it with R rather than propriety software, coupled with live-coding sessions to enhance understanding of basic concepts such as distribution and hypothesis tests.\nRepository\nCourse website\nRead more about the experience in\n\nblogpost\npresentation"
  },
  {
    "objectID": "projects/drl.html",
    "href": "projects/drl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "misc/literature.html",
    "href": "misc/literature.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "antibiotics stewardship\ninfections (community, hospital)\nelectronic health records\nquality assurance: prescripton and use"
  },
  {
    "objectID": "misc/literature.html#ab-x-ehr",
    "href": "misc/literature.html#ab-x-ehr",
    "title": "Kundan Kumar",
    "section": "AB x EHR",
    "text": "AB x EHR\nMoehring 2021, EHR identify AB use among hospitalized patients\nReenggli 2021: assess EHR conversion into AB stewardship indicators\nCairns 2021: integrate AB stewardship with EHR in australia\nJenkins 2022: AB stewardship using electornic prescribing systems, review of intervention and outcome measures\nKuijpers 2024, excessive length of AB duration for HAI, support AB stewardship high relevance"
  },
  {
    "objectID": "misc/literature.html#ehr-doctor-experience-system-design",
    "href": "misc/literature.html#ehr-doctor-experience-system-design",
    "title": "Kundan Kumar",
    "section": "EHR doctor experience, system design",
    "text": "EHR doctor experience, system design\nOstrer 2023, real time benefit tools must be designed to serve both clinicians and patients. mentions burden\nKawamoto 2019 Association of EHR add-on app for neonatal bilirubin management, physician efficiency and care quality. Good system and add-on can save clinician time and improve patient care.\nTsai 2020, EHR implementation, barriers to adoption and use high relevance\nOverhage 2020, time spent using EHR\n\nburnout\nTajirian 2020, influence of EHR on physician burnout high relevance\nKhairat 2020, EHR use with physician fatigue and efficiency\nKroth 2019, factors of EHR design and use for physician stress and burnout high relevance\nMelnick 2020, EHR usability, task load and burnout high relevance\nHilliard 2020, factors associated with burnout\nMore on this topic\nhttps://www.sciencedirect.com/science/article/abs/pii/S0897189718301356\nhttps://journals.sagepub.com/doi/full/10.1177/20552076231220241\nhttps://jamanetwork.com/journals/jamanetworkopen/article-abstract/2778909\n\n\nDesign\nGascon 2013 the process of designing a EHR lab request module\nTorres 2017, effect of EHR design on documentation and compliance high relevance"
  },
  {
    "objectID": "misc/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "href": "misc/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "title": "Kundan Kumar",
    "section": "EHR data quality, use for prediction model development",
    "text": "EHR data quality, use for prediction model development\n\nerror\nbell 2020\nhttps://www.sciencedirect.com/science/article/abs/pii/S2213076420300439\nPaper, comparison of error reporting systems. 82% undetected, 90% had no corresponding incident report, some errors not reported at all\nWestbrook 2020, changes in medication administration error rates, associated with EHR\nGildon 2019, impact of EHR on prescribing errors in pediatric clinics. introduction is relevant for pointing out the need for well designed system\nGinzburg 2018, use clinical decision support within EHR to reduce incorrect prescribing for acute sinusitis. shows prompt for physicians to fill in reason for why AB is required high relevance\n\n\nbias\nGianfrancesco 2018, potential biases in ML algorithms using EHR data. Existing EHR disparities should not be amplified by thoughtless or excess reliance on machines.\nAgniel 2018, biases in EHR due to process within the healthcare system. healthcare processes must be addressed in the analysis of observational health data, context is important. high relevance\nBower, biases in EHR based surveillance of cardiovascular disease risk\nHarding2024, addressing common sources of bias in type 2 diabetes following covid, using EHR\nBica 2020, current and future methods to address underlying challenges from EHR to treatment effects mid relevance to manuscript, but important to myself\nDesai 2020, prediction using admin EHR, hear failure"
  },
  {
    "objectID": "misc/literature.html#ab-stewardship-not-so-relevant",
    "href": "misc/literature.html#ab-stewardship-not-so-relevant",
    "title": "Kundan Kumar",
    "section": "AB stewardship (not so relevant)",
    "text": "AB stewardship (not so relevant)\nRenggli 2021: consumption of anti-meticillin resistant staphylococcus aureus antibiotics in Swiss hospitals is associateed with antibiotic stewardship measures\nVaughn 2021: AB overuse and stewardship at hospital discharge\nGraber 2019: electronic tools to decrease AB use"
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "guide/guide.html#overview",
    "href": "guide/guide.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "guide/guide.html#key-interview-components",
    "href": "guide/guide.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1️⃣ Research Portfolio Deep Dive\n\nBe able to explain your core research contributions in detail.\nClearly articulate: problem definition, novelty, methods, results, and real-world impact.\nPrepare multiple levels of technical depth (5-min, 15-min, 30-min versions).\nPractice connecting your work to broader research trends and applications.\n\n\n\n2️⃣ Technical Machine Learning Knowledge\n\nReinforcement Learning: algorithms, policy gradients, actor-critic, safe RL.\nDeep Learning: optimization, architecture design, generalization, transformers.\nProbabilistic Modeling: Bayesian inference, uncertainty estimation, graphical models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: LLM scaling laws, prompting, fine-tuning, RAG architectures.\nVision: object detection, segmentation, multi-modal perception.\n\n\n\n3️⃣ System Design / Applied ML Problems\n\nBe able to discuss:\n\nEnd-to-end ML pipelines\nData challenges (imbalance, noisy labels, drift)\nModel serving and deployment challenges\nScalability, latency, interpretability\n\n\n\n\n4️⃣ Coding and Algorithmic Skills\n\nLeetcode-style DSA for research interviews (moderate level)\nData manipulation (pandas, numpy, SQL)\nModel prototyping (PyTorch, TensorFlow, JAX)\n\n\n\n5️⃣ Behavioral and Collaboration Skills\n\n“Tell me about a time…” questions.\nCollaboration across teams.\nHandling ambiguous open-ended research problems.\nCommunication with product teams or non-research stakeholders."
  },
  {
    "objectID": "guide/guide.html#recommended-preparation-resources",
    "href": "guide/guide.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nPapers: Read papers from top-tier conferences (NeurIPS, ICML, ICLR, CVPR, ACL).\nCoding: Leetcode (medium), ML system design problems.\nSystem Design: Read “Designing Machine Learning Systems” by Chip Huyen.\nMock Interviews: Practice mock sessions with peers or mentors.\nPresentation: Prepare 1-2 strong 20-minute research talks."
  },
  {
    "objectID": "guide/guide.html#example-interview-questions",
    "href": "guide/guide.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nHow does your research contribute to state-of-the-art methods?\nWalk me through one of your recent papers.\nHow would you apply your methods to X domain?\nWhat challenges remain in your area of research?\nHow do you evaluate safety, robustness, or uncertainty in your models?\nHow would you adapt your methods if labeled data was extremely limited?"
  },
  {
    "objectID": "guide/guide.html#my-personal-advice",
    "href": "guide/guide.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "My Personal Advice",
    "text": "My Personal Advice\n\nClarity beats complexity — explain ideas simply.\nBe enthusiastic about your work and its impact.\nConnect your strengths to the job’s mission.\nShow your ability to collaborate and iterate."
  },
  {
    "objectID": "guide/guide.html#mentorship",
    "href": "guide/guide.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’re preparing for Research Scientist interviews and would like advice or mentorship, feel free to reach out at cs.kundann@gmail.com."
  },
  {
    "objectID": "guide/literature.html",
    "href": "guide/literature.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "antibiotics stewardship\ninfections (community, hospital)\nelectronic health records\nquality assurance: prescripton and use"
  },
  {
    "objectID": "guide/literature.html#ab-x-ehr",
    "href": "guide/literature.html#ab-x-ehr",
    "title": "Kundan Kumar",
    "section": "AB x EHR",
    "text": "AB x EHR\nMoehring 2021, EHR identify AB use among hospitalized patients\nReenggli 2021: assess EHR conversion into AB stewardship indicators\nCairns 2021: integrate AB stewardship with EHR in australia\nJenkins 2022: AB stewardship using electornic prescribing systems, review of intervention and outcome measures\nKuijpers 2024, excessive length of AB duration for HAI, support AB stewardship high relevance"
  },
  {
    "objectID": "guide/literature.html#ehr-doctor-experience-system-design",
    "href": "guide/literature.html#ehr-doctor-experience-system-design",
    "title": "Kundan Kumar",
    "section": "EHR doctor experience, system design",
    "text": "EHR doctor experience, system design\nOstrer 2023, real time benefit tools must be designed to serve both clinicians and patients. mentions burden\nKawamoto 2019 Association of EHR add-on app for neonatal bilirubin management, physician efficiency and care quality. Good system and add-on can save clinician time and improve patient care.\nTsai 2020, EHR implementation, barriers to adoption and use high relevance\nOverhage 2020, time spent using EHR\n\nburnout\nTajirian 2020, influence of EHR on physician burnout high relevance\nKhairat 2020, EHR use with physician fatigue and efficiency\nKroth 2019, factors of EHR design and use for physician stress and burnout high relevance\nMelnick 2020, EHR usability, task load and burnout high relevance\nHilliard 2020, factors associated with burnout\nMore on this topic\nhttps://www.sciencedirect.com/science/article/abs/pii/S0897189718301356\nhttps://journals.sagepub.com/doi/full/10.1177/20552076231220241\nhttps://jamanetwork.com/journals/jamanetworkopen/article-abstract/2778909\n\n\nDesign\nGascon 2013 the process of designing a EHR lab request module\nTorres 2017, effect of EHR design on documentation and compliance high relevance"
  },
  {
    "objectID": "guide/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "href": "guide/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "title": "Kundan Kumar",
    "section": "EHR data quality, use for prediction model development",
    "text": "EHR data quality, use for prediction model development\n\nerror\nbell 2020\nhttps://www.sciencedirect.com/science/article/abs/pii/S2213076420300439\nPaper, comparison of error reporting systems. 82% undetected, 90% had no corresponding incident report, some errors not reported at all\nWestbrook 2020, changes in medication administration error rates, associated with EHR\nGildon 2019, impact of EHR on prescribing errors in pediatric clinics. introduction is relevant for pointing out the need for well designed system\nGinzburg 2018, use clinical decision support within EHR to reduce incorrect prescribing for acute sinusitis. shows prompt for physicians to fill in reason for why AB is required high relevance\n\n\nbias\nGianfrancesco 2018, potential biases in ML algorithms using EHR data. Existing EHR disparities should not be amplified by thoughtless or excess reliance on machines.\nAgniel 2018, biases in EHR due to process within the healthcare system. healthcare processes must be addressed in the analysis of observational health data, context is important. high relevance\nBower, biases in EHR based surveillance of cardiovascular disease risk\nHarding2024, addressing common sources of bias in type 2 diabetes following covid, using EHR\nBica 2020, current and future methods to address underlying challenges from EHR to treatment effects mid relevance to manuscript, but important to myself\nDesai 2020, prediction using admin EHR, hear failure"
  },
  {
    "objectID": "guide/literature.html#ab-stewardship-not-so-relevant",
    "href": "guide/literature.html#ab-stewardship-not-so-relevant",
    "title": "Kundan Kumar",
    "section": "AB stewardship (not so relevant)",
    "text": "AB stewardship (not so relevant)\nRenggli 2021: consumption of anti-meticillin resistant staphylococcus aureus antibiotics in Swiss hospitals is associateed with antibiotic stewardship measures\nVaughn 2021: AB overuse and stewardship at hospital discharge\nGraber 2019: electronic tools to decrease AB use"
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar\n  \n\n  \n  \nHi! I’m Kundan Kumar, a Ph.D. candidate in Computer Science at Iowa State University. My research focuses on creating intelligent and adaptable AI systems for next-generation cyber-physical infrastructure, integrating deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), and computer vision.\nI develop safety-critical DRL frameworks that incorporate domain knowledge and uncertainty, enabling reliable decision-making in complex environments. Recent projects include exploring transfer learning and enhancing adversarial resilience across systems. I also create LLM-integrated simulation frameworks for autonomous systems, combining perception, trajectory planning, and natural language reasoning.\nOutside of research, I share insights on Substack and YouTube. I enjoy cooking and ice skating 🛼 in my free time.\n\n\n\n\n\n\nProjects, Notes, and Interests\n\n\n\n  \n    \n      LLM Reasoning & Agents\n      Agentic workflows (LangChain/LangGraph), tool-use, memory, retrieval, planning & reflection loops for robust multi-step reasoning.\n    \n  \n\n  \n    \n      Statistical ML\n      Uncertainty quantification, probabilistic modeling, and data-driven inference in dynamic environments.\n    \n  \n\n\n  \n    Autonomous Perception & Control\n    Vision-based perception (detection, segmentation, sensor fusion) integrated with learning-based control and trajectory planning for autonomous systems.\n  \n\n\n\n\n\n\n\nExplore My Work\n\n\nBlog\n\n\n\n\n\n\n\n\nHow to handle class-unbalanced data?\n\n\n\n\n\nNo matching items\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\nTransfer Learning in Deep Reinforcement Learning for Scalable VVC in Smart Grids\n\n\nTransferring knowledge from one grid to another grid\n\n\n\n\n\nNo matching items\n\nSee all →\n\n\nPublications\n\n\n\n\n\n\n\n\nBayesian-Optimized Bidirectional Long-Short-Term Memory network for Wind Power Forecasting with Uncertainty Quantification\n\n\n\n\n\nyear\n\n\n2025\n\n\n\n\n\n\n\n\nNo matching items\n\nSee all →\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\nCollaboration and Competition\n\n\n\n\n\nNo matching items\n\nSee all →\n\n\n\n\nNews Highlights\n\n\n\n\n\n\n[Sep 2025]\n\n\nOur paper on Bayesian-Optimized Bidirectional Long-Short-Term Memory network for Wind Power Forecasting with Uncertainty Quantification has been accepted to  Journal on Electric Power Systems Research 2025.\n\n\n\n\n[Jul 2025]\n\n\nSelected for the Cohere Machine Learning Summer School, hosted by Cohere Labs.\n\n\n\n\n[Mar 2025]\n\n\nOur paper on Advanced Semi-Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems has been accepted to IEEE PES General Meeting 2025."
  },
  {
    "objectID": "projects/nor_mortality/index.html",
    "href": "projects/nor_mortality/index.html",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "projects/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "projects/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "projects/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "projects/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "projects/dl.html",
    "href": "projects/dl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/dan/index.html",
    "href": "projects/dan/index.html",
    "title": "Data Apothecary’s Notes",
    "section": "",
    "text": "(This is my own note-taking system using quarto)\n\nAbout the notes\nData Apothecary’s Notes is a note-taking repository for modern data science skills with a focus on drug development and clinical trials. Content will be gradually added while I learn the topics. Therefore, it is by no means a complete guide by the time you read it!\nI try to organize the content in a modular way. I think these should cover the important aspects in which a data scientist / modern statistician should know.\n\nstudy design\ninference\nmodels\nreporting\nprogramming\n\n\n\nWhy quarto\nIn short, quarto has the advantage of making a very well structured website with code chunks easy. No more worry about scattered notes in different folders - put them together, render it so you can find your notes quickly!"
  },
  {
    "objectID": "projects/noreden/index.html",
    "href": "projects/noreden/index.html",
    "title": "Noreden",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "projects/ehr/index.html",
    "href": "projects/ehr/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Ggehr.” https://kundan-kumarr.github.io/projects/ehr/."
  },
  {
    "objectID": "projects/stat.html",
    "href": "projects/stat.html",
    "title": "Statistics",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/ehr_20221013_ml_icu/index.html",
    "href": "talks/ehr_20221013_ml_icu/index.html",
    "title": "Machine Learning in Intensive Care Units",
    "section": "",
    "text": "A 45 minutes trial lecture to fulfill the requirement of my PhD degree."
  },
  {
    "objectID": "talks/rstats_20240613_teaching/index.html",
    "href": "talks/rstats_20240613_teaching/index.html",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "",
    "text": "Time and place: June 13 2024. Online\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "talks/rstats_20240613_teaching/index.html#about-the-topic",
    "href": "talks/rstats_20240613_teaching/index.html#about-the-topic",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "About the topic",
    "text": "About the topic\nThe Department of Biostatistics at University of Oslo offer statistics courses at different levels for medical students and PhD candidates with clinical backgrounds. The courses were traditionally taught with a focus on theory instead of data analysis, where SPSS and STATA were the tools of choice.\nSince 2023 spring semester, we have been gradually transforming some of our statistics courses into R, using Quarto course websites and Carpentries style live-coding instruction. With new Quarto tools (such as WebR) we also added interactivity in the code blocks. So far we have transformed two courses with over 100 students who have almost no programming experience. We have observed impressive progress in the skill development, and received significantly more positive feedback when it comes to statistics education.\nIn this talk, I would like to share our experience on the successes and challenges throughout the process. Looking back, is it cost-effective? Definitely. Can we do better in the future? Almost surely. If you are also planning to adopt new technology in your teaching activities, join us to learn more about what you can do to make the transition happen!\nCourse website can be accessed here"
  },
  {
    "objectID": "talks/community_20240710_camis/index.html",
    "href": "talks/community_20240710_camis/index.html",
    "title": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations",
    "section": "",
    "text": "2024.7.8-11, Salzburg, Austria. Conference link: UseR!\nStatisticians using multiple softwares (SAS, R, Python) will have found differences in analysis results that warrant further justification. Whilst some industries may accept results not being the same as long as they are “close”, the highly regulated pharmaceutical industry would require an identical match in results. Yet, discrepancies might still occur, and knowing the reasons (different methods, options, algorithms etc) is critical to the modern statistician and subsequent regulatory submissions.\nIn this talk I will introduce CAMIS: Comparing Analysis Method Implementations in Software. https://psiaims.github.io/CAMIS/ It is a joint-project between PHUSE, the R Validation Hub, PSI AIMS, R consortium and openstatsware. The aim of CAMIS is to investigate and document differences and similarities between different statistical softwares such as SAS and R. We use Quarto and Github to document methods, algorithms and comparisons between softwares through small case studies, and all articles are contributed by the community. In the transition from proprietary to open source technology in the industry, CAMIS can serve as a guidebook to navigate this process.\n\nkeywords: cross industry collaboration, multi-lingua, open-source, quarto"
  },
  {
    "objectID": "talks/ehr_20240918_betterehr/index.html",
    "href": "talks/ehr_20240918_betterehr/index.html",
    "title": "One step closer to better Electronic Health Records data",
    "section": "",
    "text": "Real-World Data (RWD) like Electronic Health Records (EHR) is crucial for understanding drug usage and various treatments and generating Real-World Evidence (RWE). Risk prediction has been a major application where EHR is used, and there is now a shift towards causal inference, which requires data of even higher quality. Patients undergo treatments (drugs, procedures) at various times during their hospital stays, yet the data being recorded are messy and error-prone for various reasons. Analysts spend significant amount of time to sit together with clinicians to identify and understand abnormal records, and unfortunately this process is challenging to automate.\nThis talk will use an example on antibiotics prescription and use at a Nordic hospital to illustrate how some EHR systems can improve for better clinical decision-making and better data for research. I will also introduce a pilot R package (ggehr) that facilitates visual exploration of EHR data, and how it can help reconstruct patient journeys and enable analysts to perform effective quality control."
  },
  {
    "objectID": "talks/ph_20230330_sp/index.html",
    "href": "talks/ph_20230330_sp/index.html",
    "title": "Public health surveillance and reporting",
    "section": "",
    "text": "Time and place: Mar. 30, 2023 12:00 PM–1:00 PM\nHybrid: Georg Sverdrups hus and Zoom\nEvent page"
  },
  {
    "objectID": "talks/ph_20230330_sp/index.html#about-the-topic",
    "href": "talks/ph_20230330_sp/index.html#about-the-topic",
    "title": "Public health surveillance and reporting",
    "section": "About the topic",
    "text": "About the topic\nSituational awareness is key to fast response during a public health emergency, such as COVID-19 pandemic. However, making disease surveillance reports that cover different geographical units for various metrics and data registries is both resource intensive and time consuming. Open source tools such as R packages, GitHub and Airflow can make this process automatic, reproducible and scalable.\nEvery day during the pandemic, Sykdomspulsen team at the Norwegian Institute of Public Health (FHI/NIPH) fetched data from more than 15 data sources, cleaned, censored datasets and carried out a wide range of statistical analyses. Over 1000 situational reports containing automated graphs and tables were produced before breakfast time.\nGrab you matpakke and join us for a presentation from Chi Zhang about how Sykdomspulsen team used and developed open source software to make public health surveillance and reporting more efficient, followed up by a discussion on the benefits and concerns of making these data public. We will end with an open Q&A session as usual!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar\n  \n\n  \n  \nHi! I’m Kundan Kumar, a Ph.D. candidate in Computer Science at Iowa State University. My research focuses on creating intelligent and adaptable AI systems for next-generation cyber-physical infrastructure, integrating deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), and computer vision.\nI develop safety-critical DRL frameworks that incorporate domain knowledge and uncertainty, enabling reliable decision-making in complex environments. Recent projects include exploring transfer learning and enhancing adversarial resilience across systems. I also create LLM-integrated simulation frameworks for autonomous systems, combining perception, trajectory planning, and natural language reasoning.\nOutside of research, I share insights on Substack and YouTube. I enjoy cooking and ice skating 🛼 in my free time.\n\n\n\n\n\n\nOther Research Interests\n\n\n\n\n  \n    \n      LLM Reasoning & Agents\n      Agentic workflows (LangChain/LangGraph), tool-use, memory, retrieval, planning & reflection loops for robust multi-step reasoning.\n    \n  \n\n\n  \n    \n      Statistical ML\n      Uncertainty quantification, probabilistic modeling, and data-driven inference in dynamic environments.\n    \n  \n\n\n  \n    Autonomous Perception & Control\n    Vision-based perception (detection, segmentation, sensor fusion) integrated with learning-based control and trajectory planning for autonomous systems.\n  \n\n\n\n\n\n\nExplore My Work\n\n\nBlogs\n\n\n\n\n\n\n\n\nHow to handle class-unbalanced data?\n\n\n\n\n\nNo matching items\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\nTransfer Learning in Deep Reinforcement Learning for Scalable VVC in Smart Grids\n\n\nTransferring knowledge from one grid to another grid\n\n\n\n\n\nNo matching items\n\nSee all →\n\n\nPublications\n\n\n\n\n\n\n\n\nBayesian-Optimized Bidirectional Long-Short-Term Memory network for Wind Power Forecasting with Uncertainty Quantification\n\n\n\n\n\nyear\n\n\n2025\n\n\n\n\n\n\n\n\nNo matching items\n\nSee all →\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\nCollaboration and Competition\n\n\n\n\n\nNo matching items\n\nSee all →\n\n\n\n\nNews Highlights\n\n\n\n\n\n\n[Sep 2025]\n\n\nOur paper on Bayesian-Optimized Bidirectional Long-Short-Term Memory network for Wind Power Forecasting with Uncertainty Quantification has been accepted to  Journal on Electric Power Systems Research 2025.\n\n\n\n\n[Jul 2025]\n\n\nSelected for the Cohere Machine Learning Summer School, hosted by Cohere Labs.\n\n\n\n\n[Mar 2025]\n\n\nOur paper on Advanced Semi-Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems has been accepted to IEEE PES General Meeting 2025."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Kundan Kumar",
    "section": "Recent Posts",
    "text": "Recent Posts\nCheck out the latest  Papers ,  News ,  Events , and  More »"
  },
  {
    "objectID": "blog/blog_20230103_blogdown2quarto/index.html",
    "href": "blog/blog_20230103_blogdown2quarto/index.html",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "href": "blog/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "href": "blog/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "Time to try Quarto",
    "text": "Time to try Quarto\nNow that I’ve finally completed the more pressing tasks in October 2022, I can catch up to the cool kids on twitter: create a website with Quarto!\nThere were quite a lot of discussions about Quarto in the summer 2022. I wasn’t following the discussions closely, but I remember there were quite a few talks in the Rstudio conference this year. Then more and more people switched to Quarto on Twitter. Then people I know also switched to Quarto. What’s the fuzz about?\nMy experience with Quarto is focused on websites. I have not tried other forms of publishing. So far I have created:\n\na workshop website for my colleagues\na personal website (the one you are reading right now)\nan R package (qtwAcademic)that wraps three Quarto website templates for beginners\n\nHere are a few things I like about Quarto. Given that I’m not very experienced in front-end development, these comments are going to be about ease-of-use and design, rather than the technicalities.\n\nClean look for both personal and workshop/courses\nWhen I was using “academic” template in blogdown, I liked the structure of the site: projects, talks, blog, softwares and publications sections are clearly displayed at the top. What I didn’t like is that the default homepage was a very long single page; yet its customisation wasn’t the easist. Other templates were either too simple (for blog only), or more suitable for image display (photography projects). I wanted a website that keep the good structure of “academic”, which is quite suitable for academics (hence the name); while keeping each section independent.\nWith distill I could achieve the structure I wanted; but I didn’t enjoy it too much as a personal website (at least it wasn’t as flexible as Quarto). distill is still pretty decent for organisations or documentation site.\nWith Quarto, I can achieve the desired looks for not only a personal website (with or without blogs), but also a workshop, event or even course website. This is fantastic! The top, sidebar or hybrid navigation makes the site structure very clear, especially when there are lots of content. As an aspiring lecturer at university, this is really One Quarto Rules Them All.\n\n\nFlexible yet not overwhelming\nAs I mentioned above, hacking “academic” in blogdown was not that easy - simply because there were too many folders that you are not actually supposed to modify. It was confusing to know what to change in order to achieve the desired output, and multiple folders were having the same names, making it very challenging for beginners. Ironically, this is usually the first template beginners start with!\nThat’s why I immediately fell for Quarto: you only need 4 components to make a decent minimalistic website work:\n\n_quarto.yml to control the overall layout\nindex.qmd at the root folder to control the homepage\nabout.qmd for some basic information about the creator or the website\nproject.qmd for projects or any other content that the creator wants to display\n\nThe way that _quarto.yml clearly specifies the .qmd files really helps beginners to understand where things are. This has been extremely useful for me when I wanted to learn how people made their website by reading the source code - I could understand exactly where to find the information I needed. The clear structure greatly helps the creators themselves, and also those who want to learn.\n\n\nGreat community\nRstats people have a great community. I wouldn’t be able to make my site the way I wanted if people haven’t been sharing their works. I have learned a lot by reading the source code by Dr Emi Tanaka, Dr David Schoch, Bea Milz, Prof Mine Cetinnkaya-Rundel’s STA 210 - Regression Analysis course.\nI also made my own R package that wraps three templates to create Quarto websites that are frequently used by academics, qtwAcademic. In the following days I plan to write up more detailed explanations on how to use the package, along with some new features."
  },
  {
    "objectID": "blog/readnotes_2023010x_preventable_sridhar/index.html",
    "href": "blog/readnotes_2023010x_preventable_sridhar/index.html",
    "title": "Preventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar",
    "section": "",
    "text": "Advice on some measures to prepare for the next pandemic\n(From Five ways to prepare for the next pandemic by Prof. Devi Sridhar)\n\nMonitor zoonoses. Identify patogens with pandemic potential, regulate better wet markets\nSequence globally. Investment in genetic-sequencing capability\nStrengthen manufacturing. Vaccine inequality, fragility of vaccine production. Private and public sector work together - vaccine research, production and distribution.\nVaccine preparedness. For known diseases (e.g. influenza), invest in vaccines that protect against a wide range of variants. New technology and research for unknown threats\nStop the spread (long enough for the vaccines) to save lives."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing\n\n\n\n\nThere should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients.\n\n\n\n\n\nPassive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy.\n\n\n\n\nThe effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor\n\n\n\n\nInfodemic\n(these two chapters are highly technical, and they deserve a separate note)\n\n\n\nDisaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting\n\n\n\n\nThe impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal.\n\n\n\n\n\nInvest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "There should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Passive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "The effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Infodemic\n(these two chapters are highly technical, and they deserve a separate note)"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Disaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "The impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Invest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html",
    "href": "blog/technotes_20230111_deployqt/index.html",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "href": "blog/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#create-quarto-project",
    "href": "blog/technotes_20230111_deployqt/index.html#create-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "2. Create Quarto project",
    "text": "2. Create Quarto project\nThis can be a website, a book (a specific type of website) or something else.\nTest compilation by quarto render, or click the Render button."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "href": "blog/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "3. Configure Quarto project",
    "text": "3. Configure Quarto project\nIn _quarto.yml, change the project configuration to use docs as the output-dir:\nproject:\n  type: website\n  output-dir: docs\n\n\n\n\n\nThen add .nojekyll to the root of the repository. Can do this by (in terminal)\ntouch .nojekyll\nPush everything to your repository."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#configure-github-pages",
    "href": "blog/technotes_20230111_deployqt/index.html#configure-github-pages",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "4. Configure GitHub Pages",
    "text": "4. Configure GitHub Pages\nGo to Settings &gt; Pages, publish from docs of the main branch.\n\n\n\n\n\nCan check GitHub Action and deployment status.\n\n\n\n\n\n\n\n\n\n\nAfter the deployment is successful, go to view deployment, and a successful website should be published."
  },
  {
    "objectID": "blog/readnotes_20240606_bad_pharma/index.html",
    "href": "blog/readnotes_20240606_bad_pharma/index.html",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "href": "blog/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html",
    "href": "blog/technotes_20230228_clinreport_part3/index.html",
    "title": "How to handle class-unbalanced data?",
    "section": "",
    "text": "Class imbalance is a technique to handle the unbalanced data in the data-sets to build a reliable ML model and avoid the model for poor generalization."
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "title": "How to handle class-unbalanced data?",
    "section": "Principles and tools",
    "text": "Principles and tools\nReproducibility: Git (code versioning), dependencies (renv for r package dependencies, Docker for system dependencies)\n\nClean code\nCode comments: not recommended! Better to write code in a way that does not need additional comments.\nDRY: don’t repeat yourself (principle of software development), avoid copy and paste everywhere.\nSRP: single-responsibility prinicple, a function should do one thing: either plot a chart, saves a file, changes variables etc, but not all.\nNaming conventions\n\nReserve dots (.) for S3 methods (print.patient)\nReserve CamelCase for R6 classes or package names (OurPatients)\nUse snake cases (all_patients) for function names and arguments, use verb noun pattern (plot_this())\n\n\n\nCode smells\nA function might be too large: break into smaller ones (e.g. could fit in one screen)\nA function violates SRP: break into smaller ones, and be explicit in what result it is expected to return\nA function with multiple arguments: the scenarios to be tested increase rapidly. Recommended to minimize number of critical function arguments, and break the function into smaller ones.\nBad comments in the code: drop the unnecessary, unclear, outdated comments, write code that are self-explanatory.\n\n\nDevelopment workflow\nCode refactoring: change existing code without its functionality\nTDD: Test-Driven Development\n\nstart with writing a new (failing) test\nwrite code thtat passes the nenw tetst\nrefactor the code\nand repeat\n\nBenefits: your code is covered by tests; you think of testing scenarios first; “fail fast” - can immediately repair the code; more freedom to refactor (improve) the code.\nHow to test\n\nautomatically: CI/CD, after pushing Git commits\nmanually:\n\nrun all unit tests in the package (Build / Test package)\nrun tests in a selected test file (Run Tests)\nrun a single test in Rstudio console\n\n\nHow to check\n\nR CMD CHECK"
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "title": "How to handle class-unbalanced data?",
    "section": "Writing robust statistical software",
    "text": "Writing robust statistical software\nImplement complext statistical methods such that the software is reliable, and includes appropriate testing to ensure high quality and validity and ultimately credibility of statistical analysis results.\n\nchoose the right method and understand them\nsolve the core implementation problem with prototype code\n\nNeed to try a few different solutions, compare and select the best one. Might also need to involve domain experts.\n\nspend enough time on planning the design of the R package\n\nDon’t write the package right away; instead define the scope, discuss with users, and design the package.\nStart to draw a flow diagram, align names, arguments and classes; write prototype code.\n\nassume the package will evolve over time\n\nPackages you depend on will change; users will require new features\nWrite tests\n\nunit tests\nintegration tests\n\nMake the package extensible\n\nconsider object oriented package designs\ncombine functions in pipelines\n\nKeep it manageable\n\navoid too many arguments\navoid too large functions"
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#key-components",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#key-components",
    "title": "How to handle class-unbalanced data?",
    "section": "Key components",
    "text": "Key components\n\nDependency management\nInstall dependencies (system/OS level; R packages)\n\nSet repos (can be specified in options()) to e.g. CRAN, BioConductor\nrenv\ncontainer with dependencies pre-installed\n\n\n\nStatic code analysis\n\nLinting (for programmatic and syntax errors) via lintr package\nCode style enforcement via styler package\nSpell checks identifies misspelled words in vignettes, docs and R code via spelling package\n\n\n\nTesting\n\nR CMD build builds R packages as a installable artifact\nR CMD check runs 20+ checks including unit tests, reports errors, warnigns and notes\nTest coverage reports with covr, checks how many lines of code are covered with tests\nR CMD INSTALL tests R package installation\n\n\n\nDocumentation\nAuto-generated docs via Roxygen and pkgdown\n\n\nRelease and deployments\nRelease artifacts and deployments to target systems\n\nChangelog (features, bug fixes) in the NEWS.md\nRelease: create the package with R CMD build. Validation report with thevalidatoR\nPublishing: CRAN, BioConductor"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html",
    "title": "LLM Reasoning and Planning",
    "section": "",
    "text": "Unlocking Advanced AI: Prompting for LLM Reasoning and Planning\nAre you ready to unlock the advanced capabilities of Large Language Models (LLMs) and elevate your interaction with artificial intelligence? This learning path is designed to equip you with powerful prompting techniques essential for building sophisticated AI agents. Mastering these skills allows you to guide LLMs through complex, multi-step tasks, improving the accuracy, relevance, and utility of their outputs for real-world applications.\nLarge Language Models have created a new computing paradigm largely based on how we write prompts. This learning journey is focused on how advanced prompting techniques enable AI applications to become AI agents. By the end of the journey, you’ll understand the mechanics and possess the skills to apply prompting techniques to agentic AI systems. The Need for Agentic AI: Debugging Complex Code\nImagine you’re tasked with building an automated system to help developers debug complex code. Simply feeding the buggy code and the error message into a standard LLM often results in generic suggestions that miss the specific context of the larger project. The LLM might suggest superficial fixes or even introduce new bugs.\nHow can you create an AI assistant that acts more like an experienced senior developer – one that can break down the problem, hypothesize potential causes, decide which parts of the code to inspect, integrate information from different files, update tests, and even learn from its failures? This requires moving beyond single-shot prompts to build a system capable of multi-step reasoning, planning, and execution – precisely the prompting techniques we’ll master.\nWhy is a single, simple prompt often insufficient for guiding an LLM through a complex or multi-step task? Correct! A single prompt is like giving a single instruction, whereas complex tasks require an ongoing dialogue. To act like an “experienced senior developer,” the AI needs to be guided through a process of breaking down the problem, investigating, and integrating information—steps that require a sequence of prompts to manage the plan and its execution effectively.\nThink about a complex, multi-step project you’ve recently worked on, either personally or professionally. This could be anything from planning a detailed event, to troubleshooting a tricky problem at home or work, or even tackling a challenging creative endeavor.\nFoundational Understanding of LLMs: You should know what a Large Language Model (LLM) is at a conceptual level, including its general capabilities (e.g., text generation, understanding) and the basic idea of using “prompts” to interact with it.\nBriefly describe the project and list 3-4 distinct steps you had to take to move it forward.\nNow, imagine you were trying to get a standard AI assistant (like a basic chatbot) to complete the entire project for you using only a single request or prompt.\nBased on your experience with AI, at which specific step do you predict the AI would most likely fail, misunderstand, or give a generic, unhelpful response? Why do you think that particular step would be the breaking point for a single-prompt approach?\n\nprobabilistic In the context of interacting with a Large Language Model (LLM), what is a ‘prompt’?\n\nThe input text or instructions probide to guide its response Large Language Model (LLM)? A ML model trained on the vast amounts of text data to understand, sumamrize , generate and predict context\nThroughout this course, you’ll explore:\nThe world of AI Agents, understanding their core components and how they reason, plan, and interact with their digital environments.\n\nThe art and science of advanced prompting techniques, mastering how to instruct LLMs with precision and nuance.\n\nCrafting specialized personas using role-based prompting to make AI outputs more targeted and contextually relevant.\n\nUnlocking problem-solving abilities using Chain-of-Thought (CoT) to guide the LLM's reasoning process and ReAct (Reason + Act) to enable LLMs to use tools and take actions.\n\nThe process of prompt instruction refinement, learning to systematically analyze and adjust your prompts for optimal performance.\n\nBuilding multi-step agentic workflows by chaining prompts together, allowing AI to tackle more complex tasks.\n\nPrompt Chaining & Feedback Loops: Building robust, multi-step workflows that allow an AI to tackle complex tasks and even improve its own work based on feedback.\nEnd of Course Project: Agentsville Trip Planner\nAt the end of the course, you’ll attempt to build the “Agentsville Trip Planner Assistant” project, a smart agent that can take a complex request and see it through to completion. This is your opportunity to see just how powerful prompting can be in agentic AI. You will apply all of the individual skills you learned to create a truly capable system.\nWe want to wish you the very best of luck! You’re about to start a journey to harness the incredible power of Large Language Models. Get ready to move beyond basic interactions and learn how to architect and guide AI.\nBy the end of this journey, you will have acquired the skills to:\nExplain AI systems that utilize LLMs for sophisticated reasoning and planning capabilities.\nMaster a range of advanced prompting strategies to elicit precise behavior and information from LLMs.\nSystematically optimize and refine your prompts, transforming general AI responses into highly specific and useful outputs.\nConstruct multi-step AI workflows that can handle complex, real-world challenges.\nImplement mechanisms for validation and iterative improvement, leading to more reliable AI agents.\nUltimately, transform generic Large Language Models into specialized, powerful tools tailored to solve intricate problems across various domains.\nThis course will both challenge you and equip you with the cutting-edge skills needed to innovate in the rapidly evolving field of artificial intelligence. We’re excited to see what you’ll learn and, eventually, what you’ll build. Good luck!"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html#characterization",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html#characterization",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Characterization",
    "text": "Characterization\n\nWhat happened to the patients.\n\nChapter 11 Characterization\nTypical characterization questions:\n\nHow many patients…?\nHow often does…? What proportion of patients …?\nWhat is the distribution of values for …?\nWhat is the median length of exposure for patients on …?\nOther drugs the patient is using?\n\nDesired output:\n\ncount, percentage\naverages and other descriptive statistics\nprevalence, incidence rate\nrule-based phenotype\ndrug utilization, adherence, treatment pathways, line of therapy\ndisease natural history, co-morbidity profile"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Population-level estimation",
    "text": "Population-level estimation\n\nWhat are the causal effects\n\nChapter 12 Population-level Estimation\nTypical questions:\n\nWhat is the effect of …?\nWhich treatment works better?\nWhat is the risk of X on Y?\nWhat is the time-to-event of …?\n\nDesired output:\n\nRR, HR, OR\nAssociation, correlation\nATE, causal effect"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Patient-level prediction",
    "text": "Patient-level prediction\n\nWhat will happen to A?\n\nChapter 13 Patient-level Prediction\nTypical questions:\n\nWhat is the chance that this patient will…?\nWho are the candidate for…?\n\nDesired output:\n\nprobability for an individual\nprediction model\nhigh/low risk groups\nprobabilistic phenotype"
  },
  {
    "objectID": "blog/technotes_20231001_qt_webr/index.html",
    "href": "blog/technotes_20231001_qt_webr/index.html",
    "title": "Use WebR in your existing quarto website",
    "section": "",
    "text": "WebR is the new hot topic in the R community. Coupled with Quarto, you can run R code interactively in a web browser. This is achieved with the great quarto extension, Code developed by James J Balamuta.\nIn the Paper, Code and YouTube, James introduced how to make a webR empowered quarto document. It is simple enough, and you can make it work quite smoothly."
  },
  {
    "objectID": "blog/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "href": "blog/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "title": "Use WebR in your existing quarto website",
    "section": "When your render gets stuck",
    "text": "When your render gets stuck\nBut there is a twist. This works perfectly fine with a new quarto project, where no output-dir is specified yet. When I tried to replicate the same thing for my existing quarto website (with output-dir: docs so that I could deploy it with GitHub Pages), my rendered html file got stuck:\n\nIf you read the troubleshooting documentation, you’ll see that it’s a problem with the two js files. This agrees with what Rstudio Background Jobs tells us.\n\nI moved the two files (manually..) around, then render again, nothing changed.\n\nSolution: set channel-type option\nThis is a solution provided by the authors, although I don’t quite understand what it did, but it did the magic. (Thanks to Linh’s help!)\nThis is where you specify this option.\n\nRender again, now it works! WebR status turns green, and I can run code interactively in the browser."
  },
  {
    "objectID": "blog/blog_20230104_qtwAcademic/index.html",
    "href": "blog/blog_20230104_qtwAcademic/index.html",
    "title": "qtwAcademic: a quick and easy way to start your Quarto website",
    "section": "",
    "text": "qtwAcademic stands for Quarto Websites for Academics, which provides a few Quarto templates for Quarto website that are commonly used by academics.\nThe templates are designed to make it quick and easy for users with little or no Quarto experience to create a website for their personal portfolio or courses. Each template is fully customizable once the user is more familiar with Quarto.\nRead more about the package here.\nMore details about the package is being written …"
  },
  {
    "objectID": "blog/technotes_20231018_qt_styling/index.html",
    "href": "blog/technotes_20231018_qt_styling/index.html",
    "title": "Styling your quarto project",
    "section": "",
    "text": "Useful references:\n\nTalk by Emil Hvitfeldt on Styling and Templating Quarto Documents"
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html",
    "href": "blog/technotes_20230222_clinreport_part2/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "href": "blog/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Agile mindset and DevOps practices",
    "text": "Agile mindset and DevOps practices\n\nData science as a new way of thinking\nNew way of working means\n\nleverage standards and automation (CI/CD)\nadopt new data types quickly, reusing data for multiple purposes, pooling data, data marts\nopen-sourcing and collaborating cross pharma (small, readable, self-tested code)\ncoding for reusability, moving away from single-use programs\nrapidly re-arranginng re-usable components to meet analytical need at hand\n\nData scientist need to have hard skills, such as\n\nSAS, R, Python, JS, bash\ncloud, containers\nCI/CD tools\nvisualisation\nknowledge of various data types\n\nand also soft skills:\n\ncollaborative and inclusive\ntransparent and practical\ncreative and proactive\nasking the right questions\nable to wear many hats, be more flexible and resilient\n\n\n\nAgile\nProject management; a mindset: uncover better ways of working, by doing and helping others do it.\n1st principle: highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nImplementations: Kanban, Scrum, Lean, Extreme programming\nTools:\n\nbacklog\nkanban board (not started, in progress, done)\nWIP (work in progress limit)\nprogress measures: e.g. team velocity\n\n\n\nDevOps\nIncrease efficiency by improving the connection between Dev (software development) and Ops (IT operations).\nThe goal is continuous delivery and continuous improvement.\nPractices:\n\nmodular architecture\nversion control\nmerge into trunk daily\nautomated and continuous testing, continuous integration\nautomated deployment\n\n\nDevOps in clinical reporting\nRisks around production run:\n\nare all dependencies in production?\nwas all quality control completed and successful?\nis all documentation complete?\nwas the transfer to eDMS correct and successful?"
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html#version-control",
    "href": "blog/technotes_20230222_clinreport_part2/index.html#version-control",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Version control",
    "text": "Version control\nFeature branch (as opposed to master branch): one task per branch\nname feature branch: issue number and description\nEach issue should have a clear description, short and specific; instead of being long and overarching.\n\nWorkflow for clinical reporting\nRestraints of clinical deliveries: timing annd multiple deliveries; resourcing challenges\nMight need to choose between feature and GitFlow."
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "href": "blog/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Reproducible projects in R",
    "text": "Reproducible projects in R\nTo reproduce your work:\n\nGit (version control)\nR libraries\nWell structured projects\nUnderlying dependencies (e.g. operating systems, C++/C)\n\n\nWell structured projects\nClear names\nGood documentation\n\n\nR libraries and versions\nCheck session info; but not the most practical way.\nUse global libraries, .libPaths(), this gives you the path where all the packages are installed. Global libraries is useful when using a server for multiple R sessions, where they look for the packages in the same place.\nSolutions\n\nrenv package: makes each project in R self-contained.\nCheckpoint: project level library paths based on snapshots of CRAN\n\nUse Docker images! Saves R version, operating system, underlying dependencies"
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html",
    "href": "blog/blog_20230717_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Earlier this year (2023) I wrote a blog about my thoughts on the role of open source software in statisical education. Naturally, I advocate for more use of open source tools such as R/python in teaching introductory statistics to applied scientists. Nonetheless, how the material is taught will make a huge difference in the understanding and interest in the material.\nI was taught statistics in the classic way: lectures with tons of mathematical formulae and proofs, while programming and data analyses were left for students themselves to figure out. Those who were the fastest learners were the ones who already had a degree in computer science, which probably doesn’t sound surprising. I, for one, definitely struggled."
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "href": "blog/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Does statistics have to be daunting?",
    "text": "Does statistics have to be daunting?\nFor applied scientists in various fields, data analysis is a core task, and also a challenging one. You must have met clinicians or biologists who would love their data to be analysed yet don’t know how to. Yes, statistics and data skills can take some time to learn; but with the right method, they don’t have to be daunting. It is up to the educator to find a way that benefits the most students. An observation is that many researchers do not know or remember advanced math; yet do they need advanced math to grasp many fundamental statistical concepts?\nI believe that it is far more important and useful to teach basic IT skills and exploratory data analysis so that students can develop an understanding of their own data; rather than using a test blindly."
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "href": "blog/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Rebooting MF9130E classroom",
    "text": "Rebooting MF9130E classroom\nWhen I heard that the teaching team at Biostatistics Department, Faculty of Medicine was thinking about trying a novel pedagogical method on the MF9130E (2023 spring) class, I was more than excited to contribute. This is a PhD level course of 8 days long, offered three times a year (twice in Norwegian language). Students come from a variey of backgrounds in health and life sciences. Since this is an introductory course, the topics are broad rather than specialised.\nA few years ago, statistical software for the course made the transition from SPSS to Stata. To be more precise, students were introduced to, but not really explained to, or elaborated on how to use Stata proficiently. Why? The course is about statistics so only statistics is taught. Data skills such as manipulation are not part of statistics. \nWell, we will change that by starting to use R.\n\nThree open source musketeers\nR, quarto and GitHub the three musketeers in facilitating the transformation. We build a quarto course website where all the material are public, hosted with GitHub Pages. Having a course website is beneficial for students to have an overview of the course, in contrast to many scattered lecture notes and exercises to be downloaded.\nThe biggest advantage of using quarto is the rendered output from code. From a student’s perspective, it is reassuring to see the same result and plots using the data and code provided by the instructor. For the instructor, it is also convenient to see whether the code functions as expected. When we do not want to show the output, it is also very easy to suppress. We have created one copy with and one withtout rendered output as exercises, and are glad to see some students challenging themselves by attempting to solve the problems without solution.\nUsing Github and quarto together to build a course website is rather straightforward. I think the site structure is simple yet flexible enough to navigate. Collaboration across a small teaching team is also manageable. Github Pages was easy to set up, and changes made on the main branch is deployed within the minute. This proved to be useful in quite a few moments (where we had to replace some datasets or add some notice).\n\n\nThe Carpentries pedagogical model\nThe Carpentries is an organisation that teaches foundational coding and data science skills to researchers. I myself benefited from their workshop on version control and git taught at University of Oslo, and I think the traditional classroom could use some of the methods at these data science workshops.\nTo put simply, there are two things I tried with the course setup for MF9130E:\n\nLive coding demonstration, plenty of it\nSticky-notes flag and helper (teaching assistant) in class\n\nIn the live coding demonstration (which I was responsible for), I made sure that students were taught the most commonly used R commands for data manipulation and exploration. Quarto webpages on introduction to R, basic EDA, intermediate EDA have been created and guided through in class, mixed with statistical concepts and visualizations. Without knowing how your data looks like, blindly using statistical tests is dangerous - that is the motivation for doing so.\nWhether students feel supported can make a huge difference in their willingness to learn. Taking it slow at the beginning, and solve the problems on an individual basis can prevent early drop-outs, especially when programming and IT systems are involved. Naturally, when we don’t have helpers we can not help everyone; this is a limitation for this model. Students should be encouraged to help each other.\n\n\nLet them explore\nThe last important change in the class was to give time to students themselves. We reduced the lecturing on theory and computation, and added time for practice and discussion. The guided practice with live demo also came with solution and comments, so students could explore at their own pace. We left plenty of time for them to ask questions, and made sure most people can follow the exercises."
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html#how-did-it-go",
    "href": "blog/blog_20230717_teaching/index.html#how-did-it-go",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "How did it go?",
    "text": "How did it go?\nAfter the 8 day course we carried out a small survey among the ~50 students in the spring 2023 class. Student backgrounds are diverse, they work on lab data, clinical data or observational/epidemiological data:\n\nobservational study on humans 36%\nRCT 18%\nin vitro research 15%\nothers are in animal research, meta analysis or something else\n\nStatistical competency (method, software) among students are generally on the basic end. Over 75% of the cohort report themselves to have basic to very basic knowledge of statistics; 33% do not use any statistical software, around 45% have used SPSS or Stata. On the other hand, some students (7%) report to have advanced knowledge and have some R experience.\n\nSome feedback\nThis is the first time we do the course with R, live demo and put an emphasis on basic data manipulation and exploration - which means we do not have enough data, it is just an initial impression.\nHere’s what we have received. On the positive side, 86% find the course useful for their own PhD research. 75% felt they are able to use the correct methods for their analyses, which is quite encouraging. Most felt the examples and exercises were able to demonstrate the theory. Students have generally positive experience with the live demo, and find the instructors supportive. This is good!\nIn the meantime, it is only natural that some are dissatisfied (21%) in some ways. Common complaints are: R is not user friendly to absolute beginners; the leap from no software to a programming language is too big for some.\nAs for whether students have really mastered the knowledge intended, we do not have enough data to draw a conclusion. We do observe that the take home project show somewhat better understanding, but can not say for sure just yet.\nThis is a class with very diverse backgrounds, hence it is challenging to cater to everyone’s needs. Yet, we are satisfied with the trial-transformation with our introductory statistics class, and we plan to gradually implement more classes with R, and possibly hands-on practice (depending on capacity)."
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Research Scientist Interview Guide\n\n\n\n\n\n\nData science\n\n\nInterview Guide\n\n\n\nResearch Scientist Interview Guide \n\n\n\n\n\nJul 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Highlights: Positconf 2024\n\n\n\n\n\n\nData science\n\n\n\nMy picked talks for 2024 after Posit released the talks on YouTube \n\n\n\n\n\nNov 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUse Quarto, Make Friends: a two-year journey\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre\n\n\n\n\n\n\nNotes\n\n\n\nReading notes on some of the chapters. Book on Amazon: link \n\n\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: The Book of OHDSI - Data Analytics\n\n\n\n\n\n\nData science\n\n\nObservational data\n\n\n\nObservational Health Data Sciences and Informatics \n\n\n\n\n\nMay 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports with Quarto: R-Ladies Abuja Workshop\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal\n\n\n\n\n\n\nNotes\n\n\n\nReading notes on some of the chapters. Book on Amazon: link \n\n\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports with Quarto: R-Ladies DC Workshop\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStyling your quarto project\n\n\n\n\n\n\nWebsite\n\n\n\nLearning notes on how to customize a quarto project, such as website. \n\n\n\n\n\nOct 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUse WebR in your existing quarto website\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\nYou might get stuck when you try to add the trending webR to quarto extension in your website. This is one way to fix it. \n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Quarto reports improve understanding of soil health\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nagriculture\n\n\nsoil health\n\n\n\nCreating custom soil health reports with Quarto\n\n\n\n\n\nSep 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Highlights: Positconf 2023\n\n\n\n\n\n\nData science\n\n\n\nCuration of content to check out when I’ve got time \n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Highlights: CEN2023\n\n\n\n\n\n\nBiostatistics\n\n\n\nSelected summaries on the 5th Conference of the Central European Network of the International Biometric Society (IBS). \n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nShiny optimization of climate benefits from a statewide agricultural grant program\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nagriculture\n\n\nclimate\n\n\n\nDevelopment process of {WaCSE} shiny app\n\n\n\n\n\nAug 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\n\n\n\n\nQuarto\n\n\nEducation\n\n\n\nSome reflection on the experimental 8-day introductory statisics course with new teaching methods. Visit the course website here. \n\n\n\n\n\nJul 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Soil Health Initative and Climate Smart Estimator\n\n\n\n\n\n\nagriculture\n\n\nsoil health\n\n\nclimate\n\n\n\nOverview of the WA Soil Health Initative & WA Climate Smart Estimator {WaCSE} shiny app\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR package workflow\n\n\n\n\n\n\nRpkg\n\n\nRSE\n\n\n\nA step-to-step guide to make CRAN-worthy R packages \n\n\n\n\n\nMay 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping & Mapping {orcas} Encounters\n\n\n\n\n\n\nR\n\n\nweb scraping\n\n\nleaflet\n\n\n\nWeb scraping with {rvest} & mapping with {leaflet}\n\n\n\n\n\nApr 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPreventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar\n\n\n\n\n\n\nNotes\n\n\n\nReading notes on the book. Also serves as a collection of notes from Professor Devi Sridhar’s articles. \n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCourse review: making DS work for clinical reporting\n\n\n\n\n\n\nReporting\n\n\nData science\n\n\nRSE\n\n\n\nA review of the Coursera course provivded by Genentech and Roche, on “Making data science work for clinical reporting”. \n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 4\n\n\n\n\n\n\nClinical trial\n\n\nData science\n\n\nReporting\n\n\n\nThis is the Part 4 of a four-part course on Coursera. \n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 3\n\n\n\n\n\n\nClinical trial\n\n\nData science\n\n\nReporting\n\n\n\nThis is the Part 3 of a four-part course on Coursera. In this part, innerSource and OpenSource concepts are introduced, and R package development is discussed. \n\n\n\n\n\nFeb 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Shiny app and deploy to shinyapps.io\n\n\n\n\n\n\nShiny\n\n\nWebsite\n\n\n\nNotes on how to set up the free shinyapp.io to deploy a demo shiny app. \n\n\n\n\n\nFeb 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 2\n\n\n\n\n\n\nClinical trial\n\n\nData science\n\n\nReporting\n\n\n\nThis is the Part 2 of a four-part course on Coursera. In this part, agile and DevOps practices are introduced, along with version control with Git and reproducible R projects. \n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR package website with pkgdown\n\n\n\n\n\n\nRpkg\n\n\nWebsite\n\n\n\nA workflow that worked for me: when you have a few vignette documents, and want to display them nicely in a website format. \n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 1\n\n\n\n\n\n\nClinical trial\n\n\nData science\n\n\nReporting\n\n\n\nThis is the Part 1 of a four-part course on Coursera. In this part, there is an introduction to clinical trial phases, and motivation to share data. In addition, some terms (such as CDISC standard) have been introduced. \n\n\n\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOpen source reporting with R: clinical, public health, RSE and embrace the change\n\n\n\n\n\n\nReporting\n\n\nData science\n\n\nEducation\n\n\nRSE\n\n\n\nMy thoughts on the open source transition in pharma, public (health) sector and academia. A culture change is needed, and it’s done better at some places than others. As educators and researchers, there are many things that can be done. \n\n\n\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPublishing Quarto Website with GitHub Pages\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\nA workflow that worked for me. This is the third time that I go through the Quarto website publishing with GitHub Pages - even more reason to note it down! \n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nqtwAcademic: a quick and easy way to start your Quarto website\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow to prevent the next pandemic - Bill Gates\n\n\n\n\n\n\nNotes\n\n\n\nReading notes on some of the chapters. \n\n\n\n\n\nJan 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWebsite reboot: switching from Blogdown to Quarto\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\nTime to reboot the personal website. Now, with Quarto \n\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Climate Smart Estimator: Using ArcGIS Dashboards and Experience Builder\n\n\n\n\n\n\nArcGIS\n\n\nagriculture\n\n\nclimate\n\n\n\nOverview of WA Climate Smart Estimator ArcGIS Experience web app\n\n\n\n\n\nMay 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\n\n\n\nagriculture\n\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\n\n\n\nagriculture\n\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rpkg/rpkg1.html",
    "href": "rpkg/rpkg1.html",
    "title": "Research",
    "section": "",
    "text": "I aim to develop safe, interpretable, and adaptive AI systems for real-world cyber-physical environments that operate under uncertainty, constraints, and adversarial conditions. My research bridges the domains of machine learning, optimization, and control theory, with a strong emphasis on safety, robustness, and generalization.\nMy work centers around the following pillars:\n\nSafe & Trustworthy Reinforcement Learning: Designing agents that are robust to adversarial attacks, resilient to distributional shifts, and capable of safe exploration.\nPhysics-informed Deep Reinforcement Learning (DRL): Embedding physical laws and constraints into learning frameworks for stability, interpretability, and faster convergence.\nProbabilistic & Bayesian Modeling: Probabilistic & Bayesian Modeling: Capturing both epistemic and aleatoric uncertainties for reliable control in high-stakes, partially observable systems.\nLarge Language Models (LLMs) for autonomous reasoning: Leveraging large language models (LLMs) to enhance planning, explainability, and human-AI collaboration in control systems.\nVision-based simulation environments: Using platforms like CARLA and CityLearn to train agents in multimodal, visually rich, and interactive worlds.\n\nBy tightly integrating domain knowledge into learning frameworks, I aim to enable resilient, generalizable, and safe AI for critical applications including smart grids, autonomous systems, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/csdata/index.html",
    "href": "rpkg/csdata/index.html",
    "title": "csdata",
    "section": "",
    "text": "https://github.com/csids/csdata"
  },
  {
    "objectID": "rpkg/qtwAcademic/index.html",
    "href": "rpkg/qtwAcademic/index.html",
    "title": "qtwAcademic",
    "section": "",
    "text": "qtwAcademic stands for Quarto Websites for Academics, which provides a few Quarto templates for Quarto website that are commonly used by academics.\nThe templates are designed to make it quick and easy for users with little or no Quarto experience to create a website for their personal portfolio or courses. Each template is fully customizable once the user is more familiar with Quarto.\nRead more about the package here.\n\nTemplates\nSo far, 3 templates have been implemented in this package:\n\nPersonal website\nWebsite for courses or workshops\nMinimal website template that can be easily customized\n\nYou can find more details on each option in the vignettes."
  },
  {
    "objectID": "rpkg/csmaps/index.html",
    "href": "rpkg/csmaps/index.html",
    "title": "csmaps",
    "section": "",
    "text": "https://github.com/csids/csmaps"
  },
  {
    "objectID": "rpkg/mortanor/index.html",
    "href": "rpkg/mortanor/index.html",
    "title": "mortanor",
    "section": "",
    "text": "https://github.com/csids/mortanor"
  },
  {
    "objectID": "rpkg/covidnor/index.html",
    "href": "rpkg/covidnor/index.html",
    "title": "covidnor",
    "section": "",
    "text": "https://github.com/csids/covidnor"
  },
  {
    "objectID": "rpkg/bayesynergy/index.html",
    "href": "rpkg/bayesynergy/index.html",
    "title": "DRL for Smart Energy Systems",
    "section": "",
    "text": "Anxperiments\nCode"
  },
  {
    "objectID": "teaching/coms3630/index.html#coms-3630-introduction-to-database-management-systems",
    "href": "teaching/coms3630/index.html#coms-3630-introduction-to-database-management-systems",
    "title": "COMS 3630",
    "section": "COMS 3630 – Introduction to Database Management Systems",
    "text": "COMS 3630 – Introduction to Database Management Systems\n\n📘 Course Description\nCOMS 3630 offers a comprehensive introduction to database systems, covering both theoretical foundations and practical implementation. Students explore various data models (relational, object-oriented, and semistructured), learn query languages such as SQL and NoSQL, and build full-stack applications that integrate modern database backends with web interfaces.\nThe course emphasizes database design, optimization, and application development, using tools like MySQL, MongoDB, and Neo4J.\n\n\n\n🧑‍🏫 My Role as Teaching Assistant\nAs a Teaching Assistant for COMS 3630, I supported over 100 students in understanding key database concepts and building end-to-end data-driven applications. My responsibilities included:\n\nLab Instruction & Demonstrations: Led weekly sessions on topics like SQL programming, ER modeling, and NoSQL databases (MongoDB, Neo4J).\nProject Mentorship: Guided students through semester-long projects, helping them design schemas, write efficient queries, and integrate databases with web applications.\nTechnical Support: Assisted students in implementing transactions, managing storage, and debugging issues related to query execution and performance.\nDesign Review & Grading: Evaluated assignments and database design submissions, providing constructive feedback aligned with best practices.\nOffice Hours: Offered one-on-one and group-based academic support on database internals, relational algebra, and query optimization strategies.\n\n\n\n\n🎯 Learning Outcomes\nBy the end of the course, students are expected to:\n\nDesign and implement relational databases using ER modeling and normalization techniques.\n\nDevelop database-driven applications using SQL, APIs, and ORM frameworks.\n\nWork with NoSQL systems including document-based (MongoDB) and graph-based (Neo4J) databases.\n\nUnderstand the inner workings of a DBMS, including query processing, transaction management, and storage optimization.\n\n\n\n\n📚 Topics Covered\n\nEntity-Relationship (ER) Modeling\n\nRelational Model & Relational Algebra\n\nSQL Programming and Constraints\n\nNoSQL Systems (MongoDB, Neo4J)\n\nSchema Normalization & Data Dependencies\n\nDatabase Storage & Indexing\n\nCost Estimation & Query Optimization\n\nTransaction Management & Concurrency Control\n\nWeb Application Integration using SQL APIs & ORMs\n\nApplication Development using Host Languages (e.g., Python, JavaScript)\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms3190/index.html",
    "href": "teaching/coms3190/index.html",
    "title": "COMS 3190",
    "section": "",
    "text": "COMS 3190 introduces students to the fundamentals of user interface (UI) and user experience (UX) design through both theory and extensive hands-on development. The course covers human-computer interaction (HCI) concepts, front-end and back-end technologies, and the use of modern frameworks and APIs for developing web and Windows-based user interfaces.\nStudents gain experience with:\n\nUI design principles\n\nHTML, CSS, JavaScript\n\nReact, Node.js, Express\n\nDatabases (MongoDB and MySQL)\n\nUML modeling and event-driven architecture\n\nWeb and desktop-based client/server applications\n\n\n\n\n\nAs a Teaching Assistant for COMS 3190, I supported over 100 students in learning full-stack web development principles within the context of UI/UX design. My key responsibilities included:\n\nTechnical Instruction: Provided lab support and walkthroughs for building responsive interfaces using HTML, CSS, JavaScript, React, and Node.js.\nProject Mentorship: Guided students through semester-long UI/UX projects, including front-end development, API design, and database integration.\nDesign & Modeling Help: Assisted students in using UML for system modeling and behavioral analysis, including use case and interaction diagrams.\nTesting Support: Supported unit testing and UI testing procedures in JavaScript and helped students debug their web applications.\nCode Review & Feedback: Reviewed student submissions and provided feedback on usability, design consistency, and code efficiency.\nOffice Hours: Held weekly office hours to help students troubleshoot development issues and understand best practices in UI/UX design.\n\n\n\n\n\nUpon completing the course, students are expected to:\n\nDesign and implement interactive and accessible user interfaces.\nApply principles of HCI, UI design, UX testing, and event-driven architecture.\nUse industry-standard tools to build and evaluate responsive, full-stack applications.\nAnalyze and model system behavior using UML and interaction diagrams.\nDeploy and test applications with modern development environments and version control.\n\n\n\n\n\n\nDesign Principles for User Interfaces\n\nHuman-Computer Interaction (HCI) Fundamentals\n\nUX Testing and Evaluation\n\nHTML, CSS, JavaScript\n\nReact, Node.js, Express\n\nMongoDB, MySQL\n\nEvent-Driven Programming\n\nAPI & Framework Integration\n\nUML Diagrams (Structural, Behavioral, Interaction)\n\nUnit & UI Testing in JavaScript\n\nWindows-based UI Development\n\nClient/Server Architecture\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms3190/index.html#coms-3190-user-interface-design",
    "href": "teaching/coms3190/index.html#coms-3190-user-interface-design",
    "title": "COMS 3190",
    "section": "",
    "text": "COMS 3190 introduces students to the fundamentals of user interface (UI) and user experience (UX) design through both theory and extensive hands-on development. The course covers human-computer interaction (HCI) concepts, front-end and back-end technologies, and the use of modern frameworks and APIs for developing web and Windows-based user interfaces.\nStudents gain experience with:\n\nUI design principles\n\nHTML, CSS, JavaScript\n\nReact, Node.js, Express\n\nDatabases (MongoDB and MySQL)\n\nUML modeling and event-driven architecture\n\nWeb and desktop-based client/server applications\n\n\n\n\n\nAs a Teaching Assistant for COMS 3190, I supported over 100 students in learning full-stack web development principles within the context of UI/UX design. My key responsibilities included:\n\nTechnical Instruction: Provided lab support and walkthroughs for building responsive interfaces using HTML, CSS, JavaScript, React, and Node.js.\nProject Mentorship: Guided students through semester-long UI/UX projects, including front-end development, API design, and database integration.\nDesign & Modeling Help: Assisted students in using UML for system modeling and behavioral analysis, including use case and interaction diagrams.\nTesting Support: Supported unit testing and UI testing procedures in JavaScript and helped students debug their web applications.\nCode Review & Feedback: Reviewed student submissions and provided feedback on usability, design consistency, and code efficiency.\nOffice Hours: Held weekly office hours to help students troubleshoot development issues and understand best practices in UI/UX design.\n\n\n\n\n\nUpon completing the course, students are expected to:\n\nDesign and implement interactive and accessible user interfaces.\nApply principles of HCI, UI design, UX testing, and event-driven architecture.\nUse industry-standard tools to build and evaluate responsive, full-stack applications.\nAnalyze and model system behavior using UML and interaction diagrams.\nDeploy and test applications with modern development environments and version control.\n\n\n\n\n\n\nDesign Principles for User Interfaces\n\nHuman-Computer Interaction (HCI) Fundamentals\n\nUX Testing and Evaluation\n\nHTML, CSS, JavaScript\n\nReact, Node.js, Express\n\nMongoDB, MySQL\n\nEvent-Driven Programming\n\nAPI & Framework Integration\n\nUML Diagrams (Structural, Behavioral, Interaction)\n\nUnit & UI Testing in JavaScript\n\nWindows-based UI Development\n\nClient/Server Architecture\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms1130/index.html",
    "href": "teaching/coms1130/index.html",
    "title": "COMS 1130",
    "section": "",
    "text": "Credits: 3\nInstitution: Iowa State University\nFormat: In-Person Course (Offered Fall & Spring)\nCOMS 1130 is a 3-credit undergraduate course that teaches essential skills in Microsoft Excel and Microsoft Access, with a focus on hands-on, real-world data projects.\n\n\n\nBy the end of this course, students will:\n\nApply Microsoft Excel and Access tools to solve real-world problems.\nConduct data analysis and modeling for business and industrial use cases.\nBuild, manage, and query spreadsheets and relational databases.\n\n\n\n\n\nMicrosoft Excel (Spreadsheets):\n\nCreate and format spreadsheets\n\nApply mathematical, statistical, logical, and lookup functions\n\nBuild charts and data visualizations\n\nUse PivotTables and subtotals for data summarization\n\nConduct What-If Analysis\n\nImport and export spreadsheet data\n\nMicrosoft Access (Databases):\n\nDesign relational tables and define relationships\n\nCreate forms, queries, and reports\n\nSort, filter, and update records\n\nImplement data validation\n\nPerform decision-making tasks based on database queries\n\n\n\n\n\nAs a Teaching Assistant for COMS 1130, I contributed to this large foundational course by supporting student learning in both lectures and lab sessions. My responsibilities included:\n\nLab Instruction & Demonstrations: Conducted in-person lab sessions covering Excel and Access applications in real-world scenarios.\n\nStudent Mentorship: Provided 1-on-1 guidance during lab hours and office hours, assisting students with assignments and projects.\n\nContent Support: Answered student queries through Canvas and email in a timely and supportive manner.\n\nAssessment & Feedback: Evaluated lab work and provided detailed feedback to improve student understanding.\n\nCourse Coordination: Collaborated with the instructor to enhance teaching materials and improve classroom engagement.\n\nThis role helped me strengthen my teaching, mentoring, and technical skills while supporting students in building core competencies that are essential across many academic and professional domains.\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms1130/index.html#coms-1130-introduction-to-spreadsheets-and-databases",
    "href": "teaching/coms1130/index.html#coms-1130-introduction-to-spreadsheets-and-databases",
    "title": "COMS 1130",
    "section": "",
    "text": "Credits: 3\nInstitution: Iowa State University\nFormat: In-Person Course (Offered Fall & Spring)\nCOMS 1130 is a 3-credit undergraduate course that teaches essential skills in Microsoft Excel and Microsoft Access, with a focus on hands-on, real-world data projects.\n\n\n\nBy the end of this course, students will:\n\nApply Microsoft Excel and Access tools to solve real-world problems.\nConduct data analysis and modeling for business and industrial use cases.\nBuild, manage, and query spreadsheets and relational databases.\n\n\n\n\n\nMicrosoft Excel (Spreadsheets):\n\nCreate and format spreadsheets\n\nApply mathematical, statistical, logical, and lookup functions\n\nBuild charts and data visualizations\n\nUse PivotTables and subtotals for data summarization\n\nConduct What-If Analysis\n\nImport and export spreadsheet data\n\nMicrosoft Access (Databases):\n\nDesign relational tables and define relationships\n\nCreate forms, queries, and reports\n\nSort, filter, and update records\n\nImplement data validation\n\nPerform decision-making tasks based on database queries\n\n\n\n\n\nAs a Teaching Assistant for COMS 1130, I contributed to this large foundational course by supporting student learning in both lectures and lab sessions. My responsibilities included:\n\nLab Instruction & Demonstrations: Conducted in-person lab sessions covering Excel and Access applications in real-world scenarios.\n\nStudent Mentorship: Provided 1-on-1 guidance during lab hours and office hours, assisting students with assignments and projects.\n\nContent Support: Answered student queries through Canvas and email in a timely and supportive manner.\n\nAssessment & Feedback: Evaluated lab work and provided detailed feedback to improve student understanding.\n\nCourse Coordination: Collaborated with the instructor to enhance teaching materials and improve classroom engagement.\n\nThis role helped me strengthen my teaching, mentoring, and technical skills while supporting students in building core competencies that are essential across many academic and professional domains.\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms4170/index.html",
    "href": "teaching/coms4170/index.html",
    "title": "COMS 4170",
    "section": "",
    "text": "COMS 4170 is an upper-level undergraduate/graduate course at Iowa State University, taught by Professor Myra Cohen, focusing on rigorous methods for software testing and quality assurance.\nI served as the Teaching Assistant for this course in Spring 2025, where I was responsible for:\n\nLeading weekly lab sessions and guiding students through practical exercises on test case generation and software testing tools.\nAssisting in course content delivery and reinforcing core topics such as black-box and white-box testing, test adequacy criteria, integration, and regression testing.\nDesigning and grading assignments and exams, ensuring alignment with pedagogical goals.\nProviding one-on-one mentoring to students to support their understanding of software quality assurance methodologies.\n\n\n\n\nPrinciples and methodologies of software testing\nTest design techniques: black-box and white-box\nTest models and adequacy criteria\nIntegration and system testing\nRegression testing strategies\nUse of automated software testing tools\nTesting management and process strategies\n\nTextbook: Introduction to Software Testing, 2nd edition by Paul Ammann and Jeff Offutt."
  },
  {
    "objectID": "teaching/coms4170/index.html#coms-4170-software-testing",
    "href": "teaching/coms4170/index.html#coms-4170-software-testing",
    "title": "COMS 4170",
    "section": "",
    "text": "COMS 4170 is an upper-level undergraduate/graduate course at Iowa State University, taught by Professor Myra Cohen, focusing on rigorous methods for software testing and quality assurance.\nI served as the Teaching Assistant for this course in Spring 2025, where I was responsible for:\n\nLeading weekly lab sessions and guiding students through practical exercises on test case generation and software testing tools.\nAssisting in course content delivery and reinforcing core topics such as black-box and white-box testing, test adequacy criteria, integration, and regression testing.\nDesigning and grading assignments and exams, ensuring alignment with pedagogical goals.\nProviding one-on-one mentoring to students to support their understanding of software quality assurance methodologies.\n\n\n\n\nPrinciples and methodologies of software testing\nTest design techniques: black-box and white-box\nTest models and adequacy criteria\nIntegration and system testing\nRegression testing strategies\nUse of automated software testing tools\nTesting management and process strategies\n\nTextbook: Introduction to Software Testing, 2nd edition by Paul Ammann and Jeff Offutt."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Journal Articles\n\n\n\nKumar, Kundan, Ravikumar Gelli, and Christopher Quinn. 2025.\n“Physics-Informed DRL for Smart Grids.”  IEEE Transactions on Smart Grid.  Paper Code Poster\n\n\n\n\n\n\nConference Papers\n\n\n\nKumar, Kundan, and Christopher Quinn. 2024.\n“Federated DRL for Resilient Energy Systems.”  In Proceedings of ACM BuildSys.  Paper Code\n\n\n\n\n\n\nWorkshop Papers\n\n\n\nKumar, Kundan, and Nabila Masud. 2024.\n“One-Shot DRL for Low-Data Grid Settings.”  In NeurIPS Workshop on Energy Systems.  Paper Poster"
  },
  {
    "objectID": "teaching/teaching1.html",
    "href": "teaching/teaching1.html",
    "title": "Teaching",
    "section": "",
    "text": "As a Teaching Assistant at Iowa State University, I am dedicated to creating an inclusive and dynamic learning environment that enables students to excel in computer science. My teaching approach emphasizes practical application of theoretical concepts, encouraging students to develop hands-on experience in software development, user interface design, and database management. I believe in preparing students for real-world challenges in the technology industry through project-based learning and personalized support.\nCourses I have served as a Teaching Assistant or Mentor in:"
  },
  {
    "objectID": "teaching/teaching1.html#class-1",
    "href": "teaching/teaching1.html#class-1",
    "title": "Teaching",
    "section": "Class 1",
    "text": "Class 1\n\n\n\n\n\nFall 2024,   Fall 2023\nClass 1 introduction."
  },
  {
    "objectID": "teaching/teaching1.html#class-2",
    "href": "teaching/teaching1.html#class-2",
    "title": "Teaching",
    "section": "Class 2",
    "text": "Class 2\n\n\n\n\n\nSpring 2025,   Fall 2023\nClass 2 introduction."
  },
  {
    "objectID": "teaching/teaching1.html#class-3",
    "href": "teaching/teaching1.html#class-3",
    "title": "Teaching",
    "section": "Class 3",
    "text": "Class 3\n\n\n\n\n\nSpring 2025,   Spring 2024\nClass 3 introduction.\n\n\n\nFuture Goals\n\nI am committed to continually improving teaching methods and curriculum design. My goals include:\n\nIncorporating emerging AI and software development tools into teaching.\nCreating interactive, hands-on, project-based learning environments.\nContributing to the computer science education community through curriculum research and mentoring.\nExpanding opportunities for students to apply theory to real-world computing challenges.\n\nFeel free to reach out if you would like to collaborate or learn more about my teaching and mentorship activities."
  },
  {
    "objectID": "teaching/teaching1.html#future-goals",
    "href": "teaching/teaching1.html#future-goals",
    "title": "Teaching",
    "section": "Future Goals",
    "text": "Future Goals\nI am committed to continually improving teaching methods and curriculum design. My goals include:\n\nIncorporating emerging AI and software development tools into teaching.\nCreating interactive, hands-on, project-based learning environments.\nContributing to the computer science education community through curriculum research and mentoring.\nExpanding opportunities for students to apply theory to real-world computing challenges.\n\nReach out via email if you’d like to collaborate or learn more."
  },
  {
    "objectID": "teaching/teaching1.html#coms-1130-introduction-to-spreadsheets-and-databases",
    "href": "teaching/teaching1.html#coms-1130-introduction-to-spreadsheets-and-databases",
    "title": "Teaching",
    "section": "COMS 1130 – Introduction to Spreadsheets and Databases",
    "text": "COMS 1130 – Introduction to Spreadsheets and Databases\n\n\n\n\n\nSpring 2025\nAn introductory course that teaches problem solving through programming in Python. Focuses on functions, control flow, loops, data structures, and basic algorithms."
  },
  {
    "objectID": "teaching/teaching1.html#class-coms-1130-introduction-to-python",
    "href": "teaching/teaching1.html#class-coms-1130-introduction-to-python",
    "title": "Teaching",
    "section": "Class: COMS 1130 – Introduction to Python",
    "text": "Class: COMS 1130 – Introduction to Python\n\n\n\nPython\n\n\nSpring 2025, Fall 2023\nAn introductory course that teaches problem solving through programming in Python. Focuses on functions, control flow, loops, data structures, and basic algorithms."
  },
  {
    "objectID": "teaching/teaching1.html#class-coms-3630-database-management-systems",
    "href": "teaching/teaching1.html#class-coms-3630-database-management-systems",
    "title": "Teaching",
    "section": "Class: COMS 3630 – Database Management Systems",
    "text": "Class: COMS 3630 – Database Management Systems\n\n\n\nDBMS\n\n\nSpring 2024, Fall 2023\nA comprehensive introduction to database theory and applications. Students design schemas, write complex queries, and build database-backed web apps."
  },
  {
    "objectID": "teaching/teaching1.html#class-coms-3620-software-engineering-fundamentals",
    "href": "teaching/teaching1.html#class-coms-3620-software-engineering-fundamentals",
    "title": "Teaching",
    "section": "Class: COMS 3620 – Software Engineering Fundamentals",
    "text": "Class: COMS 3620 – Software Engineering Fundamentals\n\n\n\nSoftware\n\n\nFall 2024\nCovers software development lifecycle, UML modeling, version control, Agile methodology, and teamwork skills with real-world software projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-1130-introduction-to-python",
    "href": "teaching/teaching1.html#coms-1130-introduction-to-python",
    "title": "Teaching",
    "section": "COMS 1130 – Introduction to Python",
    "text": "COMS 1130 – Introduction to Python\n\n\n\nSpring 2025, Fall 2023\nAn introductory course that teaches problem solving through programming in Python. Focuses on functions, control flow, loops, data structures, and basic algorithms."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-database-management-systems",
    "href": "teaching/teaching1.html#coms-3630-database-management-systems",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\n\n\nSpring 2024\nCovers the fundamentals of database architecture, data modeling, and SQL. Students gain hands-on experience with relational databases and explore indexing, transactions, normalization, and database design."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3620-software-engineering-fundamentals",
    "href": "teaching/teaching1.html#coms-3620-software-engineering-fundamentals",
    "title": "Teaching",
    "section": "COMS 3620 – Software Engineering Fundamentals",
    "text": "COMS 3620 – Software Engineering Fundamentals\n\n\n\nFall 2024\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-database-management-systems-1",
    "href": "teaching/teaching1.html#coms-3630-database-management-systems-1",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\nSpring 2024, Fall 2023\nCovers theory and practice of relational databases. Students design schemas, write complex SQL queries, and integrate databases with full-stack apps."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-database-management-systems-2",
    "href": "teaching/teaching1.html#coms-3630-database-management-systems-2",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\nSpring 2024, Fall 2023\nCovers theory and practice of relational databases. Students design schemas, write complex SQL queries, and integrate databases with full-stack apps."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-database-management-systems-3",
    "href": "teaching/teaching1.html#coms-3630-database-management-systems-3",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\nSpring 2024, Fall 2023\nCovers theory and practice of relational databases. Students design schemas, write complex SQL queries, and integrate databases with full-stack apps."
  },
  {
    "objectID": "teaching/teaching1.html#ccoms-3620-object-oriented-analysis-and-design",
    "href": "teaching/teaching1.html#ccoms-3620-object-oriented-analysis-and-design",
    "title": "Teaching",
    "section": "CCOMS 3620 – Object-Oriented Analysis and Design",
    "text": "CCOMS 3620 – Object-Oriented Analysis and Design\n\n\n\nFall 2024\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3090-software-development-practices",
    "href": "teaching/teaching1.html#coms-3090-software-development-practices",
    "title": "Teaching",
    "section": "COMS 3090 – Software Development Practices",
    "text": "COMS 3090 – Software Development Practices\n\n\n\n\n\nSpring 2024,   Fall 2023\nHands-on course focused on modern software development practices, including Git, Agile methodologies, code reviews, unit testing, and CI/CD. Students engage in team-based projects that simulate real-world workflows, emphasizing collaboration, documentation, and iterative development."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3190-user-interface-design",
    "href": "teaching/teaching1.html#coms-3190-user-interface-design",
    "title": "Teaching",
    "section": "COMS 3190 – User Interface Design",
    "text": "COMS 3190 – User Interface Design\n\n\n\n\n\nSpring 2024\nExplores the principles of designing user-friendly and efficient interfaces. Combines hardware and data-driven UI practices, integrating Raspberry Pi for IoT applications and Nodejs for fronted visualization and interaction."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3620-object-oriented-analysis-and-design",
    "href": "teaching/teaching1.html#coms-3620-object-oriented-analysis-and-design",
    "title": "Teaching",
    "section": "COMS 3620 – Object-Oriented Analysis and Design",
    "text": "COMS 3620 – Object-Oriented Analysis and Design\n\n\n\n\n\nSpring 2024,   Fall 2023\nTeaches how to model and design software systems using object-oriented principles. Emphasis is on UML diagrams, design patterns, and implementation in Java to build modular, maintainable, and scalable software."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-introduction-to-database-management-systems",
    "href": "teaching/teaching1.html#coms-3630-introduction-to-database-management-systems",
    "title": "Teaching",
    "section": "COMS 3630 – Introduction to Database Management Systems",
    "text": "COMS 3630 – Introduction to Database Management Systems\n\n\n\n\n\nSpring 2024\nComprehensive introduction to relational databases including SQL, schema design, normalization, transactions, and database-backed apps."
  },
  {
    "objectID": "teaching/teaching1.html#ccoms-4170-software-testing",
    "href": "teaching/teaching1.html#ccoms-4170-software-testing",
    "title": "Teaching",
    "section": "CCOMS 4170 – Software Testing",
    "text": "CCOMS 4170 – Software Testing\n\n\n\nSpring 2025\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-4170-software-testing",
    "href": "teaching/teaching1.html#coms-4170-software-testing",
    "title": "Teaching",
    "section": "COMS 4170 – Software Testing",
    "text": "COMS 4170 – Software Testing\n\n\n\n\n\nFall 2024,   Fall 2023\nFocuses on the principles and practices of software verification and validation. Students learn to design and execute test cases, use debugging tools, and apply automation frameworks to ensure software quality and reliability."
  },
  {
    "objectID": "teaching/teaching1.html#coms-4170-software-testing-1",
    "href": "teaching/teaching1.html#coms-4170-software-testing-1",
    "title": "Teaching",
    "section": "COMS 4170 – Software Testing",
    "text": "COMS 4170 – Software Testing\n\n\n\n\n\nFall 2024,   Fall 2023\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-4170-introduction-to-database-management-systems",
    "href": "teaching/teaching1.html#coms-4170-introduction-to-database-management-systems",
    "title": "Teaching",
    "section": "COMS 4170 – Introduction to Database Management Systems",
    "text": "COMS 4170 – Introduction to Database Management Systems\n\n\n\n\n\nFall 2024,   Fall 2023\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-1130-spreadsheets-and-databases",
    "href": "teaching/teaching1.html#coms-1130-spreadsheets-and-databases",
    "title": "Teaching",
    "section": "COMS 1130 – Spreadsheets and Databases",
    "text": "COMS 1130 – Spreadsheets and Databases\n\n\n\n\n\nSpring 2025\nFoundational course in using tools like Microsoft Excel and Access for data organization, analysis, and reporting. Students learn formulas, pivot tables, queries, and form design for practical business and data applications."
  },
  {
    "objectID": "teaching/teaching.html#coms-1130-spreadsheets-and-databases",
    "href": "teaching/teaching.html#coms-1130-spreadsheets-and-databases",
    "title": "Teaching",
    "section": "COMS 1130 – Spreadsheets and Databases",
    "text": "COMS 1130 – Spreadsheets and Databases\n\n\n\n\n\nFall 2016\nFoundational course in using tools like Microsoft Excel and Access for data organization, analysis, and reporting. Students learn formulas, pivot tables, queries, and form design for practical business and data applications."
  },
  {
    "objectID": "teaching/teaching.html#coms-3090-software-development-practices",
    "href": "teaching/teaching.html#coms-3090-software-development-practices",
    "title": "Teaching",
    "section": "COMS 3090 – Software Development Practices",
    "text": "COMS 3090 – Software Development Practices\n\n\n\n\n\nFall 2022,   Spring 2023,   Spring 2024,   Fall 2025\nHands-on course focused on modern software development practices, including Git, Agile methodologies, code reviews, unit testing, and CI/CD. Students engage in team-based projects that simulate real-world workflows, emphasizing collaboration, documentation, and iterative development."
  },
  {
    "objectID": "teaching/teaching.html#coms-3190-user-interface-design",
    "href": "teaching/teaching.html#coms-3190-user-interface-design",
    "title": "Teaching",
    "section": "COMS 3190 – User Interface Design",
    "text": "COMS 3190 – User Interface Design\n\n\n\n\n\nFall 2023\nExplores the principles of designing user-friendly and efficient interfaces. Combines hardware and data-driven UI practices, integrating Raspberry Pi for IoT applications and Nodejs for fronted visualization and interaction."
  },
  {
    "objectID": "teaching/teaching.html#coms-3620-object-oriented-analysis-and-design",
    "href": "teaching/teaching.html#coms-3620-object-oriented-analysis-and-design",
    "title": "Teaching",
    "section": "COMS 3620 – Object-Oriented Analysis and Design",
    "text": "COMS 3620 – Object-Oriented Analysis and Design\n\n\n\n\n\nFall 2020,   Spring 2021,   Fall 2021,   Fall 2024\nTeaches how to model and design software systems using object-oriented principles. Emphasis is on UML diagrams, design patterns, and implementation in Java to build modular, maintainable, and scalable software."
  },
  {
    "objectID": "teaching/teaching.html#coms-3630-database-management-systems",
    "href": "teaching/teaching.html#coms-3630-database-management-systems",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\n\n\nSpring 2022\nCovers the fundamentals of database architecture, data modeling, and SQL. Students gain hands-on experience with relational databases and explore indexing, transactions, normalization, and database design."
  },
  {
    "objectID": "teaching/teaching.html#coms-4170-software-testing",
    "href": "teaching/teaching.html#coms-4170-software-testing",
    "title": "Teaching",
    "section": "COMS 4170 – Software Testing",
    "text": "COMS 4170 – Software Testing\n\n\n\n\n\nSpring 2025\nFocuses on the principles and practices of software verification and validation. Students learn to design and execute test cases, use debugging tools, and apply automation frameworks to ensure software quality and reliability."
  },
  {
    "objectID": "teaching/teaching.html#future-goals",
    "href": "teaching/teaching.html#future-goals",
    "title": "Teaching",
    "section": "Future Goals",
    "text": "Future Goals\nI am committed to continually improving teaching methods and curriculum design. My goals include:\n\nIncorporating emerging AI and software development tools into teaching.\nCreating interactive, hands-on, project-based learning environments.\nContributing to the computer science education community through curriculum research and mentoring.\nExpanding opportunities for students to apply theory to real-world computing challenges.\n\nReach out via email if you’d like to collaborate or learn more."
  },
  {
    "objectID": "projects/projects1.html",
    "href": "projects/projects1.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/projects1.html#large-language-models",
    "href": "projects/projects1.html#large-language-models",
    "title": "Projects",
    "section": "",
    "text": "contents: projects/llm type: grid fields: [image, title, description] sort: “date desc” image-align: right image-height: 100% :::"
  },
  {
    "objectID": "projects/projects1.html#deep-reinforcement-learning",
    "href": "projects/projects1.html#deep-reinforcement-learning",
    "title": "Projects",
    "section": "🚀 Deep Reinforcement Learning",
    "text": "🚀 Deep Reinforcement Learning\n\ncontents: projects/drl type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "projects/projects1.html#deepmachine-learning",
    "href": "projects/projects1.html#deepmachine-learning",
    "title": "Projects",
    "section": "🧠 Deep/Machine Learning",
    "text": "🧠 Deep/Machine Learning\n\n```nctuuakgt contents: projects/dl type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "projects/projects1.html#statistics",
    "href": "projects/projects1.html#statistics",
    "title": "Projects",
    "section": "📊 Statistics",
    "text": "📊 Statistics\n\n```nctuuakgt contents: projects/statistics type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "projects/projects1.html#computer-vision",
    "href": "projects/projects1.html#computer-vision",
    "title": "Projects",
    "section": "👁️‍🗨️ Computer Vision",
    "text": "👁️‍🗨️ Computer Vision\n\n```nctuuakgt contents: projects/cv type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "projects/projects1.html#robotics",
    "href": "projects/projects1.html#robotics",
    "title": "Projects",
    "section": "🤖 Robotics",
    "text": "🤖 Robotics\n\n```nctuuakgt contents: projects/robotics type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "📄 Resume"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": " Professional Experience",
    "text": "Professional Experience\n\n\n\n\n\nNational Renewable Energy Laboratory (NREL)\n\n\n\n\n\nMachine Learning Engineer (Intern)\n\n\n May 2024  —  Jan 2025 \n\n\n\n\nDeveloped novel machine learning models for automated network topology inference and resilient control policy optimization for complex distributed systems under extreme scenarios\nDesigned and developed semi‑supervised learning approaches to tackle the challenge of limited labeled data in networks, achieving 98% improvement in model accuracy with varying labeled data.\nPaper ”Advanced Semi‑Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems” accepted at IEEE Power & Energy Society General Meeting (PES GM) 2025.\n\n\n\n\n\n\n\nComcast\n\n\n\n\n\nSoftware Engineer\n\n\n Jul 2019  —  Feb 2020 \n\n\n\n\nDesigned and implemented real‑time data processing pipelines using Amazon Kinesis and RabbitMQ, processing 1TB+ daily data for fraud detection and system monitoring.\nDeveloped machine learning models for anomaly detection and user behavior analysis, reducing fraudulent activities by 70% through predictive analytics.\nCreated interactive dashboards using Presto DB and Python visualization tools, enabling real‑time monitoring of network performance metrics and fraud patterns.\n\n\n\n\n\n\nIBM\n\n\n\n\n\nSoftware Engineer\n\n\n Jan 2019  —  Jun 2019 \n\n\n\n\nLed cloud infrastructure optimization using OpenShift, implementing auto‑scaling solutions that reduced operational costs by 30%.\nDeveloped a comprehensive monitoring system using Grafana and Flask, providing real‑time visibility into 100+ cloud servers.\nImplemented automated performance monitoring and alerting system, reducing incident response time by 60%.\n\n\n\n\n\n\nHewlett Packard Enterprise (HPE)\n\n\n\n\n\nSoftware Engineer\n\n\n Apr 2017  —  Dec 2018 \n\n\n\n\n\nSpearheaded migration of critical applications from HPI to HPE domain, ensuring zero downtime during transition.\nImplemented OAuth 2.0 authentication system and RESTful services using Spring Boot, securing applications serving 50K+ users.\nDesigned and deployed microservices architecture on Apache/WebLogic servers, improving system response time by 40%.\n\n\n\n\n\n\nTata Consultancy Services (TCS)\n\n\n\n\n\nSystem Engineer\n\n\n Jul 2012  —  Dec 2015 \n\n\n\n\n\nEngineered high‑performance ETL pipelines for data warehouse integration, processing 100GB+ daily data volumes.\nOptimized database performance through advanced SQL tuning and indexing strategies, reducing query execution time by 70%.\nReceived excellence award for achieving $100K cost savings through database optimization initiatives."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": " Education",
    "text": "Education\n\n\n\n\n\nIowa State University\n\n\n\n\n\nPh.D. in Computer Science (Minor: Statistics)\n\n\n 2020  —  2025 (Expected) \n\n\n\n\n\nResearch: Deep RL, Physics-Informed AI, Uncertainty Quantification\nCourses: Deep Learning, NLP, Statistical Theory, Empirical Methods, Algorithms\n\n\n\n\nMS in Computer Science\n\n\n Jan 2015  —  Dec 2016 \n\n\n\n\nFocus: Algorithms, Databases, Network Programming"
  },
  {
    "objectID": "cv.html#communities-and-organizations",
    "href": "cv.html#communities-and-organizations",
    "title": "Curriculum Vitae",
    "section": "{{< fa user-group >}} Communities and Organizations",
    "text": "{{&lt; fa user-group &gt;}} Communities and Organizations\n Posit Data Science Hangout\n R4DS Learning Community\n Women in GovTech\n R-Ladies Seattle\n Seattle useR Group"
  },
  {
    "objectID": "cv.html#extension-scientific-writing",
    "href": "cv.html#extension-scientific-writing",
    "title": "Curriculum Vitae",
    "section": "{{< fa book >}} Extension & Scientific Writing",
    "text": "{{&lt; fa book &gt;}} Extension & Scientific Writing\nRyan, J.N., Michel, L., Gelardi, D.G. 2023. Standard Operating Procedure: Soil Health Monitoring in Washington State. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 102-923.\nNoland, K., Nickelson, A., Ryan, J.N., Drennan, M. 2021. Ambient Monitoring for Pesticides in Washington State Surface Water: 2019 Technical Report. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 102-629.\n\nAlso co-author for 2018 and 2017 Technical Reports.\n\nHancock, J., Ryan, J.N. 2021. Pesticide Groundwater Sampling in the Sumas-Blaine Surficial Aquifer. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 103-895.\nTuttle, G., Noland, K., Ryan, J.N. 2020. Glyphosate: Ecological Fate and Effects and Human Health Summary. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 809-817.\nRyan, J.N. 2020. Understanding Pesticide Leaching Potential and Protecting Groundwater. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 809-865.\nRyan, J.N. 2020. Pesticide Application and Water Quality. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 102-850.\nRyan, J.N. 2020. Understanding Pesticide Product Labels. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 102-849."
  },
  {
    "objectID": "cv.html#presentations",
    "href": "cv.html#presentations",
    "title": "Curriculum Vitae",
    "section": " Presentations",
    "text": "Presentations\n\nUpcoming\n\nParameterized Reporting using Quarto (Invited) — R-Ladies DC (Jan 2024)\n\n\n\nPast\n\nParameterized Quarto Reports — posit::conf(2023), Chicago"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": " Skills",
    "text": "Skills\n\n\n\nProgramming\n\n\nPython, R, Java, C++, SAS, MATLAB, SQL, HTML/CSS, JavaScript (Node.js, React)\n\n\nMachine/Deep Learning\n\n\nscikit-learn, TensorFlow, PyTorch, pandas, Matplotlib, Seaborn, Gym, RLlib\n\n\nLLMs & NLP\n\n\nOpenAI API, Hugging Face Transformers, LangChain, LlamaIndex, RAG (retrievers, chunking, reranking), vector DBs (FAISS, Chroma, Pinecone), prompt engineering & structured outputs, evaluation (Ragas, Promptfoo)\n\n\nAgentic Systems & Orchestration\n\n\nLangGraph (stateful workflows), LangChain Agents (ReAct/MRKL/tools), function/tool calling, multi-agent design, planning & memory, tool-use (search/code/execution), guards & grounding\n\n\nHPC & Big Data\n\n\nHadoop, Hive, Spark, Kafka, Kinesis, SLURM, MPI, OpenMP\n\n\nSimulation & Modeling\n\n\nOPAL-RT, OpenDSS (Power), CARLA (Autonomous Driving)\n\n\nOptimization\n\n\nGurobi, Pyomo, BoTorch, Optuna, Hyperopt\n\n\nVisualization & GIS\n\n\nTableau, ArcGIS, Leaflet\n\n\nCloud & DevOps\n\n\nAWS (EC2, S3, Lambda), GCP, Docker, Kubernetes, Git, Terraform, Jenkins, CircleCI"
  },
  {
    "objectID": "cv.html#footnotes",
    "href": "cv.html#footnotes",
    "title": "Curriculum Vitae",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI completed half of my classes at high school and the other half at community college.↩︎"
  },
  {
    "objectID": "cv.html#communities-organizations",
    "href": "cv.html#communities-organizations",
    "title": "\nCurriculum Vitae\n",
    "section": " Communities & Organizations",
    "text": "Communities & Organizations\n\n R4DS Learning Community\n Women in GovTech\n R-Ladies Seattle"
  },
  {
    "objectID": "cv.html#scientific-writing",
    "href": "cv.html#scientific-writing",
    "title": "\nCurriculum Vitae\n",
    "section": " Scientific Writing",
    "text": "Scientific Writing\n\nRyan, J.N., Michel, L., et al. Soil Health Monitoring SOP. Publication 102-923\nRyan, J.N. 2020. Pesticide Application and Water Quality. Publication 102-850"
  },
  {
    "objectID": "about_me.html#hi-there",
    "href": "about_me.html#hi-there",
    "title": "Kundan Kumar",
    "section": "Hi there!",
    "text": "Hi there!\nI am a researcher focused on developing intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work spans deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), safe and explainable AI, computer vision, and robotics, with real-world applications in smart energy systems, autonomous vehicles, and critical infrastructure.\nThe core of my Ph.D. research centers on developing physics-informed and safety-critical deep reinforcement learning (DRL) frameworks that embed domain knowledge and system constraints directly into the learning process. By incorporating physical laws, safety boundaries, and system dynamics into policy optimization, I design agents capable of making robust, interpretable, and reliable decisions in dynamic, high-stakes environments. My work addresses challenges such as uncertainty quantification, adversarial resilience, and safe exploration, while enabling agents to generalize across diverse network topologies, environmental conditions, and task distributions through advanced transfer learning and meta-learning techniques.\nI also leverage the CARLA simulator for autonomous driving research, combining computer vision, trajectory planning, and policy learning in complex traffic environments. My work integrates vision-based perception models for object detection, semantic segmentation, and sensor fusion, enabling robust situational awareness for autonomous agents. In parallel, I integrate LLM-based reasoning into simulation and control frameworks to support high-level planning, adaptive decision-making, and interactive human-AI collaboration for robotics and safety-critical control.\nBeyond autonomous and energy systems, my broader research interests include probabilistic modeling, statistical machine learning, and developing AI systems that are robust, trustworthy, and deployable in real-world complex environments.\nOther Research Interests:\n\nComputer Vision: Visual perception, object detection, semantic segmentation, sensor fusion for autonomous systems.\nSoftware Systems: Scalable software engineering, simulation framework development, real-time systems integration.\nStatistical Machine Learning: Uncertainty quantification, probabilistic modeling, data-driven inference in dynamic environments.\nRobotics: Learning-based control, adaptive planning, safe human-robot interaction, multi-modal robotic systems.\n\n\n\nNews\n\n\n\n\n\n\n[Jan 2025]\n\n\nOur paper on Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids has been accepted to IEEE PES Grid Edge Technologies Conference & Exposition 2025.\n\n\n\n\n[Aug 2024]\n\n\nOur paper on Workshop for High Performance Computing has been accepted at Pittsburgh Supercomputing Center 2024.\n\n\n\n\n[Jul 2024]\n\n\nOur paper on Bayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control has been accepted to IEEE PES General Meeting 2024.\n\n\n\n\n[Nov 2023]\n\n\nOur paper Deep RL-based Volt-VAR Control and Attack Resiliency for DER-Integrated Distribution Grids was accepted to IEEE ISGT 2024.\n\n\n\n\n[Aug 2022]\n\n\nParticipated in the Oxford Machine Learning Summer School, completing tracks in MLx Health and MLx Finance.\n\n\n\n\n[Apr 2022]\n\n\nOur paper on Pattern-Based Multivariate Regression using Deep Learning (PBMR-DP) was accepted to ICMLA 2022."
  },
  {
    "objectID": "cv.html#honors-awards",
    "href": "cv.html#honors-awards",
    "title": "Curriculum Vitae",
    "section": " Honors & Awards",
    "text": "Honors & Awards\n\nSelected, Seventh Workshop on Autonomous Energy Systems @ NREL (2024)\nSelected, ByteBoost Workshop on Accelerating HPC Research Skills (2024)\nSelected, Oxford Machine Learning Summer School (OxML) (2022)\nExcellence Award, Database Optimization @ TCS\n2nd Place, BAJA SAE India (Safest Terrain Vehicle Category, National Level)"
  },
  {
    "objectID": "cv.html#service",
    "href": "cv.html#service",
    "title": "Curriculum Vitae",
    "section": " Service",
    "text": "Service\n\nReviewer:\n\nIEEE Transactions on Industrial Informatics (2025)\nConference on Neural Information Processing Systems (Ethics)(2025)\nIEEE Transactions on Neural Networks and Learning Systems (2024)\nIEEE PES GM, Grid Edge & ISGT (2023, 2024)\n\nMock Interviewer: Supporting underrepresented minorities in tech.\nVolunteer, Prayaas India (BIT): NGO providing quality education to underprivileged children in slums and villages."
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Curriculum Vitae",
    "section": " Teaching Experience",
    "text": "Teaching Experience\n\n\n\n\n\nIowa State University\n\n\n\n\n\nTeaching Assistant\n\n\n 2020  —  2025 \n\n\n\nDepartment of Computer Science\n\nSupported undergraduate/graduate courses including Software Development Practices, Database Systems, and Spreadsheets.\nLed weekly lab sessions, assisted students with debugging and conceptual challenges, and held office hours.\nDesigned assignments and quizzes aligned with real-world workflows and agile development practices.\nMentored students on semester-long capstone projects simulating software engineering team experiences.\n\n\n Teaching Portfolio"
  },
  {
    "objectID": "cv.html#research-experience",
    "href": "cv.html#research-experience",
    "title": "Curriculum Vitae",
    "section": " Research Experience",
    "text": "Research Experience\n\n\n\n\n\nIowa State University\n\n\n\n\n\nResearch Assistant\n\n\n Aug 2022  —  Jul 2025 \n\n\n\n\nResearch on Physics‑Informed Deep Reinforcement Learning for Critical Infrastructure Systems, focusing on Intelligent Resource Management and Security in Large‑Scale Distributed Networks.\nApplied computational deep reinforcement learning algorithms in a Smart Energy System to analyze power simulation data, minimizing voltage violations, power loss, and control errors.\nDeveloped physics‑informed DRL algorithms incorporating domain‑specific physical constraints, achieving 30% improvement in resource allocation efficiency and reducing system violations in complex distributed networks.\nDesigned and implemented adversarial attack detection and mitigation frameworks for AI models in critical systems, enhancing robustness against security threats through systematic testing and defensive techniques.\nCreated novel transfer learning methodologies enabling DRL models to adapt across varying network sizes and topologies, reducing training time by 40% for new configurations.\nDeveloped Python‑based simulation and control framework integrating real‑time hardware (OPAL‑RT and OpenDSS) with distributed systems.\nLeveraged LLM‑driven reasoning and contextual understanding within simulation environments to support real‑time adaptive control, human‑AI collaboration, and predictive system optimization.\n\n\n\n\nResearch Assistant\n\n\n Aug 2020  —  Jul 2022 \n\n\n\n\nResearch on Deep Reinforcement Learning (DRL) and Safety‑Critical Learning for Autonomous Systems, with focus on perception, control, and decision‑making in high‑stakes environments.\nUtilized CARLA simulator for vision‑based autonomous driving tasks, including perception, object detection, trajectory planning, and policy learning in complex traffic scenarios.\nApplied deep computer vision models for object recognition, semantic segmentation, and sensor fusion, enabling robust situational awareness in autonomous driving and robotics.\n\n\n Research Portfolio"
  },
  {
    "objectID": "cv.html#teaching-experience-1",
    "href": "cv.html#teaching-experience-1",
    "title": "Curriculum Vitae",
    "section": " Teaching Experience",
    "text": "Teaching Experience\n\n\n\n\n\nIowa State University – Department of Computer ScienceAug 2020 – Present\n\n\nGraduate Teaching Assistant\n\nSupported undergraduate/graduate courses including Software Development Practices, Database Systems, and Spreadsheets.\nLed weekly lab sessions, assisted students with debugging and conceptual challenges, and held office hours.\nDesigned assignments and quizzes aligned with real-world workflows and agile development practices.\nMentored students on semester-long capstone projects simulating software engineering team experiences."
  },
  {
    "objectID": "cv.html#projects",
    "href": "cv.html#projects",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n   Fast Mixed‑Logit Estimation Fast estimation of mixed logit models with preference‑space utility.  \n   cbcTools Suite Designing choice‑based conjoint experiments and power/system analysis.  \n   LLM‑Powered Energy Optimizer Multi‑building energy optimization in CityLearn with LLM guidance.  \n\n\n 📁 Check My Projects"
  },
  {
    "objectID": "cv.html#projects-1",
    "href": "cv.html#projects-1",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n\n\n\n\n   Fast Mixed‑Logit Estimation Fast estimation of mixed logit models with preference‑space utility.  \n   cbcTools Suite Designing choice‑based conjoint experiments and power analyses.  \n   LLM‑Powered Energy Optimizer Multi‑building energy optimization in CityLearn with LLM guidance."
  },
  {
    "objectID": "cv.html#projects-2",
    "href": "cv.html#projects-2",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n   Fast estimation of mixed logit models with preference‑space utility \n   Tools for designing choice‑based conjoint experiments and power analyses. \n   LLM-Powered Multi-Building Energy Optimization in CityLearn LLM-Powered Multi-Building Energy Optimization in CityLearn"
  },
  {
    "objectID": "cv.html#projects-3",
    "href": "cv.html#projects-3",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n\n\n&lt;img src=\"assets/projects/fed‑drl.jpg\" alt=\"Federated DRL thumbnail\"&gt;\n&lt;div class=\"proj-body\"&gt;\n  &lt;h3&gt;project 1&lt;/h3&gt;\n  &lt;p&gt;with background image&lt;/p&gt;\n&lt;/div&gt;\n\n\n\n&lt;img src=\"assets/projects/sunset.jpg\" alt=\"Sunset thumbnail\"&gt;\n&lt;div class=\"proj-body\"&gt;\n  &lt;h3&gt;project 7&lt;/h3&gt;\n  &lt;p&gt;with background image&lt;/p&gt;\n&lt;/div&gt;\n\n\n\n&lt;img src=\"assets/projects/needles.jpg\" alt=\"Needles thumbnail\"&gt;\n&lt;div class=\"proj-body\"&gt;\n  &lt;h3&gt;project 2&lt;/h3&gt;\n  &lt;p&gt;a project with a background image and giscus comments&lt;/p&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "cv.html#featured-projects",
    "href": "cv.html#featured-projects",
    "title": "Projects",
    "section": "Featured Projects",
    "text": "Featured Projects\n\n\n\n\n🌱\n\n\nwashi\n\nR PACKAGE\n\n\n\nPalette and themes for Washington Soil Health Initiative branding\n\nSEP 7, 2023\n\n\n\n\n\n🗺️\n\n\nWaSHI Soil Health Roadmap\n\nARCGIS PYTHON\n\n\n\nWashington Soil Health Initiative Roadmap created with Python and ArcGIS Dashboards & Experience Builder\n\nAUG 4, 2023\n\n\n\n\n\n🐋\n\n\norcas\n\nR PACKAGE\n\n\n\nMy first personal project to learn web scraping with {rvest} and mapping with {leaflet}\n\nAPR 20, 2023"
  },
  {
    "objectID": "projects/prohect2.html",
    "href": "projects/prohect2.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Home\n    Contact"
  },
  {
    "objectID": "contact.html#visit-us",
    "href": "contact.html#visit-us",
    "title": "Kundan Kumar",
    "section": "Visit Us",
    "text": "Visit Us\nThe Lab is located in Atanasoff Hall at the Iowa State University.\n\nStreet Address: 2434 Osborn Dr , Ames, IA 50011"
  },
  {
    "objectID": "contact.html#email-us",
    "href": "contact.html#email-us",
    "title": "Kundan Kumar",
    "section": "Email Us",
    "text": "Email Us\n\n\n\n\n\n  \nName *  \nEmail *  \nSubject  — Select a topic — Research Resources Opportunities  \nMessage\n\n\n\nSubmit"
  },
  {
    "objectID": "contact.html#kundan-kumar",
    "href": "contact.html#kundan-kumar",
    "title": "Contact",
    "section": "",
    "text": "Iowa State University\nAtanasoff Hall\n2434 Osborn Dr\nAmes, IA 50011\nYou can reach me at\n📧 kkumar@iastate.edu  | \n📧 cs.kundann@gmail.com"
  },
  {
    "objectID": "contact.html#visit-us-1",
    "href": "contact.html#visit-us-1",
    "title": "Kundan Kumar",
    "section": "Visit Us",
    "text": "Visit Us\nThe AffCom Lab is located in Fraser Hall Room #455 at the University of Kansas.\n\nStreet Address: 1415 Jayhawk Blvd, Lawrence, KS 66044\nVisitor Parking: 1218 Mississippi St, Lawrence, KS 66045"
  },
  {
    "objectID": "contact.html#email-us-1",
    "href": "contact.html#email-us-1",
    "title": "Kundan Kumar",
    "section": "Email Us",
    "text": "Email Us\n\n\n\n\n\n  \nName *  \nEmail *  \nSubject  — Select a topic — Research Resources Opportunities  \nMessage\n\n\n\nSubmit"
  },
  {
    "objectID": "projects/projetcs.html",
    "href": "projects/projetcs.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Doing real-world projects is, I think, the best way to learn and also to engage the world and find out what the world is all about.\n\n-Ray Kurzweil\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaboration and Competition\n\n\nTrain a pair of agents to play tennis.\n\n\n\n\n\n\n\n\n\n\n\n\nAI-Powered Patient Education System\n\n\nDevelop an AI agent to enhance patient education by delivering personalized, on-demand health information through summaries, comprehension checks, and quizzes about relevant…\n\n\n\n\n\n\n\n\n\n\n\n\nChatbot Design using Retrieval‑Augmented Generation (RAG)\n\n\nBuilt a domain‑specific chatbot integrating vector‑based retrieval with GPT models to provide accurate, context‑aware responses\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Agent Travel Assistant System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\n\n\n\n\n\nCongressional Policy Analysis using ML and HPCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt Injection Attacks on LLM Medical Diagnosis by Symptom Elaboration\n\n\n\nR package\n\n\n\nAttacker is to trick the LLM to generate inappropriate possible medical diagnosis which could mislead the end use\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Of Ventilated and Control Flies\n\n\n\nR package\n\n\n\nHypothesis Analysis of Life Expectancy of Flies in Normal vs ill Ventilated Bottles\n\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification of Foods Based on their Quality\n\n\n\nArcGIS\n\nPython\n\n\n\nML model to assess the quality of fruit from an data set, which could be integrated into a product for use in home kitchens\n\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nP2P File Sharing Protocol\n\n\n\nShiny app\n\n\n\nBuild a peer-to-peer file sharing protocol that keeps track of which peers are sharing and what files are being shared in the network\n\n\n\nJan 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotion Prediction and Detection for Autonomous Vehicles\n\n\n\nAutonomous Systems\n\nDeep Learning\n\nComputer Vision\n\n\n\nDevelop a framework for vehicle detection and motion planning of vehicles in complex driving scenarios\n\n\n\nDec 7, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects1/cv.html",
    "href": "projects1/cv.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects1/cv.html."
  },
  {
    "objectID": "projects1/phuse/index.html",
    "href": "projects1/phuse/index.html",
    "title": "PHUSE - CAMIS",
    "section": "",
    "text": "I am contributing to two working groups at PHUSE: CAMIS, and RWD - Real World Data Guideline (early stage).\n\nCAMIS: Comparing Analysis Method Implementations in Software\nCAMIS is a cross-industry group formed of members from PHUSE, PSI and ASA.\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comparison and comprehensive explanations.\n\n\nRWD Working Group\nThis is a newly formed working group, working on statistical programming guidelines while working on RWD (read world data).\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “PHUSE - CAMIS.” https://kundan-kumarr.github.io/projects1/phuse/."
  },
  {
    "objectID": "projects1/sykdomspulsen/index.html",
    "href": "projects1/sykdomspulsen/index.html",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "projects1/sykdomspulsen/index.html#overview",
    "href": "projects1/sykdomspulsen/index.html#overview",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "projects1/projects.html",
    "href": "projects1/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Projects.” https://kundan-kumarr.github.io/projects1/projects.html."
  },
  {
    "objectID": "projects1/dan/index.html",
    "href": "projects1/dan/index.html",
    "title": "Data Apothecary’s Notes",
    "section": "",
    "text": "(This is my own note-taking system using quarto)\n\nAbout the notes\nData Apothecary’s Notes is a note-taking repository for modern data science skills with a focus on drug development and clinical trials. Content will be gradually added while I learn the topics. Therefore, it is by no means a complete guide by the time you read it!\nI try to organize the content in a modular way. I think these should cover the important aspects in which a data scientist / modern statistician should know.\n\nstudy design\ninference\nmodels\nreporting\nprogramming\n\n\n\nWhy quarto\nIn short, quarto has the advantage of making a very well structured website with code chunks easy. No more worry about scattered notes in different folders - put them together, render it so you can find your notes quickly!\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Data Apothecary’s Notes.” https://kundan-kumarr.github.io/projects1/dan/."
  },
  {
    "objectID": "projects1/dl.html",
    "href": "projects1/dl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects1/dl.html."
  },
  {
    "objectID": "projects1/nor_mortality/index.html",
    "href": "projects1/nor_mortality/index.html",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects1/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "projects1/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects1/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "projects1/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Mortality Surveillance in Norway",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "projects1/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "projects1/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Mortality Surveillance in Norway",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "projects1/drl.html",
    "href": "projects1/drl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects1/drl.html."
  },
  {
    "objectID": "projects/orcas.html",
    "href": "projects/orcas.html",
    "title": "Prompt Injection Attacks on LLM Medical Diagnosis by Symptom Elaboration",
    "section": "",
    "text": "Code \nThe goal of orcas is to scrape orca sighting data from the web and visualize it in maps and tables.\nI’ve always had an affinity for the Southern Resident Killer Whales in the Salish Sea. The Center for Whale Research does a lot of really fascinating and important work monitoring their population. They post their survey data on their website; each encounter with the orcas is a separate webpage. I was both curious and intimidated by web scraping so I decided this would make a great case study and personal project. I also learned how to use custom icons in leaflet maps! 🐋"
  },
  {
    "objectID": "projects/roadmap.html",
    "href": "projects/roadmap.html",
    "title": "Classification of Foods Based on their Quality",
    "section": "",
    "text": "App  Code \nThe Soil Health Roadmap is a science-based guide to maintaining and improving soil health in eight focus areas across Washington state.\nFor each focus area, I created crop and soil maps using the arcpy Python package. I then summarized the crop acreage and soil properties in interactive ArcGIS dashboards to complement the Paper."
  },
  {
    "objectID": "projects/washi.html",
    "href": "projects/washi.html",
    "title": "Motion Prediction and Detection for Autonomous Vehicles",
    "section": "",
    "text": "Google Colab  YOLOv5 Code  PyTorch Hub \nInspired by other open-source deep learning projects such as YOLO, ResNet, and Code, this project demonstrates object detection and motion prediction for autonomous vehicles using the Kaggle Lyft motion prediction dataset. We use YOLOv5 for real-time object detection and ResNet-50 for motion trajectory prediction."
  },
  {
    "objectID": "projects/wacse.html",
    "href": "projects/wacse.html",
    "title": "P2P File Sharing Protocol",
    "section": "",
    "text": "App  Code \nThe Washington State Department of Agriculture developed WaCSE for the Washington State Conservation Commission to use in the Sustainable Farms and Fields (SFF) program. Intended users are the Conservation Commission, conservation districts, growers, and anyone interested in reducing agricultural greenhouse gas (GHG) emissions. This interactive tool estimates the reduction of GHG emissions from different conservation practices across Washington’s diverse counties."
  },
  {
    "objectID": "projects1/prohect2.html",
    "href": "projects1/prohect2.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects1/os_teaching/index.html",
    "href": "projects1/os_teaching/index.html",
    "title": "Teach in R and Quarto",
    "section": "",
    "text": "MF9130E - Introductory course in statistics\n8-day intensive course on introductory statistics. April 2023 we made it with R rather than propriety software, coupled with live-coding sessions to enhance understanding of basic concepts such as distribution and hypothesis tests.\nRepository\nCourse website\nRead more about the experience in\n\nblogpost\npresentation\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Teach in R and Quarto.” https://kundan-kumarr.github.io/projects1/os_teaching/."
  },
  {
    "objectID": "projects1/robo.html",
    "href": "projects1/robo.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects1/robo.html."
  },
  {
    "objectID": "projects1/projetcs.html",
    "href": "projects1/projetcs.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Projects.” https://kundan-kumarr.github.io/projects1/projetcs.html."
  },
  {
    "objectID": "projects1/noreden/index.html",
    "href": "projects1/noreden/index.html",
    "title": "Noreden",
    "section": "",
    "text": "Noreden\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Noreden.” https://kundan-kumarr.github.io/projects1/noreden/."
  },
  {
    "objectID": "projects1/ehr/index.html",
    "href": "projects1/ehr/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Ggehr.” https://kundan-kumarr.github.io/projects1/ehr/."
  },
  {
    "objectID": "projects1/stat.html",
    "href": "projects1/stat.html",
    "title": "Statistics",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Statistics.” https://kundan-kumarr.github.io/projects1/stat.html."
  },
  {
    "objectID": "projects1/projects1.html",
    "href": "projects1/projects1.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rpkg/rpkg2.html",
    "href": "rpkg/rpkg2.html",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\n⚙️ Physics-informed Deep Reinforcement Learning (DRL)\n🔍 Probabilistic & Bayesian Modeling\n🧠 Large Language Models (LLMs) for autonomous reasoning\n🖼️ Vision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg2.html#research-vision",
    "href": "rpkg/rpkg2.html#research-vision",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\n⚙️ Physics-informed Deep Reinforcement Learning (DRL)\n🔍 Probabilistic & Bayesian Modeling\n🧠 Large Language Models (LLMs) for autonomous reasoning\n🖼️ Vision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg2.html#research-themes",
    "href": "rpkg/rpkg2.html#research-themes",
    "title": "Research",
    "section": "🧠 Research Themes",
    "text": "🧠 Research Themes\n\n🔒 Safe & Trustworthy Reinforcement Learning\nDeveloping control agents that ensure system safety while learning in uncertain and partially observable environments. - Constrained policy optimization and reward shaping - Physics-based priors in DRL - Adversarial resilience and anomaly detection - Epistemic and aleatoric uncertainty quantification\n\n\n🧬 Transfer Learning & Meta-Adaptation\nEnabling rapid generalization across distribution shifts in topology, weather, or load profiles. - Transferable actor-critic architectures - Simulation-to-real (Sim2Real) adaptation - Meta-RL for sample efficiency\n\n\n🧿 Vision-Simulation Integration\nCombining synthetic perception and sensor data with RL agents. - Perception-action loops using CARLA, AirSim - Multi-modal representation learning - End-to-end autonomous control pipelines\n\n\n🧠 LLM-Augmented Decision Systems\nLeveraging large language models to assist or guide RL agents with contextual reasoning. - LLMs for environment summarization & trajectory guidance - Text-to-policy translation - Human-AI collaborative control"
  },
  {
    "objectID": "rpkg/rpkg2.html#domains-of-application",
    "href": "rpkg/rpkg2.html#domains-of-application",
    "title": "Research",
    "section": "🔬 Domains of Application",
    "text": "🔬 Domains of Application\n\n\n\n\n\n\n\nDomain\nDescription\n\n\n\n\n⚡ Smart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\n🚘 Autonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world driving environments\n\n\n🛡 Secure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/rpkg2.html#selected-publications",
    "href": "rpkg/rpkg2.html#selected-publications",
    "title": "Research",
    "section": "📚 Selected Publications",
    "text": "📚 Selected Publications\n\n📝 Journal Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster"
  },
  {
    "objectID": "rpkg/rpkg2.html#ongoing-projects",
    "href": "rpkg/rpkg2.html#ongoing-projects",
    "title": "Research",
    "section": "🔄 Ongoing Projects",
    "text": "🔄 Ongoing Projects\n\n🤖 Federated DRL for Cyber-Resilient Volt-VAR Optimization\nDecentralized, communication-efficient control using LSTM-enhanced PPO agents across distributed DERs.\n⚡ One-Shot Policy Transfer with Physics Priors\nTrain agents on small topologies and adapt to IEEE 123-bus, 8500-node networks in a few iterations.\n🧠 LLM-Guided Autonomous Planning for Smart Buildings\nConvert user prompts to interpretable control policies using LLMs (OpenAI, Claude) in CityLearn environments."
  },
  {
    "objectID": "rpkg/rpkg2.html#research-themes-1",
    "href": "rpkg/rpkg2.html#research-themes-1",
    "title": "Research",
    "section": "🔬 Research Themes",
    "text": "🔬 Research Themes\n\n\n\n&lt;a href=\"safe-rl.qmd\" style=\"text-decoration: none; color: inherit;\"&gt;\n  &lt;div style=\"border: 2px solid #1e567d; border-radius: 10px; padding: 1.2rem; background: #f9f9f9; height: 100%;\"&gt;\n    &lt;h4 style=\"color: #1e567d;\"&gt;&lt;i class=\"fas fa-shield-alt\"&gt;&lt;/i&gt; Safe Reinforcement Learning&lt;/h4&gt;\n    &lt;p&gt;\n      Designing control agents that learn safely and robustly in high-stakes environments. Includes physics-based constraints, uncertainty-aware exploration, and adversarial resilience in AI systems.\n    &lt;/p&gt;\n  &lt;/div&gt;\n&lt;/a&gt;\n\n\n\n&lt;a href=\"transfer-learning.qmd\" style=\"text-decoration: none; color: inherit;\"&gt;\n  &lt;div style=\"border: 2px solid #1e567d; border-radius: 10px; padding: 1.2rem; background: #f9f9f9; height: 100%;\"&gt;\n    &lt;h4 style=\"color: #1e567d;\"&gt;&lt;i class=\"fas fa-sync-alt\"&gt;&lt;/i&gt; Transfer & Meta-Learning&lt;/h4&gt;\n    &lt;p&gt;\n      Enabling policy generalization across grids, seasons, and topologies using simulation-to-real, one-shot learning, and meta-learning strategies.\n    &lt;/p&gt;\n  &lt;/div&gt;\n&lt;/a&gt;\n\n\n\n&lt;a href=\"vision-simulation.qmd\" style=\"text-decoration: none; color: inherit;\"&gt;\n  &lt;div style=\"border: 2px solid #1e567d; border-radius: 10px; padding: 1.2rem; background: #f9f9f9; height: 100%;\"&gt;\n    &lt;h4 style=\"color: #1e567d;\"&gt;&lt;i class=\"fas fa-video\"&gt;&lt;/i&gt; Vision-Based Simulation&lt;/h4&gt;\n    &lt;p&gt;\n      Bridging perception and control with synthetic environments like CARLA. Developing pipelines that combine object detection with decision-making in robotics and autonomous driving.\n    &lt;/p&gt;\n  &lt;/div&gt;\n&lt;/a&gt;\n\n\n\n&lt;a href=\"llm-control.qmd\" style=\"text-decoration: none; color: inherit;\"&gt;\n  &lt;div style=\"border: 2px solid #1e567d; border-radius: 10px; padding: 1.2rem; background: #f9f9f9; height: 100%;\"&gt;\n    &lt;h4 style=\"color: #1e567d;\"&gt;&lt;i class=\"fas fa-robot\"&gt;&lt;/i&gt; LLM-Augmented Control&lt;/h4&gt;\n    &lt;p&gt;\n      Integrating large language models into control frameworks for prompt-driven planning, explainable reasoning, and human-in-the-loop decision-making.\n    &lt;/p&gt;\n  &lt;/div&gt;\n&lt;/a&gt;"
  },
  {
    "objectID": "rpkg/rpkg3.html",
    "href": "rpkg/rpkg3.html",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg3.html#research-vision",
    "href": "rpkg/rpkg3.html#research-vision",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg3.html#my-research-framework",
    "href": "rpkg/rpkg3.html#my-research-framework",
    "title": "Research",
    "section": "🧭 My Research Framework",
    "text": "🧭 My Research Framework\n\n\nMy Research Framework\n\n\n&lt;!-- Learning Core --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #e6f4ea; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Learning Core&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;Physics-Informed RL&lt;/li&gt;\n    &lt;li&gt;Bayesian Modeling&lt;/li&gt;\n    &lt;li&gt;Uncertainty Estimation&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;!-- Reasoning & Transfer --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #fff4d9; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Reasoning & Transfer&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;LLM-Augmented Agents&lt;/li&gt;\n    &lt;li&gt;Meta-RL&lt;/li&gt;\n    &lt;li&gt;Transfer Learning&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;!-- Perception --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #e9ecff; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Perception & Simulation&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;CARLA / OpenDSS&lt;/li&gt;\n    &lt;li&gt;Sensor Fusion&lt;/li&gt;\n    &lt;li&gt;Scene Understanding&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;!-- Safety --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #fde8ec; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Robustness & Safety&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;Adversarial Defense&lt;/li&gt;\n    &lt;li&gt;Safe Exploration&lt;/li&gt;\n    &lt;li&gt;Cyber-Resilience&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;!-- Application --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #e0f0f8; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Applications&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;Smart Grids&lt;/li&gt;\n    &lt;li&gt;Autonomous Systems&lt;/li&gt;\n    &lt;li&gt;Human-AI Collaboration&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n\nUnified through Optimization, AI Safety, and Applied Machine Learning"
  },
  {
    "objectID": "rpkg/rpkg3.html#research-themes",
    "href": "rpkg/rpkg3.html#research-themes",
    "title": "Research",
    "section": "🧠 Research Themes",
    "text": "🧠 Research Themes\n\nSafe & Trustworthy Reinforcement Learning\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nConstrained policy optimization and reward shaping\n\n\nPhysics-based priors in DRL\n\n\nAdversarial resilience and anomaly detection\n\n\nEpistemic and aleatoric uncertainty quantification\n\n\n\n\n\n\n\n\n\nTransfer Learning & Meta-Adaptation\n\n\n\n🎯 Objective\n\n\nEnabling rapid generalization across distribution shifts in topology, weather, or load profiles.\n\n\n🔍 Core Focus Areas\n\n\n\nTransferable actor-critic architectures\n\n\nSimulation-to-real (Sim2Real) adaptation\n\n\nMeta-RL for sample efficiency\n\n\n\n\n\n\n\n\n\nVision-Simulation Integration\n\n\n\n🎯 Objective\n\n\nBridge the gap between perception and control by combining synthetic sensors, simulated environments, and end-to-end learning pipelines.\n\n\n🔍 Core Focus Areas\n\n\n\nPerception-action loops with CARLA, AirSim\n\n\nMulti-modal representation fusion (image + state)\n\n\nAutonomous control with embedded perception modules\n\n\nEnd-to-end autonomous control pipelines\n\n\n\n\n\n\n\n\n\nLLM-Augmented Decision Systems\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nLLMs for summarizing environment states and guiding agents\n\n\nTranslating textual inputs into actionable policies\n\n\nFacilitating human-AI collaboration in dynamic tasks"
  },
  {
    "objectID": "rpkg/rpkg3.html#application-domains",
    "href": "rpkg/rpkg3.html#application-domains",
    "title": "Research",
    "section": "🔬 Application Domains",
    "text": "🔬 Application Domains\n\n\n\n\n\n\n\nDomain\nDescription\n\n\n\n\n⚡ Smart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\n🚘 Autonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world driving environments\n\n\n🛡 Secure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/rpkg3.html#publications",
    "href": "rpkg/rpkg3.html#publications",
    "title": "Research",
    "section": "📚 Selected Publications",
    "text": "📚 Selected Publications\n\n📝 Journal Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster"
  },
  {
    "objectID": "rpkg/rpkg3.html#ongoing-projects",
    "href": "rpkg/rpkg3.html#ongoing-projects",
    "title": "Research",
    "section": "🧪 Ongoing Projects",
    "text": "🧪 Ongoing Projects\n\nFederated DRL for Cyber-Resilient Volt-VAR Optimization\nExploring decentralized, communication-efficient coordination among DERs using LSTM-enhanced PPO agents.\nOne-Shot Policy Transfer with Physics Priors\nTraining grid agents in simpler environments and adapting to complex grids (IEEE 123-bus, 8500-node) in few iterations.\nLLM-Guided Autonomous Planning for Smart Buildings\nIntegrating OpenAI and Claude with CityLearn to convert user prompts into interpretable policy instructions."
  },
  {
    "objectID": "rpkg/rpkg.html#selected-publications",
    "href": "rpkg/rpkg.html#selected-publications",
    "title": "Kundan Kumar",
    "section": "",
    "text": "📝 Journal Papers🎤 Conference Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nPage 1Page 2\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n\n\n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n1 2\n\n\n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n1 2"
  },
  {
    "objectID": "rpkg/publications.html",
    "href": "rpkg/publications.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "📝 Journal Papers🎤 Conference Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nPage 1Page 2\n\n\n\nKundan Kumar, Kumar Utkarsh, Wang Jiyu and Padullaparti Harsha Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems\n\nIEEE PES General Meeting, 2025\n Paper Code Poster \n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n\n\n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n1 2\n\n\n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n1 2"
  },
  {
    "objectID": "rpkg/publications.html#selected-publications",
    "href": "rpkg/publications.html#selected-publications",
    "title": "Kundan Kumar",
    "section": "",
    "text": "📝 Journal Papers🎤 Conference Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nPage 1Page 2\n\n\n\nKundan Kumar, Kumar Utkarsh, Wang Jiyu and Padullaparti Harsha Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems\n\nIEEE PES General Meeting, 2025\n Paper Code Poster \n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n\n\n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n1 2\n\n\n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n1 2"
  },
  {
    "objectID": "rpkg/rpkg.html#research-themes",
    "href": "rpkg/rpkg.html#research-themes",
    "title": "Research",
    "section": "🧠 Research Themes",
    "text": "🧠 Research Themes\n\nSafe & Trustworthy Reinforcement Learning\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nConstrained policy optimization and reward shaping\n\n\nPhysics-based priors in DRL\n\n\nAdversarial resilience and anomaly detection\n\n\nEpistemic and aleatoric uncertainty quantification\n\n\n\n\n\n\n\n\n\nTransfer Learning & Meta-Adaptation\n\n\n\n🎯 Objective\n\n\nEnabling rapid generalization across distribution shifts in topology, weather, or load profiles.\n\n\n🔍 Core Focus Areas\n\n\n\nTransferable actor-critic architectures\n\n\nSimulation-to-real (Sim2Real) adaptation\n\n\nMeta-RL for sample efficiency\n\n\n\n\n\n\n\n\n\nVision-Simulation Integration\n\n\n\n🎯 Objective\n\n\nBridge the gap between perception and control by combining synthetic sensors, simulated environments, and end-to-end learning pipelines.\n\n\n🔍 Core Focus Areas\n\n\n\nPerception-action loops with CARLA, AirSim\n\n\nMulti-modal representation fusion (image + state)\n\n\nAutonomous control with embedded perception modules\n\n\nEnd-to-end autonomous control pipelines\n\n\n\n\n\n\n\n\n\nLLM-Augmented Decision Systems\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nLLMs for summarizing environment states and guiding agents\n\n\nTranslating textual inputs into actionable policies\n\n\nFacilitating human-AI collaboration in dynamic tasks"
  },
  {
    "objectID": "rpkg/rpkg.html#application-domains",
    "href": "rpkg/rpkg.html#application-domains",
    "title": "Research",
    "section": "🔬 Application Domains",
    "text": "🔬 Application Domains\n\n\n\n\n\n\n\nDomain\nDescription\n\n\n\n\n⚡ Smart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\n🚘 Autonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world driving environments\n\n\n🛡 Secure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/rpkg.html#ongoing-projects",
    "href": "rpkg/rpkg.html#ongoing-projects",
    "title": "Research",
    "section": "🔄 Ongoing Projects",
    "text": "🔄 Ongoing Projects\n\nFederated DRL for Cyber-Resilient Volt-VAR Optimization\nExploring decentralized, communication-efficient coordination among DERs using LSTM-enhanced PPO agents.\nOne-Shot Policy Transfer with Physics Priors\nTraining grid agents in simpler environments and adapting to complex grids (IEEE 123-bus, 8500-node) in few iterations.\nLLM-Guided Autonomous Planning for Smart Buildings\nIntegrating OpenAI and Claude with CityLearn to convert user prompts into interpretable policy instructions."
  },
  {
    "objectID": "blog/blog1.html",
    "href": "blog/blog1.html",
    "title": "Blogs",
    "section": "",
    "text": "Parameterized Reports with Quarto: R-Ladies Abuja Workshop\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports with Quarto: R-Ladies DC Workshop\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Quarto reports improve understanding of soil health\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nagriculture\n\n\nsoil health\n\n\n\nCreating custom soil health reports with Quarto\n\n\n\n\n\nSep 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nShiny optimization of climate benefits from a statewide agricultural grant program\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nagriculture\n\n\nclimate\n\n\n\nDevelopment process of {WaCSE} shiny app\n\n\n\n\n\nAug 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Soil Health Initative and Climate Smart Estimator\n\n\n\n\n\n\nagriculture\n\n\nsoil health\n\n\nclimate\n\n\n\nOverview of the WA Soil Health Initative & WA Climate Smart Estimator {WaCSE} shiny app\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping & Mapping {orcas} Encounters\n\n\n\n\n\n\nR\n\n\nweb scraping\n\n\nleaflet\n\n\n\nWeb scraping with {rvest} & mapping with {leaflet}\n\n\n\n\n\nApr 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Climate Smart Estimator: Using ArcGIS Dashboards and Experience Builder\n\n\n\n\n\n\nArcGIS\n\n\nagriculture\n\n\nclimate\n\n\n\nOverview of WA Climate Smart Estimator ArcGIS Experience web app\n\n\n\n\n\nMay 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\n\n\n\nagriculture\n\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\n\n\n\nagriculture\n\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/blog1.html#blogs-notes",
    "href": "blog/blog1.html#blogs-notes",
    "title": "Blogs & Notes",
    "section": "",
    "text": "📝 Blogs\n\n\nA collection of articles and personal writings about machine learning, smart energy systems, and academic reflections.\ncontents: - “blog_*/index.qmd” type: table fields: [title, date] sort: “date desc” categories: true grid-item-border: false date-format: short sort-ui: false filter-ui: false"
  },
  {
    "objectID": "blog/talks/2023-08-19_cascadia_shiny-wacse/index.html",
    "href": "blog/talks/2023-08-19_cascadia_shiny-wacse/index.html",
    "title": "Shiny optimization of climate benefits from a statewide agricultural grant program",
    "section": "",
    "text": "{{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code {{&lt; fa play-circle &gt;}} Video \n\nDetails\n📆 August 19, 2023 // 2:05 pm - 2:20 pm PT\n🏨 Seattle, WA\n🌠 Cascadia R Conf\n\n\nAbstract\nWashington’s Sustainable Farms and Fields program provides grants to growers to increase soil carbon or reduce greenhouse gas (GHG) emissions on their farms. To optimize the climate benefits of the program, we developed the Washington Climate Smart Estimator {WaCSE} using R and Shiny.\nIntegrating national climate models and datasets, this intuitive, regionally specific user interface allows farmers and policymakers to compare the climate benefits of different agricultural practices across Washington’s diverse counties and farm sizes. Users can explore GHG estimates in interactive tables and plots, download results in spreadsheets and figures, and generate PDF reports. In this talk, we present the development process of {WaCSE} and discuss the lessons we learned from creating our first ever Shiny app.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Shiny Optimization of Climate Benefits from a Statewide\n    Agricultural Grant Program},\n  date = {2023-08-19},\n  url = {https://kundan-kumarr.github.io/blog/talks/2023-08-19_cascadia_shiny-wacse/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2023. “Shiny\nOptimization of Climate Benefits from a Statewide Agricultural Grant\nProgram.” August 19, 2023. https://kundan-kumarr.github.io/blog/talks/2023-08-19_cascadia_shiny-wacse/."
  },
  {
    "objectID": "blog/talks/2021-11-14_setac_insecticides_water/index.html",
    "href": "blog/talks/2021-11-14_setac_insecticides_water/index.html",
    "title": "Aquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters",
    "section": "",
    "text": "{{&lt; fa display &gt;}} Slides\n\nDetails\n📆 November 14, 2021 // 12-minute recording\n🏨 Virtual\n🌠 Society of Environmental Toxicology and Chemistry (SETAC) North America 42nd Annual Meeting\n\n\nAbstract\nEcological risk assessments often do not consider potential additive, synergistic, or antagonistic effects from mixtures of chemicals and instead typically base risk on a single chemical. In the last decade, more tools and models have been developed to consider the interactive effects of chemicals within a mixture when conducting risk assessments. Therefore, this study uses actual environmental concentrations measured in 2018 and 2019 from the Washington State Department of Agriculture’s Surface Water Monitoring Program. Aquatic risk from exposure was assessed from chlorpyrifos, diazinon, and malathion (as individual chemicals and as binary and ternary mixtures) using the concentration addition model. These pesticides were selected because they have a common mechanism of toxicity, are frequently detected in surface waters in Washington, and were recently evaluated in a biological opinion by the National Marine Fisheries Service.\nAll detected concentrations of chlorpyrifos and malathion, assessed as individual chemicals, exceeded the predicted no effect concentration, indicating potential for adverse effects on aquatic life. Further, risk quotients for all binary and ternary mixtures were greater than one, also indicative of potential for adverse effects on aquatic life. In all samples containing a mixture, the maximum cumulative ratio suggested that a single insecticide contributed &gt;50% of the overall toxicity of each mixture. Based on the individual and mixture risk quotients, chlorpyrifos and malathion were the primary drivers of the toxicity of each mixture.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2021,\n  author = {Ryan, Jadey},\n  title = {Aquatic {Risk} {Assessment:} {Organophosphate} Insecticide\n    Mixtures in {Washington} Surface Waters},\n  date = {2021-11-14},\n  url = {https://kundan-kumarr.github.io/blog/talks/2021-11-14_setac_insecticides_water/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2021. “Aquatic Risk Assessment: Organophosphate\nInsecticide Mixtures in Washington Surface Waters.” November 14,\n2021. https://kundan-kumarr.github.io/blog/talks/2021-11-14_setac_insecticides_water/."
  },
  {
    "objectID": "blog/talks/2024-02-21_rladies-abuja-quarto-params/index.html",
    "href": "blog/talks/2024-02-21_rladies-abuja-quarto-params/index.html",
    "title": "Parameterized Reports with Quarto: R-Ladies Abuja Workshop",
    "section": "",
    "text": "{{&lt; fa globe &gt;}} Course website {{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code {{&lt; fa play-circle &gt;}} Video \n\nDetails\n📆 February 21, 2024 // 4:30 pm - 6:30 pm WAT\n🏨 Virtual\n🆓 FREE with registration\n🎥 Recording\n🏡 Workshop website\n🔖 Source tag\n\n\nAbstract\nTired of manually adjusting Quarto reports for different regions, time periods, or clients? Dreaming of using just one template to generate both interactive HTML and static Word/PDF versions of your reports?\nJoin our workshop to unlock the power of parameterized reporting with Quarto and leave with your own template and examples to modify for your own projects.\nGet a sneak preview of what you’ll learn by checking out the slides from my posit::conf(2023) talk.\nWe welcome everyone! However, if you’re new to Quarto or functional programming with {purrr}, take a look at the pre-work for some background videos/tutorials.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2024,\n  author = {Ryan, Jadey},\n  title = {Parameterized {Reports} with {Quarto:} {R-Ladies} {Abuja}\n    {Workshop}},\n  date = {2024-02-21},\n  url = {https://kundan-kumarr.github.io/blog/talks/2024-02-21_rladies-abuja-quarto-params/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2024. “Parameterized Reports with Quarto: R-Ladies\nAbuja Workshop.” February 21, 2024. https://kundan-kumarr.github.io/blog/talks/2024-02-21_rladies-abuja-quarto-params/."
  },
  {
    "objectID": "blog/talks/2024-01-18_rladies-dc_quarto-params/index.html",
    "href": "blog/talks/2024-01-18_rladies-dc_quarto-params/index.html",
    "title": "Parameterized Reports with Quarto: R-Ladies DC Workshop",
    "section": "",
    "text": "{{&lt; fa globe &gt;}} Course website {{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code {{&lt; fa play-circle &gt;}} Video \n\nDetails\n📆 January 18, 2024 // 6:30 pm - 8:30 pm EDT\n🏨 Virtual\n🆓 FREE with registration\n🏡 Workshop website\n🔖 Source tag\n\n\nAbstract\nTired of manually adjusting Quarto reports for different regions, time periods, or clients? Dreaming of using just one template to generate both interactive HTML and static Word/PDF versions of your reports?\nJoin our workshop to unlock the power of parameterized reporting with Quarto and leave with your own template and examples to modify for your own projects.\nGet a sneak preview of what you’ll learn by checking out the slides for my posit::conf(2023) talk.\nEveryone is welcome to attend. If you’re new to Quarto, we recommend watching Tom Mock’s excellent 2-hour introduction to Quarto.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2024,\n  author = {Ryan, Jadey},\n  title = {Parameterized {Reports} with {Quarto:} {R-Ladies} {DC}\n    {Workshop}},\n  date = {2024-01-18},\n  url = {https://kundan-kumarr.github.io/blog/talks/2024-01-18_rladies-dc_quarto-params/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2024. “Parameterized Reports with Quarto: R-Ladies DC\nWorkshop.” January 18, 2024. https://kundan-kumarr.github.io/blog/talks/2024-01-18_rladies-dc_quarto-params/."
  },
  {
    "objectID": "blog/talks/2021-11-09_awra_insecticides_water/index.html",
    "href": "blog/talks/2021-11-09_awra_insecticides_water/index.html",
    "title": "Aquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters",
    "section": "",
    "text": "Slides\n\nDetails\n📆 November 9, 2021 // 1:30 pm - 1:50 pm PT\n🏨 Virtual\n🌠 American Water Resources Association (AWRA)\n\n\nAbstract\nEcological risk assessments often do not consider potential additive, synergistic, or antagonistic effects from mixtures of chemicals and instead typically base risk on a single chemical. In the last decade, more tools and models have been developed to consider the interactive effects of chemicals within a mixture when conducting risk assessments. Therefore, this study uses actual environmental concentrations measured in 2018 and 2019 from the Washington State Department of Agriculture’s Surface Water Monitoring Program. Aquatic risk from exposure was assessed from chlorpyrifos, diazinon, and malathion (as individual chemicals and as binary and ternary mixtures) using the concentration addition model. These pesticides were selected because they have a common mechanism of toxicity, are frequently detected in surface waters in Washington, and were recently evaluated in a biological opinion by the National Marine Fisheries Service.\nAll detected concentrations of chlorpyrifos and malathion, assessed as individual chemicals, exceeded the predicted no effect concentration, indicating potential for adverse effects on aquatic life. Further, risk quotients for all binary and ternary mixtures were greater than one, also indicative of potential for adverse effects on aquatic life. In all samples containing a mixture, the maximum cumulative ratio suggested that a single insecticide contributed &gt;50% of the overall toxicity of each mixture. Based on the individual and mixture risk quotients, chlorpyrifos and malathion were the primary drivers of the toxicity of each mixture.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2021,\n  author = {Ryan, Jadey},\n  title = {Aquatic {Risk} {Assessment:} {Organophosphate} Insecticide\n    Mixtures in {Washington} Surface Waters},\n  date = {2021-11-09},\n  url = {https://kundan-kumarr.github.io/blog/talks/2021-11-09_awra_insecticides_water/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2021. “Aquatic Risk Assessment: Organophosphate\nInsecticide Mixtures in Washington Surface Waters.” November 9,\n2021. https://kundan-kumarr.github.io/blog/talks/2021-11-09_awra_insecticides_water/."
  },
  {
    "objectID": "blog/talks/2022-05-25_wagisa_arcgis-wacse/index.html",
    "href": "blog/talks/2022-05-25_wagisa_arcgis-wacse/index.html",
    "title": "Washington Climate Smart Estimator: Using ArcGIS Dashboards and Experience Builder",
    "section": "",
    "text": "{{&lt; fa file-pdf &gt;}} Slides {{&lt; fa play-circle &gt;}} Recording \n\nDetails\n📆 May 25, 2022 // 2:30 pm - 3:00 pm PT\n🏨 Leavenworth, WA\n🌠 Washington GIS Association (WAGISA) conference\n\n\nAbstract\nWashington’s Sustainable Farms and Fields (SFF) program provides financial incentives to growers who implement climate-smart practices that sequester soil carbon or reduce greenhouse gas emissions. The climate change mitigation potential of different on-farm practices depends on many site-specific factors such as geography, climate, soil type, and management history.\nTo maximize the climate change mitigation potential of the SFF program, and to optimize the use of every dollar, a decision support tool is required. This presentation demonstrates how two existing spatial datasets (NRCS’ COMET-Planner and WSDA’s Agricultural Land Use) can be integrated into a decision support tool using ArcGIS Dashboards and Experience Builder. This tool, called the Washington Climate Smart Estimator (WaCSE), allows users to compare the climate benefits of different agricultural practices across different counties in Washington.\nBy utilizing the intuitive user interface of ArcGIS Dashboards, WaCSE enables swift, science-based estimates of climate benefits, while remaining accessible for audiences of varied technical backgrounds.\nSee the old app built with ArcGIS.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2022,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Washington {Climate} {Smart} {Estimator:} {Using} {ArcGIS}\n    {Dashboards} and {Experience} {Builder}},\n  date = {2022-05-25},\n  url = {https://kundan-kumarr.github.io/blog/talks/2022-05-25_wagisa_arcgis-wacse/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2022. “Washington\nClimate Smart Estimator: Using ArcGIS Dashboards and Experience\nBuilder.” May 25, 2022. https://kundan-kumarr.github.io/blog/talks/2022-05-25_wagisa_arcgis-wacse/."
  },
  {
    "objectID": "blog/talks/2023-04-20_rladies_orcas-web-scraping/index.html",
    "href": "blog/talks/2023-04-20_rladies_orcas-web-scraping/index.html",
    "title": "Web Scraping & Mapping {orcas} Encounters",
    "section": "",
    "text": "{{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code \n\nDetails\n📆 April 20, 2023 // 15-minute talk\n🏨 Seattle, WA\n🆓 R-Ladies Seattle and Seattle useR Group\n\n\nAbstract\nR-Ladies Seattle invited me to give a talk for the ‘R in the Outdoors’ meetup. This was my first in-person talk of my professional career! I used this as an opportunity to learn new skills through a personal project. I’ve always had an affinity for the Southern Resident Killer Whales in the Salish Sea. The Center for Whale Research does a lot of really fascinating and important work monitoring their population. They post their survey data on their website; each encounter with the orcas is a separate webpage. Lately, I’ve been curious and intimidated by web scraping so I decided this would make a great case study and personal project.\nI ended up also going to the Seattle useR Group lightning talks meetup afterwards and spontaneously gave the same presentation there!\n\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey},\n  title = {Web {Scraping} \\& {Mapping} \\{Orcas\\} {Encounters}},\n  date = {2023-04-20},\n  url = {https://kundan-kumarr.github.io/blog/talks/2023-04-20_rladies_orcas-web-scraping/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2023. “Web Scraping & Mapping {Orcas}\nEncounters.” April 20, 2023. https://kundan-kumarr.github.io/blog/talks/2023-04-20_rladies_orcas-web-scraping/."
  },
  {
    "objectID": "blog/talks/2023-09-25_posit_parameterized-quarto/index.html",
    "href": "blog/talks/2023-09-25_posit_parameterized-quarto/index.html",
    "title": "Parameterized Quarto reports improve understanding of soil health",
    "section": "",
    "text": "{{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code {{&lt; fa play-circle &gt;}} Video \n\nDetails\n📆 September 25, 2023 // 5:30 pm - 5:40 pm CDT 🏨 Chicago, IL\n🌠 posit::conf(2023)\n\n\nAbstract\nSoil sampling data are notoriously challenging to tidy and effectively communicate to farmers. We used functional programming with the tidyverse to reproducibly streamline data cleaning and summarization. To improve project outreach, we developed a Quarto project to dynamically create interactive HTML reports and printable PDFs. Custom to every farmer, reports include project goals, measured parameter descriptions, summary statistics, maps, tables, and graphs.\nOur case study presents a workflow for data preparation and parameterized reporting, with best practices for effective data visualization, interpretation, and accessibility.\nSee an example HTML report.\nLearn more about the Washington Soil Health Initiative State of the Soils Assessment.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and McIlquham, Molly and Sarpong, Kwabena and\n    Michel, Leslie and Potter, Teal and Griffin LaHue, Deirdre and\n    Gelardi, Dani},\n  title = {Parameterized {Quarto} Reports Improve Understanding of Soil\n    Health},\n  date = {2023-09-25},\n  url = {https://kundan-kumarr.github.io/blog/talks/2023-09-25_posit_parameterized-quarto/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Molly McIlquham, Kwabena Sarpong, Leslie Michel, Teal\nPotter, Deirdre Griffin LaHue, and Dani Gelardi. 2023.\n“Parameterized Quarto Reports Improve Understanding of Soil\nHealth.” September 25, 2023. https://kundan-kumarr.github.io/blog/talks/2023-09-25_posit_parameterized-quarto/."
  },
  {
    "objectID": "blog/talks/2023-06-13_wade_washi-wacse/index.html",
    "href": "blog/talks/2023-06-13_wade_washi-wacse/index.html",
    "title": "Washington Soil Health Initative and Climate Smart Estimator",
    "section": "",
    "text": "Slides  Video \n\nDetails\n📆 June 13, 2023 // 1:30 pm - 2:20 pm PT\n🏨 Leavenworth, WA\n🌠 Washington Association of District Employees (WADE) conference\n\n\nAbstract\nWashington Soil Health Initiative overview and updates.\nHow to get the most of the Sustainable Farms and Fields Washington Climate Smart Estimator (WaCSE) tool.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Washington {Soil} {Health} {Initative} and {Climate} {Smart}\n    {Estimator}},\n  date = {2023-06-13},\n  url = {https://kundan-kumarr.github.io/blog/talks/2023-06-13_wade_washi-wacse/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2023. “Washington\nSoil Health Initative and Climate Smart Estimator.” June 13,\n2023. https://kundan-kumarr.github.io/blog/talks/2023-06-13_wade_washi-wacse/."
  },
  {
    "objectID": "blog/blog2.html",
    "href": "blog/blog2.html",
    "title": "Blog and notes",
    "section": "",
    "text": "Blog\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nPersonal Highlights: Positconf 2024\n\n\n11/5/24\n\n\n\n\nUse Quarto, Make Friends: a two-year journey\n\n\n9/23/24\n\n\n\n\nPersonal Highlights: Positconf 2023\n\n\n9/21/23\n\n\n\n\nPersonal Highlights: CEN2023\n\n\n9/6/23\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\n7/17/23\n\n\n\n\nCourse review: making DS work for clinical reporting\n\n\n3/1/23\n\n\n\n\nOpen source reporting with R: clinical, public health, RSE and embrace the change\n\n\n1/13/23\n\n\n\n\nqtwAcademic: a quick and easy way to start your Quarto website\n\n\n1/5/23\n\n\n\n\nWebsite reboot: switching from Blogdown to Quarto\n\n\n1/3/23\n\n\n\n\n\nNo matching items\n\n\n\n\nTechnical notes\nMost of the technical notes are in the newly built note repository, Data Apothecary’s Notes. Please feel free to reach out if you found any errors!\nI’d be glad if it helps you in some way.\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nResearch Scientist Interview Guide\n\n\n7/3/25\n\n\n\n\nNotes: The Book of OHDSI - Data Analytics\n\n\n5/6/24\n\n\n\n\nStyling your quarto project\n\n\n10/18/23\n\n\n\n\nUse WebR in your existing quarto website\n\n\n10/1/23\n\n\n\n\nR package workflow\n\n\n5/19/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 4\n\n\n3/1/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 3\n\n\n2/27/23\n\n\n\n\nTesting Shiny app and deploy to shinyapps.io\n\n\n2/25/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 2\n\n\n2/22/23\n\n\n\n\nR package website with pkgdown\n\n\n2/20/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 1\n\n\n2/6/23\n\n\n\n\nPublishing Quarto Website with GitHub Pages\n\n\n1/11/23\n\n\n\n\n\nNo matching items\n\n\n\n\nReading notes\nThis section is constantly being updated.\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nBad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre\n\n\n6/6/24\n\n\n\n\nWorking in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal\n\n\n2/18/24\n\n\n\n\nPreventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar\n\n\n3/17/23\n\n\n\n\nHow to prevent the next pandemic - Bill Gates\n\n\n1/4/23\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/talks/technotes_20230205_clinreport_part1/index.html",
    "href": "blog/talks/technotes_20230205_clinreport_part1/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/talks/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "href": "blog/talks/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Introduction to clinical trial",
    "text": "Introduction to clinical trial\nClinical trial: aim to demonstrate that drug is safe and effective (safety, efficacy)\n\nphase 1: 10-20 people, focus on safety (healthy volunteers)\nphase 2: 100, study of side effects, determine best dose\nphase 3: 1000, demonstrate drug efficacy, fuller safety profile (common across multiple regions, ethnicities)\n\ncollecting data from different hospitals, hence important to ensure standards are being followed\n\nevidence must be submitted to health authorities (FDA, EMA European medicines agency)\nhealth authorities determine whether the drug is submitted to market\n\nSubmit the analysis plan in advance"
  },
  {
    "objectID": "blog/talks/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "href": "blog/talks/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Why share data",
    "text": "Why share data\n\nregulatory requirements\nscientific community interest\ncompany internal research interest\nmarketing materials\n\n\nData and results sharing\n\nRegulatory req (e.g. EMA require sharing clinical trial results to gain marketing authorization for pharma products, FDA require sharing data)\nscientific community (peer review check accuracy, perform additional analyses, derive new hypothesis)\nCDISC standards\n\nCDASH (clinical data acquisition standards harmonization)\nSDTM (study data tabulation model)\n\nformat for ‘raw’ data, define datasets, structures, contents, variable attributes\n\nSEND (standard for exchange of non clinical data)\nADaM (analysis data model)\n\ndata format for data processed for analysis (e.g. converted, imputed, derived)\n\n\nDictionary\n\ne.g. nose congestion, stuffy nose, … need to be standardized\nMedDRA: standard dictionary for medical conditions, events and procedures\nWHO drug dictionary (for pharma agents)\n\nSAP statistical analysis plan\n\nbased on study protocol, focus on statistical methodology, is regulated\n\nProgramming specification\n\nbased on SAP, provides additional details on datasets and tables, listing and figures (TLFs) required for statistical analysis. focus on programming details. Not regulated\n\n\n\n\nQuality assurance\n\nGood clinical practice (GCP), issued by ICH\npurpose: prevent mistakes, reduce inefficiencies/waste in a process, increase reliability/trustworthiness of the product of a process\nClinical monitoring: performed by a clinical research assistant (CRA) at investigator sites, checks that study protocols are executed as intended, and site processes result in accurate data capture. Focus on trial subjects’ safety. Traditional goal: 100% source data verification\nData quality checks (more relevant for data scientists). Checks data for technical conformance, and data plausibility. Focus on data quality. Traditional goal: 100% accurate and format compliant data\nCode review\nDouble programming\n\n\n\nData access restrictions\n\nreasons\n\ndata collected is very sensitive (health data), need data protection\nclinical trial data is a key asset and revenue predictor for pharma companies, high confidentiality levels\nscientific validity, data is ideally double blinded, no-one should know whether a subjecttreatment is as long as the data is still being collected\n\npseudonymization: data de-identification\n\nuse pseudonym (ID), link is recorded to allow re-identification\n\nanonymization: limit the risk of re-identification\n\nremove variables, remove values, replace more precise values with more general categories, replace personal identifiers with random identifiers\n\nFSP, CRO (out-sourcing), personnels require data access at different levels\nUnblinding"
  },
  {
    "objectID": "blog/talks/readnotes_20240218_open_source/index.html",
    "href": "blog/talks/readnotes_20240218_open_source/index.html",
    "title": "Working in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal",
    "section": "",
    "text": "Github as a platform\n\nOn contribution\nNearly half of all contributors only contributed once; which accounted for less than 2% of total commits.\nThe pattern that one or a few developers do most of the work, followed by many casual contributors and even more passive users is the norm, not exception in open source.\nOn casual contributors: they primarily see themselves as users of the project, rather than a part of a contributor community.\nChallenge for maintainers: not how to get more contributors, but how to manage high volume of frequent, low-touch interactions (directing air traffic)\n\nGithub’s open source developers have more in common with solo creators on Twitter, Instagram, YouTube or Twitch.\n\nComparing early internet and social platform nowadays: the early online communities have mailing lists, online forums, membership groups, operated like villages that have their own culture, history and norms. Nowadays creators have much bigger potential audience but the relationship is one-sided, and can be overwhelming.\n\n\nOn free software and hacker\n“Free” means you are able to do what you want with the software, rather than the cost. Libre rather than gratis. At least at the beginning.\nBravado, showmanship, mischievousness, deep mistrust of authority. This culture in the 1980s and 90s was closely linked to the early open source software.\n“Bazaar”: highly participatory, versus “Cathedral”: restricted to a smaller group\nToday’s developer hardly even notice “open source” as a concept anymore, they just want to write and publish their code. They prioritize convenience over freedom or openness.\n\n\nOn licensing\nThe widespread use of permissive licensing is popularized by GH.\nCopyleft licensing (e.g. GNU General Public License GPL) is not commercial friendly as it requires companies to license their software that depend on open source GPL software to have the same license. However GPL gives developers more control over how others use their code in the long run.\n\nAs with any other online content today, sharing is the default.\n\n\n\n\nThe structure of an open source software\n\nOn how projects evolve\nCreate -&gt; Promote and distribute -&gt; Grow\nProjects are promoted like a founder would promote a startup: share on the relevant channels online, give talks at conference and meetups, encourage others to write and talk about it\nA sign that the software is used widely: when the maintainer starts doing more non-code (triage issues, review pull requests) rather than code work.\n\n\nContributor and users\nDepends on technical scope (whether there is much to do), support required (code and admin work), ease of participation (whether on Github) and user adoption (potential contributor base).\nFour types of projects\n\nhigh user growth, high contributor growth: federations. Rare, impactful, the ‘ideal’ of open source project. Roughly 3% of open source projects. Examples: Rust, Node.js, Linux\nhigh user growth, low contributor growth: stadiums: powered by one or a few developers. Centralized.\nlow user growth, high contributor growth: club. Similar to meetup or hobby groups, do not have a wide reach but are loved and built by enthusiasts.\nlow user growth, low contributor growth: toys. Personal project, isn’t trying to grow its user base. Projects on Github with less than 10 stars. Authors do not expect to receive contributions nor do they care about whether people are watching.\n\nDecentralized communities (clubs and federations) have the potential for high user growth - recruit new contributors, reduce contribution friction.\nCentralized communities (stadium) depends on the creators to manage user demand - automation, elimination of noise\n\n\n\nRoles, incentives and relationships\n\nFirms or communities\nFirms (companies, organizations): centralized resources; from a coordination standpoint, managing resources would be more efficient within the same organization - which does not explain why open source developers make software together without formal contracts and financial compensation.\n\n\nThe commons and peer production\nTragedy of the commons: resources depleted by people acting in their own self-interest rather than in the collective interest.\n(One of the 8 design principles by Ostrom on) successful commons:\n\nThose who are affecteed by the rules can participate in modifying them.\n\nStrong sense of group identity maks rules, dispute resolution more meaningful.\nCoordination cost is lower when self-organized based on who wants to do the work most, anyone can do the advertised work and volunteer.\nIn contrast, in companies - solicit, evaluate, hire, manage employees; only employees can do the work limited by their job functions.\nPeople collaborating online for no obvious reason beyond personal satisfaction (intrinsic motivation)\nModular and granular tasks: how tasks are organized, and how big each task is.\nLow coordination costs: quality control over thee modules, integrate the contributions into the finished product\n\n\nContribution beyond code\nSome users do not consider them a contributor, but do actually contribute by education, spreading the word, support (forum), bug reports and more.\nThese active users are similar to contributors but operate independently from project’s contributor community.\n\n\n\n\n\nCitationBibTeX citation:@online{zhang2024,\n  author = {Zhang, Chi},\n  title = {Working in {Public:} {The} {Making} and {Maintenance} of\n    {Open} {Source} {Software} - {Nadia} {Eghbal}},\n  date = {2024-02-18},\n  url = {https://kundan-kumarr.github.io/blog/talks/readnotes_20240218_open_source/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2024. “Working in Public: The Making and Maintenance\nof Open Source Software - Nadia Eghbal.” February 18, 2024. https://kundan-kumarr.github.io/blog/talks/readnotes_20240218_open_source/."
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html",
    "href": "blog/talks/blog_20241105_positconf2024/index.html",
    "title": "Personal Highlights: Positconf 2024",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html#quarto",
    "href": "blog/talks/blog_20241105_positconf2024/index.html#quarto",
    "title": "Personal Highlights: Positconf 2024",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html#python",
    "href": "blog/talks/blog_20241105_positconf2024/index.html#python",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Python",
    "text": "Python\nEmily Riederer: Python Rgonomics\nPython alternatives to R. Worth rewatching!"
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html#teaching-and-education",
    "href": "blog/talks/blog_20241105_positconf2024/index.html#teaching-and-education",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Teaching and education",
    "text": "Teaching and education\nAndrew Gard: Teaching and learning data science in the era of AI\nStudents don’t know enough to be able to edit the prompt to reach a sensible code chunk, AI guessed and guessed wrong. We should not expect AI to guess information that we do not provide!\nStudents should still learn to code, and teachers should ask better questions - instead of asking for the final result (create a bar plot), ask students to critically think: why doesn’t the AI-generated code work? what information is missing? how do you improve the prompt?\nJames Wade: Posit Academy in the Age of Generative AI - Lessons from the Frontlines\nchattr, gptstudio, github copilot\nPosit Academy learners (over half) give AI code assistants 2 star rating or less\nRewarding, high-growth period. Threshold concepts: once understood, transforms your perception and approach of a discipline, and these must be encountered not told.\nTC in DS:\n\ntidy data enables efficient analysis\nmodular code enhances re-usablity and clarity\nvisualization as a tool for exploration and communication\n\nHow to incorporate AI code assistants (in DS class)\n\nearly stage: explain this code piece by piece\nmid stage: add a roxygen skeleton to my code\nlate stage: try code assistants in the IDE\n\nTC for code assistants:\n\ndrive faster but don’t forget to steer\nprompting matters, learning how to use these tools is a skill"
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html#statistics",
    "href": "blog/talks/blog_20241105_positconf2024/index.html#statistics",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Statistics",
    "text": "Statistics\nHannah Frick: tidymodels for time-to-event data\nMax Kuhn: Evaluating time-to-event models is hard\nDemetri Pananos - Making sense of marginal effects"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Open source packages",
    "text": "Open source packages\nExmample:\n\nsurvival: 8 developers, &gt;18 years\nadmiral: 25 developers, &gt;1 year\ntern: 77 developers, 5 years\nrtables: 21 developers, 4 years\n\nEngagement across these packages is different, some receive more issues and comments, some receive more code contributions.\nStale: stable? abandoned?\nContribution is highly skewed, a few contributors write the majority of the code.\nR package life cycles (indicative, not guaranteed)\n\nexperimental (ready to use?)\nstable (safe to use?)\ndeprecated, no longer maintained\nsuperseded, something better exists\n&lt;1.0: big changes likely; &gt;=v1.0: is it safe to use?"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Risk mitigation for R packages",
    "text": "Risk mitigation for R packages\nCombine external and internal packages (CI/CD release)\n-&gt; automated package data collection\n-&gt; automated quality checks: if not pass, assess\n-&gt; package repo integration tests\n-&gt; publish to package repo, generate package validation report"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Assess external packages for statistical methods",
    "text": "Assess external packages for statistical methods\nDoes it provide the required functionality?\n\nCorrect statistical method?\nCould you extend it?\nCorrect results? (compare with another software)\nDo you understand the method? (check the paper linked with package)\n\nDoes it work reliably?\n\nPublished? (e.g. on CRAN)\nDifferent inputs?\nFast?\nDo other people use it? (downloads)\nDoes other software use it? (reverse dependencies)\n\nDoes the code look robust and well tested?\n\nHow are the functions implemented\nIs the source code readable\nCoverage with unit tests\nMature package?\n\nIs it well documented?\n\nDocumented functions?\nVignettes?\nPublished?\nInformative NEWS entry?\n\nWho are the authors, are they responsive?\n\nDid they publish statistics papers on this topic\nIs a github site with issues available"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html#tools",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html#tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Tools",
    "text": "Tools\ncovr and unit tests\nriskmetric and the R Validation Hub\npharmaverse.org, with end-to-end examples"
  },
  {
    "objectID": "blog/talks/blog_20230904_cen2023/index.html",
    "href": "blog/talks/blog_20230904_cen2023/index.html",
    "title": "Personal Highlights: CEN2023",
    "section": "",
    "text": "The IBS (International Biometric Society) conference of the Central European Network, CEN2023 has been a great opportunity to keep myself up to date with the latest development of biostatistics, both in academia and industry. Thanks to the great effort made by the organizing committee and almighty Google Meet/Zoom, I have been able to follow the talks without any issue, and have definitely learned a lot.\nGiven my background, I paid more attention on talks and workshops on\n\nStatistical software, R programming and simulation\nCausal inference\n\nThere were also two topics that drew my attention: one is on statistical education towards medial professionals, the other is on a Data Challenge using RCT data.\n\nStatistical software\n\nSoftware Engineering Working Group (SWE WG), MMRM\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nDaniel Sabanes Bove (Roche). First year of the Software Engineering working group - working together across organizations\nGonzalo Duran-Pacheco (Roche). Comparing R libraries with SAS’s PROC MIXED for the analysis of longitudinal continuous endpoints using MMRM\n\n\n\n\nThe ASA Biopharmaceutical Section (BIOP) Software Engineering Working Group SWE WG was established in 2022. Currently they have 3 work streams:\n\nmmrm implements MMRM (mixed models with repeated measures)\nbrms.mmrm, the Bayesian version of MMRM\nHealth Technology Assessment with R\n\nAt a later talk, mmrm was compared with SAS’s PROC MIXED and R’s nlme, glmmTMB for analyses of longitudinal continuous endpoints. In terms of speed and convergence, mmrm is superior than others; while the estimate prodouced by mmrm is very close to PROC MIXED and glmmTMB.\nThis looks like a very interesting tool to try out! Vignette\n\n\nSimulation tools and RWD\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMichael Kammer (Medical University of Vienna). An overview of R software tools to support simulation studies: towards standardizing coding practices.\n\n\n\n\nKammer and colleagues did a review on R packages for simulation, and selected 14 top simulation packages, including simstudy, simdata, synthpop, bigsimr and others. The full list is made available here.\nA real-world dataset, NHANES was also introduced here. The data can be accessed with R package nhanesA.\n\n\n\nCausal Inference\n\n\n\n\n\n\nInformation\n\n\n\n\n\nWorkshop: Implementing the estimand framework in global drug development: Application of causal inference approaches (Mouna Akacha, Björn Bornkamp, Alex Ocampo, Jiawei Wei at Novartis)\nKeynote: Ruth Keogh (LSHTM). Causal inference with observational data: A survival guide\n\n\n\nThese two workshop / talk cover slightly different scenarios: one in RCT, one for observational data. It deserves a whole article or more to elaborate on this topic, so I’m only putting some resources here.\nCausal inference is definitely gaining traction in recent years in both academia and industry. Techniques such as g-computation, IPW and doubly robust estimation are starting to become mainstream. It is fascinating that these techniques themselves are not bound to a fixed model.\nResources:\n\nWorkshop repository Causal-inference-in-RCTs\nBook: Causal Inference: What If by Hernán and Robins (2020)\nPrincipal stratum strategy, Bornkamp et al. (2021)\nTime-dependent covariates, Keogh et al. (2023)\nTarget Trial Emulation (TTE), Hernán and Robins (2016)\n\n\n\nCovariate adjustment and data challenge with RCT data\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nKelly Van Lancker (Ghent University). Improving Power in Randomized Trials by Leveraging Baseline Variables\nDominic Magirr (Novartis). Organizing a Data Challenge on Covariate Adjustment in RCTs\nCraig Wang (Novartis). Participating in a Data Challenge on Covariate Adjustment in RCTs\n\nPanel discussion: Jonathan Bartlett (LSHTM)\n\n\n\n23 teams at Novartis participated in a Data Challenge on Covariate Adjustment. They were given a fixed outcome model, and 5 prior studies trial data, and their task was to create the design matrices that improve the precision compared to unadjusted data.\nIf I were to select talks based on the category titles, I would probably missed the whole session. However, it is surprisingly similar to using not trial, but real-world data (such as EHR) to make predictions. The conclusion were similar as well: using “supercovariates” created by ML isn’t gaining much compared to simple models such as ANCOVA. Possible reasons:\n\nsmall to moderate data size\nlinear relationship between covariates and outcome\ngood enough prognostic variables\n\nIt was also mentioned that the winning team did some trick to reduce the variance among the covariates. Would be interesting to read about it.\nSome resources:\n\nLancker et al. The use of covariate adjustment in randomized controlled trials: an overview link\nCovariate adjustment tutorial, link\n\n\n\nStatistical education\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMaren Vens et al (University of Lübeck). Biostatistics/Biometrics for physicians – essential or unnecessary? How do practicing physicians and dentists evaluate biostatistics? A cross-sectional survey\n\n\n\n\nStatistical education to students / professionals who are not used to working with data has always been tricky. Students generally think statistics is difficult, and need help from a statistician. However there are only limited number of statisticians. The talk by Vens and colleagues confirms what practicing statisticians know, but can’t do much about: most (87%) physicians and dentists in the survey need a statistician to help with their work.\nHow to improve the statistical competency is an important and relevant topic for discussion, and might require systematic changes in how it is taught. Use of modern technology can help, yet it’s only helpful when students start to not fear, or not find math and technology boring.\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {Personal {Highlights:} {CEN2023}},\n  date = {2023-09-06},\n  url = {https://kundan-kumarr.github.io/blog/talks/blog_20230904_cen2023/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “Personal Highlights: CEN2023.” September\n6, 2023. https://kundan-kumarr.github.io/blog/talks/blog_20230904_cen2023/."
  },
  {
    "objectID": "blog/talks/blog_20230301_ds_clinreport/index.html",
    "href": "blog/talks/blog_20230301_ds_clinreport/index.html",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera (course link). It is not necessary to have a paid coursera membership to view the course, everyone could access it.\nIt is a 4 part course released one month ago (Jan/Feb 2023), and it seems that a follow-up will be released in the future.\nOverall I think it strikes a good balance between high-level introduction of the good practices, and examples with how they are implemented. Even though the course focuses on clinical reporting in the pharmaceutical industry, the practices are highly relevant in other sectors as well (e.g. public health, academia, other industries that use open-source software).\nSpecific statistical methods, packages are introduced only at a high-level; which means the course is not for learning how to use this or that packages; but good practice guidelines.\nIn my opinion,\n\nit would be useful if the learner has some experience with software development and/or statistics; otherwise learners might not know how to practice them.\nmost of the examples are related to R packages (understandable), so some experience with R package (use or develop) is useful.\nit could be a very good study material for university students in related subjects.\n\n\n\n\nModule 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software\n\n\n\n\nI have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog/talks/blog_20230301_ds_clinreport/index.html#each-module",
    "href": "blog/talks/blog_20230301_ds_clinreport/index.html#each-module",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "Module 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software"
  },
  {
    "objectID": "blog/talks/blog_20230301_ds_clinreport/index.html#highlight",
    "href": "blog/talks/blog_20230301_ds_clinreport/index.html#highlight",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "I have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog/talks/blog_20240923_quartofriends/index.html",
    "href": "blog/talks/blog_20240923_quartofriends/index.html",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "",
    "text": "I wrote a blog back in early 2023 when I first switched from blogdown to Quarto on my initial impression (read here), and this is a two-year follow-up on my journey since I started using Quarto, for my personal website, teaching, scientific works and collaborative community projects."
  },
  {
    "objectID": "blog/talks/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "href": "blog/talks/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a teaching tool",
    "text": "Quarto as a teaching tool\n\nFrom personal to workshop website\nI switched from blogdown to Quarto in late 2022, right after my PhD. It was initially a cure for a severe burnout from a combination of work-related stressors, when I desperately needed something other than research. My mental state was like the famous painting by Norwegian artist Edvard Munch:\n\n\n\n\n\nThe experience of the switch was explained in the previously mentioned blog. Briefly, it was light like a feather. Since I was quite satisfied, I thought, why don’t I make a workshop website? So I did.\nThe result was quite good, I made the (as far as I knew) first quarto workshop website at University of Oslo for the Oslo Bioinformatics Workshop Week 2022. Feedback from students were positive, and the instructor team thought it hosts the material in a more organized way.\n\n\nSingle day workshop -&gt; two week course\nI was greatly encouraged by the experience, so when I got a 50% position at University of Oslo as biostatistics lecturer, I thought, why don’t we have the same thing for the course?\n\n\n\n\n\nOh well, the workload is crushing. There were a few key differences:\n\nR scripts and material were unavailable since the course was originally in STATA. Everything need to be done from scratch, for at least 12 lab sessions;\nThe students generally have little IT skills, which means more effort need to be done to guide them through the ‘get started’ part.\n\n\n\n\n\n\nIt took one month to create the first version of the website. More details about the experience can be read here.\n\n\nAdding WebR to the course\nOne year later, as technology advances, we added new content to some parts of the website. Most notably is the interactivity achieved through WebR. For example, I made this page on randomness and statistical distribution where students can interactively modify code chunks in a web browser."
  },
  {
    "objectID": "blog/talks/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "href": "blog/talks/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a collaboration tool",
    "text": "Quarto as a collaboration tool\nA static (or even interactive) website is not exactly what you call ‘collaborative tool’. However, if you work as a group towards something cool, Quarto might just be the tool you need. Check out the CAMIS project to find out what I mean by this!"
  },
  {
    "objectID": "blog/talks/blog_20240923_quartofriends/index.html#what-else",
    "href": "blog/talks/blog_20240923_quartofriends/index.html#what-else",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "What else?",
    "text": "What else?\nThe associated talk is available on YouTube, check it out!"
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html",
    "href": "blog/talks/blog_20230921_positconf2023/index.html",
    "title": "Personal Highlights: Positconf 2023",
    "section": "",
    "text": "The yearly party of Positconf (formerly Rstudio conf) has come to an end. I joined the virtual experience at home, it is of course not the same as attending in-person, yet the atmosphere in discord was still great!\nIt’s hard to choose which talks to watch since multiple were scheduled at the same time, so one has to prioritize. I definitely will re-visit some of the talks at a later point, so this blog acts as a placeholder for links so that I can find them in the future."
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html#make-interactive-things",
    "href": "blog/talks/blog_20230921_positconf2023/index.html#make-interactive-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make interactive things",
    "text": "Make interactive things\nWebDev is definitely a big thing at this year’s positconf. If I’m learning one thing from the conference, I’d check out webR.\nI still remember when R was mainly for statistical analysis and computing back when I learned it. Now it’s become much more fun! Strictly speaking, webRand quarto are not R per se. However, they’ve become the gateway drugs for R programmers to dabble in WebDev. With web assembly (wasm), now one can execute R code in a browser and even run shiny app.\nUnlock the power of dataViz animation and interactivity in quarto by Deepsha Menghani used a super fun example (F-bomb) to demonstrate how to add interactivity to your barplot (or other plots) with Crosstalk. Check out the talk here. The presentation was as interactive as the quarto slides, good job Deepsha!\nRunning shiny without a server by Joe Cheng (repo): this was a big announcement. I used shiny at work, but for my own projects or smaller teaching projects I tried to stay away from shiny - I was concerned about the fee. This looks like a promising thing to try out once it’s stable, although I’d probably do webR first."
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html#make-pretty-things",
    "href": "blog/talks/blog_20230921_positconf2023/index.html#make-pretty-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make pretty things",
    "text": "Make pretty things\nIt is fascinating to see so many organizations and individual R developers make their own themes for better branding, recognition and storytelling. More and more peple have realized that making beautiful plots is important, and totally possible as well. Work on layout, color, font and sizes!\n\nThemes\nAdding a touch of glitr: Developing a package of themes on top of ggplot by Aaron Chafetz, Karishma Srikanth and colleagues at USAID. repo\n\n\nTables\nMaking tables with gt has been on my to-do list for a while now. It is very inspiring to see so many cool tables that makes you wonder, “is it really JUST a table?” For example, check out this gallery by Posit community.\nThe book Creating beautiful tables in R with gt by Albert Rapp would be a good place to learn how to make nice tables. Actually the reason why I wanted to use gt is that it seems to be the mainsteam in clinical reporting in pharma. I bumped into this blog post some time ago, and this would be my starting point.\n\n\nQuarto\nIf you want to go one step further and start making your quarto project pretty, there are a few things to try out.\nAlbert Rapp in his talk HTML and CSS for R Users stated that quarto is a gateway drug to WebDev. It reminds me of my very first presentation at my local R users community (2019) was about building a website with blogdown, and when I really spent a lot of time to make my markdown documentation colorful with span style - and that was about everything I knew.\nNow I want more. Learning HTML and CSS can make your dataviz, tables, slides and dashboards look not only professional but also special. I’m going to check out the scss variables in quarto which defines the theme, theme_file.scss. Emil Hvitfeldt (Styling and templating quarto documents) showed us how to make really pretty and animated (!) quarto sldies themes, and shared this template with us, quarto-revealjs-earth. I really like how revealjs slides look like, just that the MacOS Keynote (or MS ppt) drag-and-drop seems more flexible to me (?) Guess it’s something I should get used to over time.\nRichard Iannone (Extending quarto) introduced quarto shortcode extensions to add a bunch of fancy-looking icons to quarto files. To create extensions in general: https://quarto.org/docs/extensions/creating. This is for more pro-users since you needs to learn lua."
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html#quarto-updates",
    "href": "blog/talks/blog_20230921_positconf2023/index.html#quarto-updates",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Quarto updates",
    "text": "Quarto updates\nQuarto is definitely one of the most discussed topics in the year 2022-2023 in the R community. For good reasons. I need to catch up the the latest developments annd use-cases:\n\nWhat’s new in quarto? by Charlotte Wickham\nReproducible manuscripts with Quarto by Mine Çetinkaya-Rundel\nParametrized quarto reports improves understanding of soil health by Jadey Ryan\n\nand so many more. I couldn’t follow all the talks and I’m sure there are lots of great examples of how quarto is better than traditional ways of reporting."
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "href": "blog/talks/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "title": "Personal Highlights: Positconf 2023",
    "section": "A few other things to check out",
    "text": "A few other things to check out\nBeyond the web and quarto topics, I think there are some existing and new tools that can be useful for my work. For example,\n\nI should review Hadley and Jenny’s R package book (2e).\nthis package targets for pipeline automation and management look like something that can be used for my analysis\n…\n\nIt will take a while to digest the latest developments. But little by little, we’ll get there! People in the R community are doing great things."
  },
  {
    "objectID": "blog/talks/technotes_20230220_pkgdown/index.html",
    "href": "blog/talks/technotes_20230220_pkgdown/index.html",
    "title": "R package website with pkgdown",
    "section": "",
    "text": "1. Create the website skeleton.\nBefore editing the details, we need to create the skeleton for the website. It can be done with usethis and pkgdown packages.\nIn R, run this:\nusethis::use_pkgdown()\nThis creates the _pkgdown.yml file, which is the place you configure your site.\nTo view the initial package website, use the following command:\npkgdown::build_site()\nThis creates docs/ directory containing a website\n\nREADME.md becomes the homepage,\ndocumentation in man/ generates a function reference,\nvignettes are rendered into articles/.\n\n\n\n2. Edit the vignette documentation\nMake sure that the vignette index is consistent with Title, otherwise it will not render.\n\n\n3. Build and preview your site\nNow check if the site looks good, and contents are correctly positioned.\npkgdown::preview_site()\npkgdown::build_site()\nYou can also do this to build the site.\npkgdown::build_site_github_pages()\n\n\n4. Deploy site with GitHub Pages\nThere seems to be two options:\n\nusethis::use_pkgdown_github_pages(), this function should take care of everything after pushing changes to GH.\nif you used pkgdown::build_site_github_pages() and pushed everything to GitHub, it might not automatically deploy your site to GH pages. I tried to go to Settings -&gt; Pages -&gt; Deploy from a branch -&gt; main -&gt; /docs, this makes Action deploy your site from the docs folder.\n\ndouble check if you have .nojekyll file\nif a website does not show, check whether you have docs in the .gitignore file; since you are deploying from that folder.\n\n\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {R Package Website with `Pkgdown`},\n  date = {2023-02-20},\n  url = {https://kundan-kumarr.github.io/blog/talks/technotes_20230220_pkgdown/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “R Package Website with `Pkgdown`.”\nFebruary 20, 2023. https://kundan-kumarr.github.io/blog/talks/technotes_20230220_pkgdown/."
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "",
    "text": "Useful references:"
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html#considerations",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html#considerations",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Considerations",
    "text": "Considerations\nA few ways to do it: Shiny Server (free), shinyapps.io (free and premium), and professional Rstudio Connect (paid).\nI choose to test out the second option, since it allows more possibilities compared to the free open-source Shiny Server.\nThe free option should allow me to create 5 apps, which is more than enough for personal use. It also allows 25 active hours per month; a note on that at the end."
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html#configuration",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html#configuration",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Configuration",
    "text": "Configuration\nSign up with GitHub account; or something else. It is possible to change account name afterwards.\nIn Rstudio,\n\nfirst install.packages('rsconnect')\nthen, configure the account. It can be done with rsconnect::setAccountInfo() with information provided in your own shinyapps.io page.\n\nBefore the last step, it is necessary to have an app to deploy!"
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Create my first shiny project",
    "text": "Create my first shiny project\nHere I use my usual workflow of creating a new R project:\n\nCreate a new repo on GitHub;\nClone the repo locally, by opening a new R project with version control.\n\nNow copy the two R scripts from the demo example:\n\nserver.R\nui.R\n\nTest locally by running shiny::runApp(). This should render the app."
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Deploy to shinyapps.io",
    "text": "Deploy to shinyapps.io\nrsconnect::deployApp() will deploy the app, with an automatically generated url that links to your account.\nThe demo app is deployed here.\n\nNote on active hours\nAfter deployment, the site seems to be active until you shut it down manually; or timeout. The default timeout is 15 minutes, which can be reduced to 5 minutes.\n25 hours per month suggests that I can open the site for 300 times (without manually shuting it down). It might be necessary to start using the paid options, if I have more than one site, or multiple users want to access it …"
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html",
    "href": "blog/talks/technotes_20250703_research_guide/index.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers.\nGitHub repository: Research Scientist Preparation Guide\nData Science Notes: Data Science Intro\nStatistical Learning Notes: Statistical Analysis"
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#overview",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "Overview",
    "text": "Overview\nThis guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers.\nGitHub repository: Research Scientist Preparation Guide\nData Science Notes: Data Science Intro\nStatistical Learning Notes: Statistical Analysis"
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#key-interview-components",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1. Research Portfolio Deep Dive\n\nBe able to explain your core research contributions clearly and concisely.\nClearly define the problem, highlight novelty, describe methods, show results, and explain real-world relevance.\nPractice different versions of your explanation (5-min, 15-min, 30-min).\nShow awareness of how your work connects to broader trends and open challenges.\n\n\n\n2. Technical Machine Learning Knowledge\n\nReinforcement Learning: Policy gradients, actor-critic, safe RL, off-policy and on-policy methods.\nDeep Learning: Architecture design, optimization, transformers, generalization.\nStatistical Learning: Model selection, regularization (L1/L2), kernel methods, bias-variance tradeoff. (See Notes)\nProbabilistic Modeling: Bayesian inference, uncertainty quantification, latent variable models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: Scaling laws, fine-tuning, instruction tuning, prompting, RAG architectures.\nComputer Vision: Object detection, segmentation, multimodal learning.\nData Science Foundations: Data pipelines, EDA, data cleaning, transformation. (See Notes)\n\n\n\n3. System Design / Applied ML Problems\n\nUnderstand how to build ML systems end-to-end:\n\nFeature engineering and data preprocessing\nModel training, hyperparameter tuning, and validation\nDeployment (latency, scalability, monitoring)\nAddressing data drift, noisy labels, and real-world constraints\n\n\n\n\n4. Coding and Algorithmic Skills\n\nModerate-level data structures and algorithms:\n\nArrays, strings, trees, graphs, dynamic programming\n\nML coding:\n\nData manipulation using pandas/numpy\nPrototyping in PyTorch, TensorFlow, or JAX\nBasic SQL queries and joins\n\n\n\n\n5. Behavioral and Collaboration Skills\n\nPractice behavioral questions:\n\n“Tell me about a time you resolved a conflict”\n“How do you approach ambiguous research questions?”\n\nDemonstrate collaboration across disciplines and teams\nEmphasize your ability to communicate technical ideas to non-experts"
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#recommended-preparation-resources",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nKey Papers: NeurIPS, ICLR, ICML, CVPR, ACL\nBooks:\n\nDesigning Machine Learning Systems – Chip Huyen\nDeep Learning – Ian Goodfellow\nThe Elements of Statistical Learning – Hastie, Tibshirani, Friedman\nBayesian Reasoning and Machine Learning – David Barber\n\nCoding Practice: Leetcode (focus on Medium problems)\nSystem Design Practice: ML interview prep platforms, YouTube walkthroughs\nMock Interviews: Conduct with peers, mentors, or online platforms"
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#example-interview-questions",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nWhat are the main contributions of your most recent paper?\nHow does your approach compare to existing methods?\nHow would you adapt your method if data was scarce or noisy?\nWhat assumptions underlie your models, and how would you validate them?\nHow do you handle uncertainty and interpretability?\nHow would you apply your method to a new domain or product?"
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#my-personal-advice",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "💡 My Personal Advice",
    "text": "💡 My Personal Advice\n\nClarity &gt; Complexity: Simplify without oversimplifying.\nBe excited about your work — your passion makes you memorable.\nThink like a collaborator, not just a scientist.\nHighlight real-world impact and where you can contribute.\nShow your ability to iterate, learn, and adapt."
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#mentorship",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’re preparing for Research Scientist interviews and would like guidance or mentorship, feel free to reach out at cs.kundann@gmail.com. I’m happy to connect with motivated researchers and offer support."
  },
  {
    "objectID": "blog/talks/blog_20230112_roche_opensource/index.html",
    "href": "blog/talks/blog_20230112_roche_opensource/index.html",
    "title": "Open source reporting with R: clinical, public health, RSE and embrace the change",
    "section": "",
    "text": "Two days ago (Jan 11 2023) I watched a presentation by data scientists at Roche about why they are making their clinical trials in 2023 open source with R. As someone who uses R for most of the time and has done similar works (not in pharma, but in public health surveillance and reporting: watch my talk, slides to find out what we do), I watched the presentation with great interest. Here are my notes, combined with some thoughts on open-source in the industry, public sector and academia.\n\nThree reasons for why I am writing this blog\n\nNote down some of the technology which points towards the future of the field\nRelate to my experience of open-source applied in public health, specifically public health reporting\nShare some thoughts in statistical education of applied students/researchers (e.g. medicine), and training Research Software Engineers\n\n\n\nMy experience with statistical software\nTo put my opinions in perspective,\n\nI do not have experience with SAS or pharma, so I do not have first-hand knowledge on the functionality, ease-of-use or the popularity of commercial softwares in the industry.\nI did my MSc and PhD in statistics/biostatistics/medical informatics and R had always been a default choice.\nI worked in public health for a few years, where Excel is possibly the most common tool, and STATA and R are scarcely used (statisticians, epidemiologists, bioinformaticians).\nIn the past few years, my university has made the switch from SPSS to STATA for intro statistics for medical students (while students at higher level, or doing advanced analyses might use R/python), and a test-run with R might be in motion.\n\n\n\n\nClinical Reporting\nIn drug development at pharmaceutical companies (and/or research institutes and hospitals), these data related tasks are very common:\n\nsummarise safety and efficacy data\nprovide accurate picture of trial outcomes\nmanage data collection across different sites\n\nCompleting these tasks in a correct, efficient and reproducible manner is crucial for patient safety. However, these tasks are also highly resource intensive: highly trained scientist, statisticians and technincians must be involved in the process. Historically, pharma use commercial software such as SAS.\n\nRegulation and exploration needs\nThere are requirements for clinical reporting: both regulartory and exploratory. From the regulatory side, there exist industry standards (CDISC) in the clinical research process, such as SDTM (Model for Tabulation of Study Data) and ADaM (Analysis Data Model). Statistical analyses, tables, listings and graphs (TLGs) also fall into this cateogory.\nFrom the exploratory side, clinical data are highly context dependent, and new formats of data such as imaging are more and more used in prediction modeling and drug development.\nIn addition, it is not hard to imagine that the technical competency of employees differ, especially in large organizations. Enabling people with less experience to analyse trial data in a reproducible manner is helpful for not only the learning and growth of employees, but also the productivity of organizations.\nThe existing commercial tools are not able to adapt to the rapid changes in the field.\n\n\nTransition into Open-Source\nIn this talk, Dr Kieran Martin at Roche introduced that they started using R as their core data science tool, aiming to move their codebase to having a core R. In the future, they plan to have something that is lanugage agnostic: meaning that python, Stan, C++, Julia and beyond can be used for different tasks.\nI only noted down a few of the things they mentioned on the infrastructure side:\n\nOCEAN - a lanugage agnostic computing platform on AWS (docker)\nGit, Gitlab for version control and collaboration\nRstudio connect server\nSnakemake for orchestrate production\n\n\n\nR and Shiny\nThere are obvious benefits of using R. It is convenient to install and use (if you used python and R, you’d probably agree), and the latest development in Shiny made it very easy to develop interactive visualizations, suitable for exploration. Package development is critical for reproducibility and distributing works - which R does it very well. A few packages developed by pharma are Teal and admiral: the ADaM in R, which I intend to check out at one point.\nR has deep roots in academia which means the newest statistical methods are well covered; which also affects the skill sets that talents own - fresh graduates probably already learned it at university. R being open source means that collaboration with external partners is much more efficient, and transparent. Strong community support is another positive thing that encourages beginners to enter the field and learn.\n\n\n\n\nOpen Sourcing Public Health\n\nSurveillance and reporting\nOne key functionality of public health (PH) authorities is stay informed and inform. They collect data from labs, hospitals and clinics across the country, summarize into useful statistics in tables and graphs, make reports, then inform the policy makers to make decisions (such as vaccination campaigns).\nCompared to clinical reporting (in my understanding), there are many similarities - we make TLG (tables, listings and graphs). There are also features that make reporting in public health unique:\n\nPH surveillance and reporting are dynamic and real-time, which can change in a matter of days. That is because the situation of different infectious diseases can evolve rapidly, so PH authorities need to make appropriate adjustments.\nTime and location (spatial-temporal) are important. Different time granularity (daily, weekly) and geographical units (nation, county, municipality, city districts) are typically required for reporting.\n\n\n\nScale up and automate with open source tools\nTraditionally, these reports are made manually - one location, one graph per time on a certain disease. When a global pandemic hits, this is definitely not fast enough. At my team (Sykdomspulsen team at the Norwegian Institute of Public Health), we tried a different approach. Details of what we did can be found in this talk(slides), but to make it brief:\n\nWe developed a fully automated pipeline that connects 15 registries (vaccination, lab, hospital and intensive care and many more). The data is gathered, censored, cleaned and pre-processed for down-stream analysis\nStatistical analysis, tables, graphs and maps are made for all locations in Norway for various outcomes of interest, such as Covid, influenza, respiratory and gastrointestinal infections\nOver 1000 customized reports with over 30 graphs and tables are produced daily and sent to local PH officials, where we also had a shiny website (Kommunehelsetjenester for Kommunelege) for over 300 PH officials to get most up-to-date information about their own municipality\n\nBy automation, every year Sykdomspulsen can save 700 000 NOK (roughly 70k USD) while making 400 times more real-time reports for public health. Even better, with reproducibility and quality control.\n\n\nToolbox\nSykdomspulsen is a small team (8 people, 3 are statisticians and 1 engineer), and our infrastructure was built upon R packages, which we call splverse. Our infrastructure is not fundamentally different from the one Roche introduced, basically:\n\nR does the task planning and project organization. On top of this, the data cleaning, statistical analysis are implemented. Graphs, tables and maps are made with appropriate R packages\nRmarkdown does automated reporting into .docx and .xlsx. Some reports are also in .html tables to be embedded into customized emails\nRstudio Workbench and GitHub help with teamwork\n\nDocker, GoCD and Airflow do the CI/CD and orchestration\n\n\n\n\nEmbrace the transition\n\nCulture change needed\nUnfortunately, not all organizations are eager to abandon the old way. Even at our own institute where researchers are the majority, open source and modern day programming is hardly practiced (by my observation). Even worse, under the budget cuts in 2023-24, a large number of younger employees who have the technical skills have left - which left the public health surveillance even more vulnerable now that Covid is far from over.\nIn my opinion, public health needs open-source and good programming even more than pharmaceutical companies. Both save lifes - and PH has less money to invest in softwares, infrastructures and talents. In this situation, resources should be spent in fields that are critical and most cost-effective; yet in reality this is often not the case.\nThe slow culture change at big organizations can happen, but only if there is a sufficient amount of employees who are willing to embrace the new technology. In the talk by Roche they about about their training strategy. It is not possible to train all users, and not everyone has the same needs at the same time. Therefore, self study with certain study paths is encouraged and supported.\n\n\nTeach programming to students in various fields\nBased on my experience in the UK and Norway, students (myself included) learn R programming in one of the two ways\n\nLearning by Googling (self-taught): a university degree needs to use it: provides a short introduction, then students learn by using. This is how I learned R at my MSc Statistics degree, and this is probably the most common way\nWorkshops at university: organizations such as the Carpentries provide course material and teaching a few times per year, where interested students (usually from subjects such as biology) come and learn. These classes are quite popular, and usually have a long waiting list.\n\nFrom learning by googling to some organized teaching - that is already some good progress. However, if not, can we improve?\nIn my experience with statistical advising with the university hospital, clinical researchers and medical students are enthusastic to get their statistics done, some are also eager to do some analysis themselves. That is good. Yet, there is generally lack of capacity - either knowledge or software skills. Once the statistician who helps with the project stops, the project ends. There is the need to have in-house statistical capacity. To this end, open-source softwares such as R, and good programming practice (reproducibility for example) can help a lot: the license doesn’t end, and everything is documented so that the next person can continue the work.\nI’m glad that my university has made some transitional efforts in this regard: STATA instead of SPSS is being taught to medical students as part of their statistics course. There might be a test-run in R soon, which is very exciting (since I’ll be involved in the teaching)!\n\n\nStatistical engineering and RSEs\nThat was the capacity building to get beginners more independent. On the other side, there is also the need for better programming practice for researchers at more advanced level. Research Software Engineering (RSE) is starting to get more and more attention, because it is not only relevant for research (i.e. getting papers published), but in broader applications.\nFor example, in the talk by Roche, they mentioned that “RSE teams need to accelerate adoption of new statistical methods and biomarker data analysis”, and the implementation with R packages and templates is at its core. In the future more languages would be included such as Python, Stan, C++ and Julia.\nHowever, RSE as a job title or career path is still a new thing. I know two RSEs at my university, and RSE is definitely not your typical academic faculty position: only departments that think it’s important makes positions, often not permanent. To get any new methods actually used in either industry or the public sector outside research, translating methods into tools is must-do. In the future I hope RSE becomes a stable and common career path, and more exciting things can happen.\n\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {Open Source Reporting with {R:} Clinical, Public Health,\n    {RSE} and Embrace the Change},\n  date = {2023-01-13},\n  url = {https://kundan-kumarr.github.io/blog/talks/blog_20230112_roche_opensource/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “Open Source Reporting with R: Clinical, Public\nHealth, RSE and Embrace the Change.” January 13, 2023. https://kundan-kumarr.github.io/blog/talks/blog_20230112_roche_opensource/."
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html",
    "title": "R package workflow",
    "section": "",
    "text": "This checklist is being updated over time. Mostly for my own use; but great if it helps you as well!\nFor a complete treatment, please refer to R Packages (2e) by Hadley Wickham and Jennifer Bryan."
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "title": "R package workflow",
    "section": "Initialize the project",
    "text": "Initialize the project\n\nusethis::create_package('path_to_pkg/pkgname') \n\nIt opens a new R project (directory) named pkgname, with the following items:\n\nDESCRIPTION\nNAMESPACE\ndirectory R/\n.Rbuildignore and .gitignore\nand the project icon, pkgname.Rproj.\n\nIf you have an existing R project but wish to build a package there, copy everything but pkgname.Rproj, and modify the files in your existing pkg directory. Pay extra attention to the hidden files like .Rbuildignore.\n\nusethis::use_mit_license() # modify name to yours\nusethis::use_readme_md() # if you do not have this already\nusethis::use_news_md()\nusethis::use_test()\n\n# create a folder for future data documentation\nx &lt;- 1 \nusethis::use_data() \n\nIn addition, URL and bug reports should be added in the DESCRIPTION."
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html#planning",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html#planning",
    "title": "R package workflow",
    "section": "Planning",
    "text": "Planning\nIt is good practice to start with planning the package, rather than directly start coding.\nCreate a folder called dev. To prevent it from being built, add the following line in .Rbuildignore"
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "title": "R package workflow",
    "section": "Write, test and document",
    "text": "Write, test and document\nCreate exported functions in R/, development code in script/ (or somewhere else, such as dev/).\n\nData: raw and processed\nNeed to be clear in mind where the data files go. There are a few data related folders:\n\nraw data files, in the format of excel sheets or csv. Usually placed as inst/data_name.csv\nR scripts to process the raw data so that we create data object inside the package, put inside data-raw\ndata objects that can be called as pkg::data_name, are placed in data. These files are usually directly generated by executing write.rda().\ndata documentation, usually placed in R/data_documentation.R. These are Roxygen2 documents for the data.\n\n\n\nDocumentation\nYou need to configure the Build tools.\nThese three things should be done:\n\nFunction documentation\nCreate a function f1, and put your cursor on it. Go to Code -&gt; Insert Roxygen Skeleton to create the template.\nAlternatively, use #' to start.\n\n#' A simple placehold function \n#'\n#' @param x a numeric value\n#'\n#' @return a value 3 greater than the input\n#' @export\n#'\n#' @examples \n#' f1(5)\nf1 &lt;- function(x){\n  x+3\n}\n\n\n\nData documentation\nIt can be beneficial to create a separate file to document data only, say data_documentation.R under the R/ directory.\n\n#' Placeholder data x\n#'\n#' This dataset contains one value, x\n#'\n#' @format\n#' \\describe{\n#' \\item{x}{The placeholder data x}\n#' }\n#' @examples\n#' print(x)\n\"x\"\n\n\n\nVignette documentation\n\nusethis::use_vignette('your_vignette')\n\n\n\nDeploy to pkgdown\nCheck this reference here"
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "title": "R package workflow",
    "section": "Build package and check",
    "text": "Build package and check\nIt is possible that your checks don’t pass on the first try.\n\nWhat to ignore when build?\n^.*\\.Rproj$\n^\\.Rproj\\.user$\n^dev$\n^_pkgdown\\.yml$\n^license\\.md$\nMakefile\ndata-raw\ncran-comments.md\n^\\.github$"
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index.html",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index.html",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "Time to try Quarto",
    "text": "Time to try Quarto\nNow that I’ve finally completed the more pressing tasks in October 2022, I can catch up to the cool kids on twitter: create a website with Quarto!\nThere were quite a lot of discussions about Quarto in the summer 2022. I wasn’t following the discussions closely, but I remember there were quite a few talks in the Rstudio conference this year. Then more and more people switched to Quarto on Twitter. Then people I know also switched to Quarto. What’s the fuzz about?\nMy experience with Quarto is focused on websites. I have not tried other forms of publishing. So far I have created:\n\na workshop website for my colleagues\na personal website (the one you are reading right now)\nan R package (qtwAcademic)that wraps three Quarto website templates for beginners\n\nHere are a few things I like about Quarto. Given that I’m not very experienced in front-end development, these comments are going to be about ease-of-use and design, rather than the technicalities.\n\nClean look for both personal and workshop/courses\nWhen I was using “academic” template in blogdown, I liked the structure of the site: projects, talks, blog, softwares and publications sections are clearly displayed at the top. What I didn’t like is that the default homepage was a very long single page; yet its customisation wasn’t the easist. Other templates were either too simple (for blog only), or more suitable for image display (photography projects). I wanted a website that keep the good structure of “academic”, which is quite suitable for academics (hence the name); while keeping each section independent.\nWith distill I could achieve the structure I wanted; but I didn’t enjoy it too much as a personal website (at least it wasn’t as flexible as Quarto). distill is still pretty decent for organisations or documentation site.\nWith Quarto, I can achieve the desired looks for not only a personal website (with or without blogs), but also a workshop, event or even course website. This is fantastic! The top, sidebar or hybrid navigation makes the site structure very clear, especially when there are lots of content. As an aspiring lecturer at university, this is really One Quarto Rules Them All.\n\n\nFlexible yet not overwhelming\nAs I mentioned above, hacking “academic” in blogdown was not that easy - simply because there were too many folders that you are not actually supposed to modify. It was confusing to know what to change in order to achieve the desired output, and multiple folders were having the same names, making it very challenging for beginners. Ironically, this is usually the first template beginners start with!\nThat’s why I immediately fell for Quarto: you only need 4 components to make a decent minimalistic website work:\n\n_quarto.yml to control the overall layout\nindex.qmd at the root folder to control the homepage\nabout.qmd for some basic information about the creator or the website\nproject.qmd for projects or any other content that the creator wants to display\n\nThe way that _quarto.yml clearly specifies the .qmd files really helps beginners to understand where things are. This has been extremely useful for me when I wanted to learn how people made their website by reading the source code - I could understand exactly where to find the information I needed. The clear structure greatly helps the creators themselves, and also those who want to learn.\n\n\nGreat community\nRstats people have a great community. I wouldn’t be able to make my site the way I wanted if people haven’t been sharing their works. I have learned a lot by reading the source code by Dr Emi Tanaka, Dr David Schoch, Bea Milz, Prof Mine Cetinnkaya-Rundel’s STA 210 - Regression Analysis course.\nI also made my own R package that wraps three templates to create Quarto websites that are frequently used by academics, qtwAcademic. In the following days I plan to write up more detailed explanations on how to use the package, along with some new features."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_preventable_sridhar/index.html",
    "href": "blog/talks/readnotes_2023010x_preventable_sridhar/index.html",
    "title": "Preventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar",
    "section": "",
    "text": "Advice on some measures to prepare for the next pandemic\n(From Five ways to prepare for the next pandemic by Prof. Devi Sridhar)\n\nMonitor zoonoses. Identify patogens with pandemic potential, regulate better wet markets\nSequence globally. Investment in genetic-sequencing capability\nStrengthen manufacturing. Vaccine inequality, fragility of vaccine production. Private and public sector work together - vaccine research, production and distribution.\nVaccine preparedness. For known diseases (e.g. influenza), invest in vaccines that protect against a wide range of variants. New technology and research for unknown threats\nStop the spread (long enough for the vaccines) to save lives.\n\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {Preventable: {How} a {Pandemic} {Changed} the {World} \\&\n    {How} to {Stop} the {Next} {One} - {Devi} {Sridhar}},\n  date = {2023-03-17},\n  url = {https://kundan-kumarr.github.io/blog/talks/readnotes_2023010x_preventable_sridhar/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “Preventable: How a Pandemic Changed the World\n& How to Stop the Next One - Devi Sridhar.” March 17, 2023.\nhttps://kundan-kumarr.github.io/blog/talks/readnotes_2023010x_preventable_sridhar/."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing\n\n\n\n\nThere should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients.\n\n\n\n\n\nPassive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy.\n\n\n\n\nThe effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor\n\n\n\n\nInfodemic\n(these two chapters are highly technical, and they deserve a separate note)\n\n\n\nDisaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting\n\n\n\n\nThe impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal.\n\n\n\n\n\nInvest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "There should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Passive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "The effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Infodemic\n(these two chapters are highly technical, and they deserve a separate note)"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Disaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "The impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Invest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html",
    "href": "blog/talks/technotes_20230111_deployqt/index.html",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "href": "blog/talks/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html#create-quarto-project",
    "href": "blog/talks/technotes_20230111_deployqt/index.html#create-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "2. Create Quarto project",
    "text": "2. Create Quarto project\nThis can be a website, a book (a specific type of website) or something else.\nTest compilation by quarto render, or click the Render button."
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "href": "blog/talks/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "3. Configure Quarto project",
    "text": "3. Configure Quarto project\nIn _quarto.yml, change the project configuration to use docs as the output-dir:\nproject:\n  type: website\n  output-dir: docs\n\n\n\n\n\nThen add .nojekyll to the root of the repository. Can do this by (in terminal)\ntouch .nojekyll\nPush everything to your repository."
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html#configure-github-pages",
    "href": "blog/talks/technotes_20230111_deployqt/index.html#configure-github-pages",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "4. Configure GitHub Pages",
    "text": "4. Configure GitHub Pages\nGo to Settings &gt; Pages, publish from docs of the main branch.\n\n\n\n\n\nCan check GitHub Action and deployment status.\n\n\n\n\n\n\n\n\n\n\nAfter the deployment is successful, go to view deployment, and a successful website should be published."
  },
  {
    "objectID": "blog/talks/readnotes_20240606_bad_pharma/index.html",
    "href": "blog/talks/readnotes_20240606_bad_pharma/index.html",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog/talks/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "href": "blog/talks/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog/talks/technotes_20230228_clinreport_part3/index.html",
    "href": "blog/talks/technotes_20230228_clinreport_part3/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/talks/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "href": "blog/talks/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Principles and tools",
    "text": "Principles and tools\nReproducibility: Git (code versioning), dependencies (renv for r package dependencies, Docker for system dependencies)\n\nClean code\nCode comments: not recommended! Better to write code in a way that does not need additional comments.\nDRY: don’t repeat yourself (principle of software development), avoid copy and paste everywhere.\nSRP: single-responsibility prinicple, a function should do one thing: either plot a chart, saves a file, changes variables etc, but not all.\nNaming conventions\n\nReserve dots (.) for S3 methods (print.patient)\nReserve CamelCase for R6 classes or package names (OurPatients)\nUse snake cases (all_patients) for function names and arguments, use verb noun pattern (plot_this())\n\n\n\nCode smells\nA function might be too large: break into smaller ones (e.g. could fit in one screen)\nA function violates SRP: break into smaller ones, and be explicit in what result it is expected to return\nA function with multiple arguments: the scenarios to be tested increase rapidly. Recommended to minimize number of critical function arguments, and break the function into smaller ones.\nBad comments in the code: drop the unnecessary, unclear, outdated comments, write code that are self-explanatory.\n\n\nDevelopment workflow\nCode refactoring: change existing code without its functionality\nTDD: Test-Driven Development\n\nstart with writing a new (failing) test\nwrite code thtat passes the nenw tetst\nrefactor the code\nand repeat\n\nBenefits: your code is covered by tests; you think of testing scenarios first; “fail fast” - can immediately repair the code; more freedom to refactor (improve) the code.\nHow to test\n\nautomatically: CI/CD, after pushing Git commits\nmanually:\n\nrun all unit tests in the package (Build / Test package)\nrun tests in a selected test file (Run Tests)\nrun a single test in Rstudio console\n\n\nHow to check\n\nR CMD CHECK"
  },
  {
    "objectID": "blog/talks/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "href": "blog/talks/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Writing robust statistical software",
    "text": "Writing robust statistical software\nImplement complext statistical methods such that the software is reliable, and includes appropriate testing to ensure high quality and validity and ultimately credibility of statistical analysis results.\n\nchoose the right method and understand them\nsolve the core implementation problem with prototype code\n\nNeed to try a few different solutions, compare and select the best one. Might also need to involve domain experts.\n\nspend enough time on planning the design of the R package\n\nDon’t write the package right away; instead define the scope, discuss with users, and design the package.\nStart to draw a flow diagram, align names, arguments and classes; write prototype code.\n\nassume the package will evolve over time\n\nPackages you depend on will change; users will require new features\nWrite tests\n\nunit tests\nintegration tests\n\nMake the package extensible\n\nconsider object oriented package designs\ncombine functions in pipelines\n\nKeep it manageable\n\navoid too many arguments\navoid too large functions"
  },
  {
    "objectID": "blog/talks/technotes_20230228_clinreport_part3/index.html#key-components",
    "href": "blog/talks/technotes_20230228_clinreport_part3/index.html#key-components",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Key components",
    "text": "Key components\n\nDependency management\nInstall dependencies (system/OS level; R packages)\n\nSet repos (can be specified in options()) to e.g. CRAN, BioConductor\nrenv\ncontainer with dependencies pre-installed\n\n\n\nStatic code analysis\n\nLinting (for programmatic and syntax errors) via lintr package\nCode style enforcement via styler package\nSpell checks identifies misspelled words in vignettes, docs and R code via spelling package\n\n\n\nTesting\n\nR CMD build builds R packages as a installable artifact\nR CMD check runs 20+ checks including unit tests, reports errors, warnigns and notes\nTest coverage reports with covr, checks how many lines of code are covered with tests\nR CMD INSTALL tests R package installation\n\n\n\nDocumentation\nAuto-generated docs via Roxygen and pkgdown\n\n\nRelease and deployments\nRelease artifacts and deployments to target systems\n\nChangelog (features, bug fixes) in the NEWS.md\nRelease: create the package with R CMD build. Validation report with thevalidatoR\nPublishing: CRAN, BioConductor"
  },
  {
    "objectID": "blog/talks/technotes_20240506_ohdsi_part1/index.html",
    "href": "blog/talks/technotes_20240506_ohdsi_part1/index.html",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "",
    "text": "The Book of OHDSI written by the OHDSI community.\nWhat is required to go from origin (source data) to destination (evidence):\nOMOP: Observational Medical Outcomes Partnership, aims to identify true drug safety association.\nOMOP CDM: common data model, a mechanism to standardize the structure, content and semantics to make it possible to write statistical code that can be reused at every data site.\nOHDSI community (2014) has created libraries of open-source analytics tools atop OMOP CDM to support:"
  },
  {
    "objectID": "blog/talks/technotes_20240506_ohdsi_part1/index.html#characterization",
    "href": "blog/talks/technotes_20240506_ohdsi_part1/index.html#characterization",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Characterization",
    "text": "Characterization\n\nWhat happened to the patients.\n\nChapter 11 Characterization\nTypical characterization questions:\n\nHow many patients…?\nHow often does…? What proportion of patients …?\nWhat is the distribution of values for …?\nWhat is the median length of exposure for patients on …?\nOther drugs the patient is using?\n\nDesired output:\n\ncount, percentage\naverages and other descriptive statistics\nprevalence, incidence rate\nrule-based phenotype\ndrug utilization, adherence, treatment pathways, line of therapy\ndisease natural history, co-morbidity profile"
  },
  {
    "objectID": "blog/talks/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "href": "blog/talks/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Population-level estimation",
    "text": "Population-level estimation\n\nWhat are the causal effects\n\nChapter 12 Population-level Estimation\nTypical questions:\n\nWhat is the effect of …?\nWhich treatment works better?\nWhat is the risk of X on Y?\nWhat is the time-to-event of …?\n\nDesired output:\n\nRR, HR, OR\nAssociation, correlation\nATE, causal effect"
  },
  {
    "objectID": "blog/talks/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "href": "blog/talks/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Patient-level prediction",
    "text": "Patient-level prediction\n\nWhat will happen to A?\n\nChapter 13 Patient-level Prediction\nTypical questions:\n\nWhat is the chance that this patient will…?\nWho are the candidate for…?\n\nDesired output:\n\nprobability for an individual\nprediction model\nhigh/low risk groups\nprobabilistic phenotype"
  },
  {
    "objectID": "blog/talks/technotes_20231001_qt_webr/index.html",
    "href": "blog/talks/technotes_20231001_qt_webr/index.html",
    "title": "Use WebR in your existing quarto website",
    "section": "",
    "text": "WebR is the new hot topic in the R community. Coupled with Quarto, you can run R code interactively in a web browser. This is achieved with the great quarto extension, quarto-webr developed by James J Balamuta.\nIn the positconf 2023 talk, documentation and YouTube, James introduced how to make a webR empowered quarto document. It is simple enough, and you can make it work quite smoothly."
  },
  {
    "objectID": "blog/talks/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "href": "blog/talks/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "title": "Use WebR in your existing quarto website",
    "section": "When your render gets stuck",
    "text": "When your render gets stuck\nBut there is a twist. This works perfectly fine with a new quarto project, where no output-dir is specified yet. When I tried to replicate the same thing for my existing quarto website (with output-dir: docs so that I could deploy it with GitHub Pages), my rendered html file got stuck:\n\nIf you read the troubleshooting documentation, you’ll see that it’s a problem with the two js files. This agrees with what Rstudio Background Jobs tells us.\n\nI moved the two files (manually..) around, then render again, nothing changed.\n\nSolution: set channel-type option\nThis is a solution provided by the authors, although I don’t quite understand what it did, but it did the magic. (Thanks to Linh’s help!)\nThis is where you specify this option.\n\nRender again, now it works! WebR status turns green, and I can run code interactively in the browser."
  },
  {
    "objectID": "blog/talks/blog_20230104_qtwAcademic/index.html",
    "href": "blog/talks/blog_20230104_qtwAcademic/index.html",
    "title": "qtwAcademic: a quick and easy way to start your Quarto website",
    "section": "",
    "text": "qtwAcademic stands for Quarto Websites for Academics, which provides a few Quarto templates for Quarto website that are commonly used by academics.\nThe templates are designed to make it quick and easy for users with little or no Quarto experience to create a website for their personal portfolio or courses. Each template is fully customizable once the user is more familiar with Quarto.\nRead more about the package here.\nMore details about the package is being written … \n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {qtwAcademic: A Quick and Easy Way to Start Your {Quarto}\n    Website},\n  date = {2023-01-05},\n  url = {https://kundan-kumarr.github.io/blog/talks/blog_20230104_qtwAcademic/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “qtwAcademic: A Quick and Easy Way to Start Your\nQuarto Website.” January 5, 2023. https://kundan-kumarr.github.io/blog/talks/blog_20230104_qtwAcademic/."
  },
  {
    "objectID": "blog/talks/technotes_20231018_qt_styling/index.html",
    "href": "blog/talks/technotes_20231018_qt_styling/index.html",
    "title": "Styling your quarto project",
    "section": "",
    "text": "Useful references:\n\nTalk by Emil Hvitfeldt on Styling and Templating Quarto Documents\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {Styling Your Quarto Project},\n  date = {2023-10-18},\n  url = {https://kundan-kumarr.github.io/blog/talks/technotes_20231018_qt_styling/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “Styling Your Quarto Project.” October\n18, 2023. https://kundan-kumarr.github.io/blog/talks/technotes_20231018_qt_styling/."
  },
  {
    "objectID": "blog/talks/technotes_20230222_clinreport_part2/index.html",
    "href": "blog/talks/technotes_20230222_clinreport_part2/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/talks/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "href": "blog/talks/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Agile mindset and DevOps practices",
    "text": "Agile mindset and DevOps practices\n\nData science as a new way of thinking\nNew way of working means\n\nleverage standards and automation (CI/CD)\nadopt new data types quickly, reusing data for multiple purposes, pooling data, data marts\nopen-sourcing and collaborating cross pharma (small, readable, self-tested code)\ncoding for reusability, moving away from single-use programs\nrapidly re-arranginng re-usable components to meet analytical need at hand\n\nData scientist need to have hard skills, such as\n\nSAS, R, Python, JS, bash\ncloud, containers\nCI/CD tools\nvisualisation\nknowledge of various data types\n\nand also soft skills:\n\ncollaborative and inclusive\ntransparent and practical\ncreative and proactive\nasking the right questions\nable to wear many hats, be more flexible and resilient\n\n\n\nAgile\nProject management; a mindset: uncover better ways of working, by doing and helping others do it.\n1st principle: highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nImplementations: Kanban, Scrum, Lean, Extreme programming\nTools:\n\nbacklog\nkanban board (not started, in progress, done)\nWIP (work in progress limit)\nprogress measures: e.g. team velocity\n\n\n\nDevOps\nIncrease efficiency by improving the connection between Dev (software development) and Ops (IT operations).\nThe goal is continuous delivery and continuous improvement.\nPractices:\n\nmodular architecture\nversion control\nmerge into trunk daily\nautomated and continuous testing, continuous integration\nautomated deployment\n\n\nDevOps in clinical reporting\nRisks around production run:\n\nare all dependencies in production?\nwas all quality control completed and successful?\nis all documentation complete?\nwas the transfer to eDMS correct and successful?"
  },
  {
    "objectID": "blog/talks/technotes_20230222_clinreport_part2/index.html#version-control",
    "href": "blog/talks/technotes_20230222_clinreport_part2/index.html#version-control",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Version control",
    "text": "Version control\nFeature branch (as opposed to master branch): one task per branch\nname feature branch: issue number and description\nEach issue should have a clear description, short and specific; instead of being long and overarching.\n\nWorkflow for clinical reporting\nRestraints of clinical deliveries: timing annd multiple deliveries; resourcing challenges\nMight need to choose between feature and GitFlow."
  },
  {
    "objectID": "blog/talks/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "href": "blog/talks/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Reproducible projects in R",
    "text": "Reproducible projects in R\nTo reproduce your work:\n\nGit (version control)\nR libraries\nWell structured projects\nUnderlying dependencies (e.g. operating systems, C++/C)\n\n\nWell structured projects\nClear names\nGood documentation\n\n\nR libraries and versions\nCheck session info; but not the most practical way.\nUse global libraries, .libPaths(), this gives you the path where all the packages are installed. Global libraries is useful when using a server for multiple R sessions, where they look for the packages in the same place.\nSolutions\n\nrenv package: makes each project in R self-contained.\nCheckpoint: project level library paths based on snapshots of CRAN\n\nUse Docker images! Saves R version, operating system, underlying dependencies"
  },
  {
    "objectID": "blog/talks/blog_20230717_teaching/index.html",
    "href": "blog/talks/blog_20230717_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Earlier this year (2023) I wrote a blog about my thoughts on the role of open source software in statisical education. Naturally, I advocate for more use of open source tools such as R/python in teaching introductory statistics to applied scientists. Nonetheless, how the material is taught will make a huge difference in the understanding and interest in the material.\nI was taught statistics in the classic way: lectures with tons of mathematical formulae and proofs, while programming and data analyses were left for students themselves to figure out. Those who were the fastest learners were the ones who already had a degree in computer science, which probably doesn’t sound surprising. I, for one, definitely struggled."
  },
  {
    "objectID": "blog/talks/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "href": "blog/talks/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Does statistics have to be daunting?",
    "text": "Does statistics have to be daunting?\nFor applied scientists in various fields, data analysis is a core task, and also a challenging one. You must have met clinicians or biologists who would love their data to be analysed yet don’t know how to. Yes, statistics and data skills can take some time to learn; but with the right method, they don’t have to be daunting. It is up to the educator to find a way that benefits the most students. An observation is that many researchers do not know or remember advanced math; yet do they need advanced math to grasp many fundamental statistical concepts?\nI believe that it is far more important and useful to teach basic IT skills and exploratory data analysis so that students can develop an understanding of their own data; rather than using a test blindly."
  },
  {
    "objectID": "blog/talks/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "href": "blog/talks/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Rebooting MF9130E classroom",
    "text": "Rebooting MF9130E classroom\nWhen I heard that the teaching team at Biostatistics Department, Faculty of Medicine was thinking about trying a novel pedagogical method on the MF9130E (2023 spring) class, I was more than excited to contribute. This is a PhD level course of 8 days long, offered three times a year (twice in Norwegian language). Students come from a variey of backgrounds in health and life sciences. Since this is an introductory course, the topics are broad rather than specialised.\nA few years ago, statistical software for the course made the transition from SPSS to Stata. To be more precise, students were introduced to, but not really explained to, or elaborated on how to use Stata proficiently. Why? The course is about statistics so only statistics is taught. Data skills such as manipulation are not part of statistics. \nWell, we will change that by starting to use R.\n\nThree open source musketeers\nR, quarto and GitHub the three musketeers in facilitating the transformation. We build a quarto course website where all the material are public, hosted with GitHub Pages. Having a course website is beneficial for students to have an overview of the course, in contrast to many scattered lecture notes and exercises to be downloaded.\nThe biggest advantage of using quarto is the rendered output from code. From a student’s perspective, it is reassuring to see the same result and plots using the data and code provided by the instructor. For the instructor, it is also convenient to see whether the code functions as expected. When we do not want to show the output, it is also very easy to suppress. We have created one copy with and one withtout rendered output as exercises, and are glad to see some students challenging themselves by attempting to solve the problems without solution.\nUsing Github and quarto together to build a course website is rather straightforward. I think the site structure is simple yet flexible enough to navigate. Collaboration across a small teaching team is also manageable. Github Pages was easy to set up, and changes made on the main branch is deployed within the minute. This proved to be useful in quite a few moments (where we had to replace some datasets or add some notice).\n\n\nThe Carpentries pedagogical model\nThe Carpentries is an organisation that teaches foundational coding and data science skills to researchers. I myself benefited from their workshop on version control and git taught at University of Oslo, and I think the traditional classroom could use some of the methods at these data science workshops.\nTo put simply, there are two things I tried with the course setup for MF9130E:\n\nLive coding demonstration, plenty of it\nSticky-notes flag and helper (teaching assistant) in class\n\nIn the live coding demonstration (which I was responsible for), I made sure that students were taught the most commonly used R commands for data manipulation and exploration. Quarto webpages on introduction to R, basic EDA, intermediate EDA have been created and guided through in class, mixed with statistical concepts and visualizations. Without knowing how your data looks like, blindly using statistical tests is dangerous - that is the motivation for doing so.\nWhether students feel supported can make a huge difference in their willingness to learn. Taking it slow at the beginning, and solve the problems on an individual basis can prevent early drop-outs, especially when programming and IT systems are involved. Naturally, when we don’t have helpers we can not help everyone; this is a limitation for this model. Students should be encouraged to help each other.\n\n\nLet them explore\nThe last important change in the class was to give time to students themselves. We reduced the lecturing on theory and computation, and added time for practice and discussion. The guided practice with live demo also came with solution and comments, so students could explore at their own pace. We left plenty of time for them to ask questions, and made sure most people can follow the exercises."
  },
  {
    "objectID": "blog/talks/blog_20230717_teaching/index.html#how-did-it-go",
    "href": "blog/talks/blog_20230717_teaching/index.html#how-did-it-go",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "How did it go?",
    "text": "How did it go?\nAfter the 8 day course we carried out a small survey among the ~50 students in the spring 2023 class. Student backgrounds are diverse, they work on lab data, clinical data or observational/epidemiological data:\n\nobservational study on humans 36%\nRCT 18%\nin vitro research 15%\nothers are in animal research, meta analysis or something else\n\nStatistical competency (method, software) among students are generally on the basic end. Over 75% of the cohort report themselves to have basic to very basic knowledge of statistics; 33% do not use any statistical software, around 45% have used SPSS or Stata. On the other hand, some students (7%) report to have advanced knowledge and have some R experience.\n\nSome feedback\nThis is the first time we do the course with R, live demo and put an emphasis on basic data manipulation and exploration - which means we do not have enough data, it is just an initial impression.\nHere’s what we have received. On the positive side, 86% find the course useful for their own PhD research. 75% felt they are able to use the correct methods for their analyses, which is quite encouraging. Most felt the examples and exercises were able to demonstrate the theory. Students have generally positive experience with the live demo, and find the instructors supportive. This is good!\nIn the meantime, it is only natural that some are dissatisfied (21%) in some ways. Common complaints are: R is not user friendly to absolute beginners; the leap from no software to a programming language is too big for some.\nAs for whether students have really mastered the knowledge intended, we do not have enough data to draw a conclusion. We do observe that the take home project show somewhat better understanding, but can not say for sure just yet.\nThis is a class with very diverse backgrounds, hence it is challenging to cater to everyone’s needs. Yet, we are satisfied with the trial-transformation with our introductory statistics class, and we plan to gradually implement more classes with R, and possibly hands-on practice (depending on capacity)."
  },
  {
    "objectID": "index.html#lately",
    "href": "index.html#lately",
    "title": "Kundan Kumar",
    "section": "Lately …",
    "text": "Lately …\n\n\nTalks\n\n\n\nSee all →"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blogs",
    "section": "",
    "text": "How to handle class-unbalanced data?\n\n\n\nclass imbalance\n\nData science\n\n\n\nThe majority class dominates while the minority class is underrepresented, leading models to bias their predictions toward the majority class.\n\n\n\n\n\nSep 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Scientist Interview Guide\n\n\n\nData science\n\nInterview Guide\n\n\n\nResearch Scientist Interview Guide\n\n\n\n\n\nJul 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Reasoning and Planning\n\n\n\nData science\n\nLarge Language Models\n\nPrompting\n\n\n\nPrompting for LLM Reasoning and Planning\n\n\n\n\n\nMay 6, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index1.html",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index1.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index1.html#overview",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index1.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index1.html#key-interview-components",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index1.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1️⃣ Research Portfolio Deep Dive\n\nBe able to explain your core research contributions in detail.\nClearly articulate: problem definition, novelty, methods, results, and real-world impact.\nPrepare multiple levels of technical depth (5-min, 15-min, 30-min versions).\nPractice connecting your work to broader research trends and applications.\n\n\n\n2️⃣ Technical Machine Learning Knowledge\n\nReinforcement Learning: algorithms, policy gradients, actor-critic, safe RL.\nDeep Learning: optimization, architecture design, generalization, transformers.\nProbabilistic Modeling: Bayesian inference, uncertainty estimation, graphical models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: LLM scaling laws, prompting, fine-tuning, RAG architectures.\nVision: object detection, segmentation, multi-modal perception.\n\n\n\n3️⃣ System Design / Applied ML Problems\n\nBe able to discuss:\n\nEnd-to-end ML pipelines\nData challenges (imbalance, noisy labels, drift)\nModel serving and deployment challenges\nScalability, latency, interpretability\n\n\n\n\n4️⃣ Coding and Algorithmic Skills\n\nLeetcode-style DSA for research interviews (moderate level)\nData manipulation (pandas, numpy, SQL)\nModel prototyping (PyTorch, TensorFlow, JAX)\n\n\n\n5️⃣ Behavioral and Collaboration Skills\n\n“Tell me about a time…” questions.\nCollaboration across teams.\nHandling ambiguous open-ended research problems.\nCommunication with product teams or non-research stakeholders."
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index1.html#recommended-preparation-resources",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index1.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nPapers: Read papers from top-tier conferences (NeurIPS, ICML, ICLR, CVPR, ACL).\nCoding: Leetcode (medium), ML system design problems.\nSystem Design: Read “Designing Machine Learning Systems” by Chip Huyen.\nMock Interviews: Practice mock sessions with peers or mentors.\nPresentation: Prepare 1-2 strong 20-minute research talks."
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index1.html#example-interview-questions",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index1.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nHow does your research contribute to state-of-the-art methods?\nWalk me through one of your recent papers.\nHow would you apply your methods to X domain?\nWhat challenges remain in your area of research?\nHow do you evaluate safety, robustness, or uncertainty in your models?\nHow would you adapt your methods if labeled data was extremely limited?"
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index1.html#my-personal-advice",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index1.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "My Personal Advice",
    "text": "My Personal Advice\n\nClarity beats complexity — explain ideas simply.\nBe enthusiastic about your work and its impact.\nConnect your strengths to the job’s mission.\nShow your ability to collaborate and iterate."
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index1.html#mentorship",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index1.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’re preparing for Research Scientist interviews and would like advice or mentorship, feel free to reach out at cs.kundann@gmail.com."
  },
  {
    "objectID": "blog/talks/index.html",
    "href": "blog/talks/index.html",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/talks/index.html#from-blogdown-to-distill",
    "href": "blog/talks/index.html#from-blogdown-to-distill",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/talks/index.html#time-to-try-quarto",
    "href": "blog/talks/index.html#time-to-try-quarto",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "Time to try Quarto",
    "text": "Time to try Quarto\nNow that I’ve finally completed the more pressing tasks in October 2022, I can catch up to the cool kids on twitter: create a website with Quarto!\nThere were quite a lot of discussions about Quarto in the summer 2022. I wasn’t following the discussions closely, but I remember there were quite a few talks in the Rstudio conference this year. Then more and more people switched to Quarto on Twitter. Then people I know also switched to Quarto. What’s the fuzz about?\nMy experience with Quarto is focused on websites. I have not tried other forms of publishing. So far I have created:\n\na workshop website for my colleagues\na personal website (the one you are reading right now)\nan R package (qtwAcademic)that wraps three Quarto website templates for beginners\n\nHere are a few things I like about Quarto. Given that I’m not very experienced in front-end development, these comments are going to be about ease-of-use and design, rather than the technicalities.\n\nClean look for both personal and workshop/courses\nWhen I was using “academic” template in blogdown, I liked the structure of the site: projects, talks, blog, softwares and publications sections are clearly displayed at the top. What I didn’t like is that the default homepage was a very long single page; yet its customisation wasn’t the easist. Other templates were either too simple (for blog only), or more suitable for image display (photography projects). I wanted a website that keep the good structure of “academic”, which is quite suitable for academics (hence the name); while keeping each section independent.\nWith distill I could achieve the structure I wanted; but I didn’t enjoy it too much as a personal website (at least it wasn’t as flexible as Quarto). distill is still pretty decent for organisations or documentation site.\nWith Quarto, I can achieve the desired looks for not only a personal website (with or without blogs), but also a workshop, event or even course website. This is fantastic! The top, sidebar or hybrid navigation makes the site structure very clear, especially when there are lots of content. As an aspiring lecturer at university, this is really One Quarto Rules Them All.\n\n\nFlexible yet not overwhelming\nAs I mentioned above, hacking “academic” in blogdown was not that easy - simply because there were too many folders that you are not actually supposed to modify. It was confusing to know what to change in order to achieve the desired output, and multiple folders were having the same names, making it very challenging for beginners. Ironically, this is usually the first template beginners start with!\nThat’s why I immediately fell for Quarto: you only need 4 components to make a decent minimalistic website work:\n\n_quarto.yml to control the overall layout\nindex.qmd at the root folder to control the homepage\nabout.qmd for some basic information about the creator or the website\nproject.qmd for projects or any other content that the creator wants to display\n\nThe way that _quarto.yml clearly specifies the .qmd files really helps beginners to understand where things are. This has been extremely useful for me when I wanted to learn how people made their website by reading the source code - I could understand exactly where to find the information I needed. The clear structure greatly helps the creators themselves, and also those who want to learn.\n\n\nGreat community\nRstats people have a great community. I wouldn’t be able to make my site the way I wanted if people haven’t been sharing their works. I have learned a lot by reading the source code by Dr Emi Tanaka, Dr David Schoch, Bea Milz, Prof Mine Cetinnkaya-Rundel’s STA 210 - Regression Analysis course.\nI also made my own R package that wraps three templates to create Quarto websites that are frequently used by academics, qtwAcademic. In the following days I plan to write up more detailed explanations on how to use the package, along with some new features."
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#from-blogdown-to-distill",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#from-blogdown-to-distill",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#time-to-try-quarto",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#time-to-try-quarto",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "Time to try Quarto",
    "text": "Time to try Quarto\nNow that I’ve finally completed the more pressing tasks in October 2022, I can catch up to the cool kids on twitter: create a website with Quarto!\nThere were quite a lot of discussions about Quarto in the summer 2022. I wasn’t following the discussions closely, but I remember there were quite a few talks in the Rstudio conference this year. Then more and more people switched to Quarto on Twitter. Then people I know also switched to Quarto. What’s the fuzz about?\nMy experience with Quarto is focused on websites. I have not tried other forms of publishing. So far I have created:\n\na workshop website for my colleagues\na personal website (the one you are reading right now)\nan R package (qtwAcademic)that wraps three Quarto website templates for beginners\n\nHere are a few things I like about Quarto. Given that I’m not very experienced in front-end development, these comments are going to be about ease-of-use and design, rather than the technicalities.\n\nClean look for both personal and workshop/courses\nWhen I was using “academic” template in blogdown, I liked the structure of the site: projects, talks, blog, softwares and publications sections are clearly displayed at the top. What I didn’t like is that the default homepage was a very long single page; yet its customisation wasn’t the easist. Other templates were either too simple (for blog only), or more suitable for image display (photography projects). I wanted a website that keep the good structure of “academic”, which is quite suitable for academics (hence the name); while keeping each section independent.\nWith distill I could achieve the structure I wanted; but I didn’t enjoy it too much as a personal website (at least it wasn’t as flexible as Quarto). distill is still pretty decent for organisations or documentation site.\nWith Quarto, I can achieve the desired looks for not only a personal website (with or without blogs), but also a workshop, event or even course website. This is fantastic! The top, sidebar or hybrid navigation makes the site structure very clear, especially when there are lots of content. As an aspiring lecturer at university, this is really One Quarto Rules Them All.\n\n\nFlexible yet not overwhelming\nAs I mentioned above, hacking “academic” in blogdown was not that easy - simply because there were too many folders that you are not actually supposed to modify. It was confusing to know what to change in order to achieve the desired output, and multiple folders were having the same names, making it very challenging for beginners. Ironically, this is usually the first template beginners start with!\nThat’s why I immediately fell for Quarto: you only need 4 components to make a decent minimalistic website work:\n\n_quarto.yml to control the overall layout\nindex.qmd at the root folder to control the homepage\nabout.qmd for some basic information about the creator or the website\nproject.qmd for projects or any other content that the creator wants to display\n\nThe way that _quarto.yml clearly specifies the .qmd files really helps beginners to understand where things are. This has been extremely useful for me when I wanted to learn how people made their website by reading the source code - I could understand exactly where to find the information I needed. The clear structure greatly helps the creators themselves, and also those who want to learn.\n\n\nGreat community\nRstats people have a great community. I wouldn’t be able to make my site the way I wanted if people haven’t been sharing their works. I have learned a lot by reading the source code by Dr Emi Tanaka, Dr David Schoch, Bea Milz, Prof Mine Cetinnkaya-Rundel’s STA 210 - Regression Analysis course.\nI also made my own R package that wraps three templates to create Quarto websites that are frequently used by academics, qtwAcademic. In the following days I plan to write up more detailed explanations on how to use the package, along with some new features."
  },
  {
    "objectID": "blog/talks/index1.html",
    "href": "blog/talks/index1.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/talks/index1.html#overview",
    "href": "blog/talks/index1.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/talks/index1.html#key-interview-components",
    "href": "blog/talks/index1.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1️⃣ Research Portfolio Deep Dive\n\nBe able to explain your core research contributions in detail.\nClearly articulate: problem definition, novelty, methods, results, and real-world impact.\nPrepare multiple levels of technical depth (5-min, 15-min, 30-min versions).\nPractice connecting your work to broader research trends and applications.\n\n\n\n2️⃣ Technical Machine Learning Knowledge\n\nReinforcement Learning: algorithms, policy gradients, actor-critic, safe RL.\nDeep Learning: optimization, architecture design, generalization, transformers.\nProbabilistic Modeling: Bayesian inference, uncertainty estimation, graphical models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: LLM scaling laws, prompting, fine-tuning, RAG architectures.\nVision: object detection, segmentation, multi-modal perception.\n\n\n\n3️⃣ System Design / Applied ML Problems\n\nBe able to discuss:\n\nEnd-to-end ML pipelines\nData challenges (imbalance, noisy labels, drift)\nModel serving and deployment challenges\nScalability, latency, interpretability\n\n\n\n\n4️⃣ Coding and Algorithmic Skills\n\nLeetcode-style DSA for research interviews (moderate level)\nData manipulation (pandas, numpy, SQL)\nModel prototyping (PyTorch, TensorFlow, JAX)\n\n\n\n5️⃣ Behavioral and Collaboration Skills\n\n“Tell me about a time…” questions.\nCollaboration across teams.\nHandling ambiguous open-ended research problems.\nCommunication with product teams or non-research stakeholders."
  },
  {
    "objectID": "blog/talks/index1.html#recommended-preparation-resources",
    "href": "blog/talks/index1.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nPapers: Read papers from top-tier conferences (NeurIPS, ICML, ICLR, CVPR, ACL).\nCoding: Leetcode (medium), ML system design problems.\nSystem Design: Read “Designing Machine Learning Systems” by Chip Huyen.\nMock Interviews: Practice mock sessions with peers or mentors.\nPresentation: Prepare 1-2 strong 20-minute research talks."
  },
  {
    "objectID": "blog/talks/index1.html#example-interview-questions",
    "href": "blog/talks/index1.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nHow does your research contribute to state-of-the-art methods?\nWalk me through one of your recent papers.\nHow would you apply your methods to X domain?\nWhat challenges remain in your area of research?\nHow do you evaluate safety, robustness, or uncertainty in your models?\nHow would you adapt your methods if labeled data was extremely limited?"
  },
  {
    "objectID": "blog/talks/index1.html#my-personal-advice",
    "href": "blog/talks/index1.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "My Personal Advice",
    "text": "My Personal Advice\n\nClarity beats complexity — explain ideas simply.\nBe enthusiastic about your work and its impact.\nConnect your strengths to the job’s mission.\nShow your ability to collaborate and iterate."
  },
  {
    "objectID": "blog/talks/index1.html#mentorship",
    "href": "blog/talks/index1.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’re preparing for Research Scientist interviews and would like advice or mentorship, feel free to reach out at cs.kundann@gmail.com."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects2/index.html",
    "href": "projects2/index.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Talks on A Multi-Objective Optimization Framework for Carbon-Aware Smart Energy Management in NAPS"
  },
  {
    "objectID": "talks/index.html#upcoming",
    "href": "talks/index.html#upcoming",
    "title": "Talks",
    "section": "",
    "text": "Talks on A Multi-Objective Optimization Framework for Carbon-Aware Smart Energy Management in NAPS"
  },
  {
    "objectID": "talks/index.html#selected-previous-talks",
    "href": "talks/index.html#selected-previous-talks",
    "title": "Talks",
    "section": "Selected previous talks",
    "text": "Selected previous talks\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nDate\n\n\n\n\n\n\n\n\nTransfer Learning in Deep Reinforcement Learning for Scalable VVC in Smart Grids\n\n\nWorkshop on Autonomous Energy Systems \n\n\n2025-07-16\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CopyOfindex.html",
    "href": "CopyOfindex.html",
    "title": "Quarto Academic, PhD",
    "section": "",
    "text": "Lab\n  \n  \n    \n     Uprofile\n  \n  \n    \n     E-mail\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     YouTube\n  \n  \n     {{&lt; ai orcid &gt;}} ORCID\n  \n  \n     {{&lt; ai clarivate &gt;}} Web of Science\n  \n  \n     {{&lt; ai scopus &gt;}} Scopus\n  \n  \n     {{&lt; ai google-scholar &gt;}} Google Scholar\n  \n\n  \n  \nI’m a Quarto Academeic Website Template adapted by Dr. Gang He.\nI’m a template that you can fork to build your own website with easy adaptation and deployment.\n\nStar this repository to bookmark it for future reference.\n\nFork this repository and rename it to YourGitHubUserName.github.io.\n\nUpdate the _quarto.yml file to configure your site’s basic settings.\n\nAdd or edit content in the following files and folders:\n\n/posts/ – posts about publications, news, events\n\nteaching.qmd – teaching information\n\nprojects.yml – research or other projects\n\npeople.qmd and /people/ – team or collaborators\n\n/files/ - profiles, images, pdfs, and includes\n\nRender and preview your site locally.\n\nPublish your site using GitHub Pages (make sure configuring your GitHub repository to publish from the docs directory, not the root folder).\nRefine and polish your content and design as needed.\n\n✅ Enjoy your new website!\n\n\n\n\nResearcher, Univerisity\n\n\n\n\n\nPhD, Univerisity"
  },
  {
    "objectID": "CopyOfindex.html#experience",
    "href": "CopyOfindex.html#experience",
    "title": "Quarto Academic, PhD",
    "section": "",
    "text": "Researcher, Univerisity"
  },
  {
    "objectID": "CopyOfindex.html#education",
    "href": "CopyOfindex.html#education",
    "title": "Quarto Academic, PhD",
    "section": "",
    "text": "PhD, Univerisity"
  },
  {
    "objectID": "CopyOfindex.html#recent-posts",
    "href": "CopyOfindex.html#recent-posts",
    "title": "Quarto Academic, PhD",
    "section": "Recent Posts",
    "text": "Recent Posts\nCheck out the latest  Papers ,  News ,  Events , and  More »\n\n\n\n\n\nNo matching items\n\n\n\nAll Posts »"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "What’s New & Updated",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects1.html",
    "href": "projects1.html",
    "title": "Projects",
    "section": "",
    "text": "Deep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nThis porfolio includes openly available educational material I have created, along with works, software, and tools I have contributed to or designed.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nThis porfolio includes openly available educational material I have created, along with works, software, and tools I have contributed to or designed.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects1/ehr-title/index.html",
    "href": "projects1/ehr-title/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Ggehr.” https://kundan-kumarr.github.io/projects1/ehr-title/."
  },
  {
    "objectID": "index.html#recently",
    "href": "index.html#recently",
    "title": "Kundan Kumar",
    "section": "Recently …",
    "text": "Recently …\n\n\nBlog\n\n\n\n\n\n\n\n\nResearch Scientist Interview Guide\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\nMy Dummy Talk\n\n\nLearning How to Show a Card\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nPublications\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nProjects\n\n\n\n\n\nNo matching items\n\n\nSee all →"
  },
  {
    "objectID": "talks/dummy_talk/index.html",
    "href": "talks/dummy_talk/index.html",
    "title": "My Dummy Talk",
    "section": "",
    "text": "This is a simple demo of a Quarto talk listing with an image, title, and subtitle."
  },
  {
    "objectID": "rpkg/rpkg.html#research-highlights",
    "href": "rpkg/rpkg.html#research-highlights",
    "title": "Research",
    "section": "🌟 Research Highlights",
    "text": "🌟 Research Highlights\n\n✅ Proposed the first Physics-Informed LSTM-PPO agent for volt-var control on 8500-node networks.\n📉 Achieved 98% reduction in voltage violations and 3× faster convergence in federated DRL.\n🧠 Developed one-shot transfer learning for control agents in complex topologies.\n🤖 Integrated LLM-guided planning into multi-building simulations via CityLearn.\n🔒 Built resilient DRL systems that withstand adversarial and distributional attacks."
  },
  {
    "objectID": "rpkg/rpkg.html#research-focus",
    "href": "rpkg/rpkg.html#research-focus",
    "title": "Research",
    "section": "🧠 ## 🧠 Research Focus Areas",
    "text": "🧠 ## 🧠 Research Focus Areas\n\nSafe & Trustworthy Reinforcement Learning\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nConstrained policy optimization and reward shaping\n\n\nPhysics-based priors in DRL\n\n\nAdversarial resilience and anomaly detection\n\n\nEpistemic and aleatoric uncertainty quantification\n\n\n\n\n\n\n\n\n\nTransfer Learning & Meta-Adaptation\n\n\n\n🎯 Objective\n\n\nEnabling rapid generalization across distribution shifts in topology, weather, or load profiles.\n\n\n🔍 Core Focus Areas\n\n\n\nTransferable actor-critic architectures\n\n\nSimulation-to-real (Sim2Real) adaptation\n\n\nMeta-RL for sample efficiency\n\n\n\n\n\n\n\n\n\nVision-Simulation Integration\n\n\n\n🎯 Objective\n\n\nBridge the gap between perception and control by combining synthetic sensors, simulated environments, and end-to-end learning pipelines.\n\n\n🔍 Core Focus Areas\n\n\n\nPerception-action loops with CARLA, AirSim\n\n\nMulti-modal representation fusion (image + state)\n\n\nAutonomous control with embedded perception modules\n\n\nEnd-to-end autonomous control pipelines\n\n\n\n\n\n\n\n\n\nLLM-Augmented Decision Systems\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nLLMs for summarizing environment states and guiding agents\n\n\nTranslating textual inputs into actionable policies\n\n\nFacilitating human-AI collaboration in dynamic tasks"
  },
  {
    "objectID": "rpkg/rpkg.html#awards",
    "href": "rpkg/rpkg.html#awards",
    "title": "Research",
    "section": "🏅 Awards & Recognition",
    "text": "🏅 Awards & Recognition\n\nIEEE PES-GM 2024 Travel Grant Recipient\n\nResearch Excellence Award – Iowa State University\n\nBest Poster Award – Grid Edge Technologies 2025\n\nNREL Outstanding Internship Recognition"
  },
  {
    "objectID": "rpkg/rpkg2.html#research-highlights",
    "href": "rpkg/rpkg2.html#research-highlights",
    "title": "Research",
    "section": "🌟 Research Highlights",
    "text": "🌟 Research Highlights\n\n✅ Proposed the first Physics-Informed LSTM-PPO agent for volt-var control on 8500-node networks.\n📉 Achieved 98% reduction in voltage violations and 3× faster convergence in federated DRL.\n🧠 Developed one-shot transfer learning for control agents in complex topologies.\n🤖 Integrated LLM-guided planning into multi-building simulations via CityLearn.\n🔒 Built resilient DRL systems that withstand adversarial and distributional attacks."
  },
  {
    "objectID": "rpkg/rpkg2.html#research-focus",
    "href": "rpkg/rpkg2.html#research-focus",
    "title": "Research",
    "section": "🧠 Research Focus Areas",
    "text": "🧠 Research Focus Areas\n\n\n\n\n🔐 Safe & Trustworthy RL\n\n\n\n\n\nRobust & Stable Learning\n\n\nDevelop agents that ensure system safety, robustness, and interpretability under uncertainty.\n\n\n\n\n\nUncertainty-Aware Policies\n\n\nQuantify epistemic and aleatoric uncertainty in high-stakes, partially observable settings.\n\n\n\n\n\n🔄 Transfer & Meta-Adaptation\n\n\n\n\n\nDomain Adaptation\n\n\nEnable agents to generalize across grids with different topologies, dynamics, and loads.\n\n\n\n\n\nMeta-RL for Efficiency\n\n\nLeverage meta-reasoning to accelerate learning in low-data, high-variance scenarios.\n\n\n\n\n\n👁️ Vision-Simulation Integration\n\n\n\n\n\nPerception-Control Fusion\n\n\nUse CARLA and AirSim to train end-to-end systems in visual RL tasks with sensors.\n\n\n\n\n\nMulti-modal Representations\n\n\nCombine visual, state, and contextual features for better decision-making.\n\n\n\n\n\n🧠 LLM-Augmented Decision Systems\n\n\n\n\n\nLLM-Guided Control\n\n\nTranslate natural language into actionable policies for real-world environments.\n\n\n\n\n\nHuman-AI Collaboration\n\n\nFacilitate interactive control loops between humans and agents using LLMs."
  },
  {
    "objectID": "rpkg/rpkg2.html#application-domains",
    "href": "rpkg/rpkg2.html#application-domains",
    "title": "Research",
    "section": "🔬 Application Domains",
    "text": "🔬 Application Domains\n\n\n\n\n\n\n\nDomain\nDescription\n\n\n\n\n⚡ Smart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\n🚘 Autonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world environments\n\n\n🛡 Secure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/rpkg2.html#awards",
    "href": "rpkg/rpkg2.html#awards",
    "title": "Research",
    "section": "🏅 Awards & Recognition",
    "text": "🏅 Awards & Recognition\n\nIEEE PES-GM 2024 Travel Grant Recipient\n\nResearch Excellence Award – Iowa State University\n\nBest Poster Award – Grid Edge Technologies 2025\n\nNREL Outstanding Internship Recognition"
  },
  {
    "objectID": "rpkg/rpkg2.html#publications",
    "href": "rpkg/rpkg2.html#publications",
    "title": "Research",
    "section": "📚 Publications",
    "text": "📚 Publications\n\n📝 Journal Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies, 2025\nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for DRL in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\nKundan Kumar, Gelli Ravikumar\nVolt-VAR Control and Attack Resiliency using Deep RL\nIEEE ISGT, 2024\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nSensor Data Regression using Deep Learning & Patterns\nIEEE ICMLA, 2022\nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, et al.\nDeep Value of Information Estimators for Human-Machine Collaboration\nACM/IEEE ICCPS, 2016"
  },
  {
    "objectID": "index1.html",
    "href": "index1.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar\n  \n\n  \n  \nHi! I’m Kundan Kumar, a Ph.D. candidate and researcher focused on building intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work bridges deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), safe and explainable AI, and computer vision, with real-world applications in smart grids, autonomous vehicles, and critical infrastructure.\nMy Ph.D. research centers on physics-informed and safety-critical DRL frameworks that embed domain knowledge, safety constraints, and uncertainty into the learning process—enabling agents to make robust and interpretable decisions in dynamic, complex environments. My research within DRL focuses on techniques such as transfer learning, uncertainty quantification, and adversarial resilience to improve generalization, safety, and reliability across diverse tasks and environments.\nI also develop LLM-integrated simulation frameworks for robotics and autonomous systems, combining vision-based perception, trajectory planning, and natural language reasoning to support high-level control and human-AI collaboration.\nBeyond research, I enjoy sharing my insights through educational content on Substack and YouTube. Outside of work, I love cooking and skating. 🛼\n\n\n\n\n\n\nOther Research Interests\n\n\n\n\nComputer Vision\n\n\nVisual perception, object detection, semantic segmentation, and sensor fusion for autonomous systems.\n\n\n\n\n\nStatistical ML\n\n\nUncertainty quantification, probabilistic modeling, and data-driven inference in dynamic environments.\n\n\n\n\n\nSelf-Driving Systems\n\n\nLearning-based control, trajectory planning, vision-based perception, and sensor fusion in autonomous driving environments.\n\n\n\n\n\n\n\nExplore My Work\n\n\nBlog\n\n\n\n\n\n\n\n\nResearch Scientist Interview Guide\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\nMy Dummy Talk\n\n\nLearning How to Show a Card\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nPublications\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nProjects\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\n\n\n\nNews Highlights\n\n\n\n\n\n\n[Jul 2025]\n\n\nSelected for the Cohere Machine Learning Summer School, hosted by Cohere Labs.\n\n\n\n\n[Mar 2025]\n\n\nOur paper on Advanced Semi-Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems has been accepted to IEEE PES General Meeting 2025.\n\n\n\n\n[Jan 2025]\n\n\nOur paper on Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids has been accepted to IEEE PES Grid Edge Technologies Conference & Exposition 2025."
  },
  {
    "objectID": "index1.html#research-summary",
    "href": "index1.html#research-summary",
    "title": "Kundan Kumar",
    "section": "🧬 Research Summary",
    "text": "🧬 Research Summary\nI focus on developing intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work spans:\n\n🔁 Physics-informed DRL\n🧠 LLM-guided autonomous reasoning\n🔍 Uncertainty-aware control\n🤖 Robotics and vision-based decision systems\n\nI design agents that embed physical laws, constraints, and uncertainties into their learning loop, enabling them to generalize across varying system topologies and environmental dynamics. I also explore transfer/meta-learning, adversarial robustness, and real-time control."
  },
  {
    "objectID": "index1.html#technical-interests",
    "href": "index1.html#technical-interests",
    "title": "Kundan Kumar",
    "section": "🔬 Technical Interests",
    "text": "🔬 Technical Interests\n\nComputer Vision: Object detection, semantic segmentation, sensor fusion\nSoftware Systems: Simulation + control co-design, OpenDSS integration\nStatistical ML: Bayesian modeling, probabilistic inference\nRobotics: Multi-modal reasoning, safe human-robot interaction"
  },
  {
    "objectID": "index1.html#news-highlights",
    "href": "index1.html#news-highlights",
    "title": "Kundan Kumar",
    "section": "📰 News Highlights",
    "text": "📰 News Highlights\n\n\n\n[Jan 2025]\n\n\nPaper accepted at IEEE Grid Edge Technologies 2025.\n\n\n\n\n[Aug 2024]\n\n\nWorkshop accepted at Pittsburgh Supercomputing Center.\n\n\n\n\n[Jul 2024]\n\n\nBayesian DRL paper accepted to IEEE PES GM 2024.\n\n\n\n\n[Nov 2023]\n\n\nDRL + Attack Resilience paper accepted to IEEE ISGT 2024.\n\n\n\n\n[Aug 2022]\n\n\nCompleted ML tracks at Oxford Machine Learning Summer School.\n\n\n\n\n[Apr 2022]\n\n\nPBMR-DP paper accepted at ICMLA 2022."
  },
  {
    "objectID": "index1.html#explore-my-work",
    "href": "index1.html#explore-my-work",
    "title": "Kundan Kumar",
    "section": "🔗 Explore My Work",
    "text": "🔗 Explore My Work\n\n\nBlog\n\n\n\nSee all →\n\n\nTalks\n\n\n\nSee all →\n\n\nPublications\n\n\n\nSee all →\n\n\nProjects\n\n\n\nSee all →"
  },
  {
    "objectID": "index1.html#other-research-interests",
    "href": "index1.html#other-research-interests",
    "title": "Kundan Kumar",
    "section": "🔍 Other Research Interests",
    "text": "🔍 Other Research Interests\n\n\n\n\n👁️ Computer Vision\n\n\nVisual perception, object detection, semantic segmentation, and sensor fusion for autonomous systems.\n\n\n\n\n\n📊 Statistical ML\n\n\nUncertainty quantification, probabilistic modeling, and data-driven inference in dynamic environments.\n\n\n\n\n\n🤖 Robotics\n\n\nLearning-based control, adaptive planning, safe human-robot interaction, and multi-modal robotics."
  },
  {
    "objectID": "others/about_me1.html",
    "href": "others/about_me1.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Gmail\n  \n  \n    \n     Github\n  \n  \n    \n     Linkedin\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar"
  },
  {
    "objectID": "others/about_me1.html#hi-there",
    "href": "others/about_me1.html#hi-there",
    "title": "Kundan Kumar",
    "section": "Hi there!",
    "text": "Hi there!\nI am a researcher focused on developing intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work spans deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), safe and explainable AI, computer vision, and robotics, with real-world applications in smart energy systems, autonomous vehicles, and critical infrastructure.\nThe core of my Ph.D. research centers on developing physics-informed and safety-critical deep reinforcement learning (DRL) frameworks that embed domain knowledge and system constraints directly into the learning process. By incorporating physical laws, safety boundaries, and system dynamics into policy optimization, I design agents capable of making robust, interpretable, and reliable decisions in dynamic, high-stakes environments. My work addresses challenges such as uncertainty quantification, adversarial resilience, and safe exploration, while enabling agents to generalize across diverse network topologies, environmental conditions, and task distributions through advanced transfer learning and meta-learning techniques.\nI also leverage the CARLA simulator for autonomous driving research, combining computer vision, trajectory planning, and policy learning in complex traffic environments. My work integrates vision-based perception models for object detection, semantic segmentation, and sensor fusion, enabling robust situational awareness for autonomous agents. In parallel, I integrate LLM-based reasoning into simulation and control frameworks to support high-level planning, adaptive decision-making, and interactive human-AI collaboration for robotics and safety-critical control.\nBeyond autonomous and energy systems, my broader research interests include probabilistic modeling, statistical machine learning, and developing AI systems that are robust, trustworthy, and deployable in real-world complex environments.\nOther Research Interests:\n\nComputer Vision: Visual perception, object detection, semantic segmentation, sensor fusion for autonomous systems.\nSoftware Systems: Scalable software engineering, simulation framework development, real-time systems integration.\nStatistical Machine Learning: Uncertainty quantification, probabilistic modeling, data-driven inference in dynamic environments.\nRobotics: Learning-based control, adaptive planning, safe human-robot interaction, multi-modal robotic systems.\n\n\n\nNews\n\n\n\n\n\n\n[Jan 2025]\n\n\nOur paper on Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids has been accepted to IEEE PES Grid Edge Technologies Conference & Exposition 2025.\n\n\n\n\n[Aug 2024]\n\n\nOur paper on Workshop for High Performance Computing has been accepted at Pittsburgh Supercomputing Center 2024.\n\n\n\n\n[Jul 2024]\n\n\nOur paper on Bayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control has been accepted to IEEE PES General Meeting 2024.\n\n\n\n\n[Nov 2023]\n\n\nOur paper Deep RL-based Volt-VAR Control and Attack Resiliency for DER-Integrated Distribution Grids was accepted to IEEE ISGT 2024.\n\n\n\n\n[Aug 2022]\n\n\nParticipated in the Oxford Machine Learning Summer School, completing tracks in MLx Health and MLx Finance.\n\n\n\n\n[Apr 2022]\n\n\nOur paper on Pattern-Based Multivariate Regression using Deep Learning (PBMR-DP) was accepted to ICMLA 2022."
  },
  {
    "objectID": "others/index1.html",
    "href": "others/index1.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar\n  \n\n  \n  \nHi! I’m Kundan Kumar, a Ph.D. candidate and researcher focused on building intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work bridges deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), safe and explainable AI, and computer vision, with real-world applications in smart grids, autonomous vehicles, and critical infrastructure.\nMy Ph.D. research centers on physics-informed and safety-critical DRL frameworks that embed domain knowledge, safety constraints, and uncertainty into the learning process—enabling agents to make robust and interpretable decisions in dynamic, complex environments. My research within DRL focuses on techniques such as transfer learning, uncertainty quantification, and adversarial resilience to improve generalization, safety, and reliability across diverse tasks and environments.\nI also develop LLM-integrated simulation frameworks for robotics and autonomous systems, combining vision-based perception, trajectory planning, and natural language reasoning to support high-level control and human-AI collaboration.\nBeyond research, I enjoy sharing my insights through educational content on Substack and YouTube. Outside of work, I love cooking and skating. 🛼\n\n\n\n\n\n\nOther Research Interests\n\n\n\n\nComputer Vision\n\n\nVisual perception, object detection, semantic segmentation, and sensor fusion for autonomous systems.\n\n\n\n\n\nStatistical ML\n\n\nUncertainty quantification, probabilistic modeling, and data-driven inference in dynamic environments.\n\n\n\n\n\nSelf-Driving Systems\n\n\nLearning-based control, trajectory planning, vision-based perception, and sensor fusion in autonomous driving environments.\n\n\n\n\n\n\n\nExplore My Work\n\n\nBlog\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\nMy Dummy Talk\n\n\nLearning How to Show a Card\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nPublications\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\nDRL\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\n\n\n\nNews Highlights\n\n\n\n\n\n\n[Jul 2025]\n\n\nSelected for the Cohere Machine Learning Summer School, hosted by Cohere Labs.\n\n\n\n\n[Mar 2025]\n\n\nOur paper on Advanced Semi-Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems has been accepted to IEEE PES General Meeting 2025.\n\n\n\n\n[Jan 2025]\n\n\nOur paper on Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids has been accepted to IEEE PES Grid Edge Technologies Conference & Exposition 2025."
  },
  {
    "objectID": "cv1.html",
    "href": "cv1.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "📄 Resume"
  },
  {
    "objectID": "cv1.html#professional-experience",
    "href": "cv1.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": " Professional Experience",
    "text": "Professional Experience\n\n\n\n\n\nNational Renewable Energy Laboratory (NREL)\n\n\n\n\n\nMachine Learning Engineer (Intern)\n\n\n May 2024  —  Jan 2025 \n\n\n\n\nDeveloped novel machine learning models for automated network topology inference and resilient control policy optimization for complex distributed systems under extreme scenarios\nDesigned and developed semi‑supervised learning approaches to tackle the challenge of limited labeled data in networks, achieving 98% improvement in model accuracy with varying labeled data.\nPaper ”Advanced Semi‑Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems” accepted at IEEE Power & Energy Society General Meeting (PES GM) 2025.\n\n\n\n\n\n\n\nComcast\n\n\n\n\n\nSoftware Engineer\n\n\n Jul 2019  —  Feb 2020 \n\n\n\n\nDesigned and implemented real‑time data processing pipelines using Amazon Kinesis and RabbitMQ, processing 1TB+ daily data for fraud detection and system monitoring.\nDeveloped machine learning models for anomaly detection and user behavior analysis, reducing fraudulent activities by 70% through predictive analytics.\nBuilt scalable Spring Boot microservices handling 10K+ concurrent requests, achieving 99.9% uptime for critical system components.\nCreated interactive dashboards using Presto DB and Python visualization tools, enabling real‑time monitoring of network performance metrics and fraud patterns.\n\n\n\n\n\n\n\nIBM\n\n\n\n\n\nSoftware Engineer\n\n\n Jan 2019  —  Jun 2019 \n\n\n\n\nLed cloud infrastructure optimization using OpenShift, implementing auto‑scaling solutions that reduced operational costs by 30%.\nDeveloped a comprehensive monitoring system using Grafana and Flask, providing real‑time visibility into 100+ cloud servers.\nImplemented automated performance monitoring and alerting system, reducing incident response time by 60%.\n\n\n\n\n\n\n\nHewlett Packard Enterprise (HPE)\n\n\n\n\n\nSoftware Engineer\n\n\n Apr 2017  —  Dec 2018 \n\n\n\n\n\nSpearheaded migration of critical applications from HPI to HPE domain, ensuring zero downtime during transition.\nImplemented OAuth 2.0 authentication system and RESTful services using Spring Boot, securing applications serving 50K+ users.\nDesigned and deployed microservices architecture on Apache/WebLogic servers, improving system response time by 40%.\n\n\n\n\n\n\n\nTata Consultancy Services (TCS)\n\n\n\n\n\nSystem Engineer\n\n\n Jul 2012  —  Dec 2015 \n\n\n\n\n\nEngineered high‑performance ETL pipelines for data warehouse integration, processing 100GB+ daily data volumes.\nOptimized database performance through advanced SQL tuning and indexing strategies, reducing query execution time by 70%.\nReceived excellence award for achieving $100K cost savings through database optimization initiatives."
  },
  {
    "objectID": "cv1.html#education",
    "href": "cv1.html#education",
    "title": "Curriculum Vitae",
    "section": " Education",
    "text": "Education\n\n\n\n\n\nIowa State University\n\n\n\n\n\nPh.D. in Computer Science (Minor: Statistics)\n\n\n 2020  —  2025 (Expected) \n\n\n\n\n\nResearch: Deep RL, Physics-Informed AI, Uncertainty Quantification\nCourses: Deep Learning, NLP, Statistical Theory, Empirical Methods, Algorithms\n\n\n\n\nMS in Computer Science\n\n\n Jan 2015  —  Dec 2016 \n\n\n\n\nFocus: Algorithms, Databases, Network Programming"
  },
  {
    "objectID": "cv1.html#teaching-experience",
    "href": "cv1.html#teaching-experience",
    "title": "Curriculum Vitae",
    "section": " Teaching Experience",
    "text": "Teaching Experience\n\n\n\n\n\nIowa State University\n\n\n\n\n\nTeaching Assistant\n\n\n 2020  —  2025 \n\n\n\nDepartment of Computer Science\n\nSupported undergraduate/graduate courses including Software Development Practices, Database Systems, and Spreadsheets.\nLed weekly lab sessions, assisted students with debugging and conceptual challenges, and held office hours.\nDesigned assignments and quizzes aligned with real-world workflows and agile development practices.\nMentored students on semester-long capstone projects simulating software engineering team experiences."
  },
  {
    "objectID": "cv1.html#research-experience",
    "href": "cv1.html#research-experience",
    "title": "Curriculum Vitae",
    "section": " Research Experience",
    "text": "Research Experience\n\n\n\n\n\nIowa State University\n\n\n\n\n\nResearch Assistant\n\n\n Aug 2022  —  Jul 2025 \n\n\n\n\nResearch on Physics‑Informed Deep Reinforcement Learning for Critical Infrastructure Systems, focusing on Intelligent Resource Management and Security in Large‑Scale Distributed Networks.\nApplied computational deep reinforcement learning algorithms in a Smart Energy System to analyze power simulation data, minimizing voltage violations, power loss, and control errors.\nDeveloped physics‑informed DRL algorithms incorporating domain‑specific physical constraints, achieving 30% improvement in resource allocation efficiency and reducing system violations in complex distributed networks.\nDesigned and implemented adversarial attack detection and mitigation frameworks for AI models in critical systems, enhancing robustness against security threats through systematic testing and defensive techniques.\nCreated novel transfer learning methodologies enabling DRL models to adapt across varying network sizes and topologies, reducing training time by 40% for new configurations.\nDeveloped Python‑based simulation and control framework integrating real‑time hardware (OPAL‑RT and OpenDSS) with distributed systems.\nLeveraged LLM‑driven reasoning and contextual understanding within simulation environments to support real‑time adaptive control, human‑AI collaboration, and predictive system optimization.\n\n\n\n\nResearch Assistant\n\n\n Aug 2020  —  Jul 2022 \n\n\n\n\nResearch on Deep Reinforcement Learning (DRL) and Safety‑Critical Learning for Autonomous Systems, with focus on perception, control, and decision‑making in high‑stakes environments.\nUtilized CARLA simulator for vision‑based autonomous driving tasks, including perception, object detection, trajectory planning, and policy learning in complex traffic scenarios.\nApplied deep computer vision models for object recognition, semantic segmentation, and sensor fusion, enabling robust situational awareness in autonomous driving and robotics."
  },
  {
    "objectID": "cv1.html#skills",
    "href": "cv1.html#skills",
    "title": "Curriculum Vitae",
    "section": " Skills",
    "text": "Skills\n\n\n\nProgramming Languages\n\n\nPython, R, Java, C++, SAS, MATLAB, SQL, HTML, JS, Node.js, React.js\n\n\n\n\nML & Data Analysis\n\n\nscikit-learn, TensorFlow, PyTorch, Pandas, Matplotlib, Seaborn, Gym, RLlib\n\n\n\n\nLLMs & NLP\n\n\nHugging Face Transformers, LangChain, RAG, Prompt Engineering\n\n\n\n\nHPC & Big Data\n\n\nHadoop, Hive, Spark, Kafka, Kinesis, SLURM, MPI, OpenMP\n\n\n\n\nSimulation & Modeling\n\n\nOpal-RT, OpenDSS (Power), Carla (Autonomous Driving)\n\n\n\n\nOptimization\n\n\nGurobi, Pyomo, BoTorch, Optuna, Hyperopt\n\n\n\n\nVisualization & GIS\n\n\nTableau, ArcGIS, Leaflet\n\n\n\n\nCloud & DevOps\n\n\nAWS (EC2, S3, Lambda), GCP, Docker, Kubernetes, Git, Terraform, Jenkins, CircleCI"
  },
  {
    "objectID": "cv1.html#honors-awards",
    "href": "cv1.html#honors-awards",
    "title": "Curriculum Vitae",
    "section": " Honors & Awards",
    "text": "Honors & Awards\n\nSelected, Seventh Workshop on Autonomous Energy Systems @ NREL (2024)\nSelected, ByteBoost Workshop on Accelerating HPC Research Skills (2024)\nSelected, Oxford Machine Learning Summer School (OxML) (2022)\nExcellence Award, Database Optimization @ TCS\n2nd Place, BAJA SAE India (Safest Terrain Vehicle Category, National Level)"
  },
  {
    "objectID": "cv1.html#service",
    "href": "cv1.html#service",
    "title": "Curriculum Vitae",
    "section": " Service",
    "text": "Service\n\nReviewer:\n\nIEEE Transactions on Industrial Informatics (2025)\nConference on Neural Information Processing Systems (Ethics)(2025)\nIEEE Transactions on Neural Networks and Learning Systems (2024)\nIEEE PES GM, Grid Edge & ISGT (2023, 2024)\n\nMock Interviewer: Supporting underrepresented minorities in tech.\nVolunteer, Prayaas India (BIT): NGO providing quality education to underprivileged children in slums and villages."
  },
  {
    "objectID": "cv1.html#projects",
    "href": "cv1.html#projects",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n\n\n\n\n   Fast Mixed‑Logit Estimation Fast estimation of mixed logit models with preference‑space utility.  \n   cbcTools Suite Designing choice‑based conjoint experiments and power and system analyses.  \n   LLM‑Powered Energy Optimizer Multi‑building energy optimization in CityLearn with LLM guidance.  \n\n\n 📁 Check My Projects"
  },
  {
    "objectID": "others/Index2.html",
    "href": "others/Index2.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar\n  \n\n  \n  \nHello!\nI’m Kundan Kumar — a Ph.D. candidate and researcher building safe and trustworthy AI systems for real-world autonomous applications. My research integrates deep reinforcement learning (DRL), physics-informed AI, and safe control to create robust agents for smart energy systems, robotics, and autonomous vehicles.\nIn 2024, I worked as a Machine Learning Engineer Intern at the National Renewable Energy Laboratory (NREL), where I designed a Bayesian semi-supervised learning algorithm for phase identification on small, limited, and unreliable datasets. This work will be presented at the IEEE PES General Meeting 2025.\nI’m passionate about democratizing AI knowledge. I create educational content simplifying AI, ML, and statistics on Substack and YouTube.\nOutside of research, I enjoy cooking and skating. 🛼"
  },
  {
    "objectID": "others/Index2.html#recently",
    "href": "others/Index2.html#recently",
    "title": "Kundan Kumar",
    "section": "Recently …",
    "text": "Recently …\n\n\nBlog\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\nMy Dummy Talk\n\n\nLearning How to Show a Card\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nPublications\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\nDRL\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →"
  },
  {
    "objectID": "rpkg/research.html",
    "href": "rpkg/research.html",
    "title": "Research",
    "section": "",
    "text": "I aim to develop safe, interpretable, and adaptive AI systems for real-world cyber-physical environments that operate under uncertainty, constraints, and adversarial conditions. My research bridges the domains of machine learning, optimization, and control theory, with a strong emphasis on safety, robustness, and generalization.\nMy work centers around the following pillars:\n\nSafe & Trustworthy Reinforcement Learning: Designing agents that are robust to adversarial attacks, resilient to distributional shifts, and capable of safe exploration.\nPhysics-informed Deep Reinforcement Learning (DRL): Embedding physical laws and constraints into learning frameworks for stability, interpretability, and faster convergence.\nProbabilistic & Bayesian Modeling: Probabilistic & Bayesian Modeling: Capturing both epistemic and aleatoric uncertainties for reliable control in high-stakes, partially observable systems.\nLarge Language Models (LLMs) for autonomous reasoning: Leveraging large language models (LLMs) to enhance planning, explainability, and human-AI collaboration in control systems.\nVision-based simulation environments: Using platforms like CARLA and CityLearn to train agents in multimodal, visually rich, and interactive worlds.\n\nBy tightly integrating domain knowledge into learning frameworks, I aim to enable resilient, generalizable, and safe AI for critical applications including smart grids, autonomous systems, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/research.html#research-vision",
    "href": "rpkg/research.html#research-vision",
    "title": "Research",
    "section": "",
    "text": "I aim to develop safe, interpretable, and adaptive AI systems for real-world cyber-physical environments that operate under uncertainty, constraints, and adversarial conditions. My research bridges the domains of machine learning, optimization, and control theory, with a strong emphasis on safety, robustness, and generalization.\nMy work centers around the following pillars:\n\nSafe & Trustworthy Reinforcement Learning: Designing agents that are robust to adversarial attacks, resilient to distributional shifts, and capable of safe exploration.\nPhysics-informed Deep Reinforcement Learning (DRL): Embedding physical laws and constraints into learning frameworks for stability, interpretability, and faster convergence.\nProbabilistic & Bayesian Modeling: Probabilistic & Bayesian Modeling: Capturing both epistemic and aleatoric uncertainties for reliable control in high-stakes, partially observable systems.\nLarge Language Models (LLMs) for autonomous reasoning: Leveraging large language models (LLMs) to enhance planning, explainability, and human-AI collaboration in control systems.\nVision-based simulation environments: Using platforms like CARLA and CityLearn to train agents in multimodal, visually rich, and interactive worlds.\n\nBy tightly integrating domain knowledge into learning frameworks, I aim to enable resilient, generalizable, and safe AI for critical applications including smart grids, autonomous systems, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/research.html#research-highlights",
    "href": "rpkg/research.html#research-highlights",
    "title": "Research",
    "section": "🌟 Research Highlights",
    "text": "🌟 Research Highlights\n\n✅ Proposed the first Physics-Informed LSTM-PPO agent for volt-var control on 8500-node networks.\n📉 Achieved 98% reduction in voltage violations and 3× faster convergence in federated DRL.\n🧠 Developed one-shot transfer learning for control agents in complex topologies.\n🤖 Integrated LLM-guided planning into multi-building simulations via CityLearn.\n🔒 Built resilient DRL systems that withstand adversarial and distributional attacks."
  },
  {
    "objectID": "rpkg/research.html#research-focus",
    "href": "rpkg/research.html#research-focus",
    "title": "Research",
    "section": "My Research Focus Areas",
    "text": "My Research Focus Areas\n\n\n\n\n\nDRL-based Control\n\n\n   DRL for Volt-VAR Design control agents for voltage regulation and reactive power optimization in smart distribution grids.  \n   Physics-Informed Actor-Critic Embed grid physics and control limits directly into the DRL learning loop for stable and efficient decisions.  \n   Sim-to-Real Transfer Train agents in simulated OpenDSS environments and deploy them on real-time OPAL-RT setups.  \n\n\n\n\nSafe & Trustworthy RL\n\n\n   Robust & Stable Learning Develop agents that ensure system safety, robustness, and interpretability under uncertainty.  \n   Uncertainty-Aware Policies Quantify epistemic and aleatoric uncertainty in high-stakes, partially observable settings.  \n\n\n\n\nTransfer & Meta-Adaptation\n\n\n   Domain Adaptation Enable agents to generalize across grids with different topologies, dynamics, and loads.  \n   Meta-RL for Efficiency Leverage meta-reasoning to accelerate learning in low-data, high-variance scenarios.  \n\n\n\nVision-Simulation Integration\n\n\n   Perception-Control Fusion Use CARLA and AirSim to train end-to-end systems in visual RL tasks with sensors.  \n   Multi-modal Representations Combine visual, state, and contextual features for better decision-making.  \n\n\n\nLLM-Augmented Decision Systems\n\n\n\n\n   LLM-Guided Control Translate natural language into actionable policies for real-world environments."
  },
  {
    "objectID": "rpkg/research.html#application-domains",
    "href": "rpkg/research.html#application-domains",
    "title": "Research",
    "section": "Application Domains",
    "text": "Application Domains\n\n\n\nDomain\nDescription\n\n\n\n\nSmart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\nAutonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world environments\n\n\nSecure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/research.html#publications",
    "href": "rpkg/research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\nJournal PapersConference Papers\n\n\n\nArif Hussian, Kundan Kumar, Gelli Ravikumar\nBayesian-optimized bidirectional long-short-term memory network for wind power forecasting with uncertainty quantification , Electric Power Systems Research, 2026\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review), IEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\n\nKundan Kumar, Kumar Utkarsh, Wang Jiyu and Padullaparti Harsha Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems in Proceedings of the IEEE PES General Meeting, 2025\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids in Proceedings of the IEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control in Proceedings of the IEEE PES General Meeting, 2024\n Paper Code Poster \n\n\n\n\nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids in Proceedings of the IEEE ISGT, 2024  Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression in Proceedings of the IEEE ICMLA, 2022  Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering in Proceedings of the ACM/IEEE ICCPS, 2016  Paper Code Poster \n\n\n\n\n Show More"
  },
  {
    "objectID": "rpkg/research.html#ongoing-projects",
    "href": "rpkg/research.html#ongoing-projects",
    "title": "Research",
    "section": "Ongoing Projects",
    "text": "Ongoing Projects\n\nFederated DRL for Cyber-Resilient Volt-VAR Optimization\nDecentralized, communication-efficient control using LSTM-enhanced PPO agents across distributed DERs.\nOne-Shot Policy Transfer with Physics Priors\nTrain agents on small topologies and adapt to IEEE 123-bus, 8500-node networks in a few iterations.\nLLM-Guided Autonomous Planning for Smart Buildings\nConvert user prompts to interpretable control policies using LLMs (OpenAI, Claude) in CityLearn environments."
  },
  {
    "objectID": "rpkg/research1.html",
    "href": "rpkg/research1.html",
    "title": "Research",
    "section": "",
    "text": "I aim to develop safe, interpretable, and adaptive AI systems for real-world cyber-physical environments that operate under uncertainty, constraints, and adversarial conditions. My research bridges the domains of machine learning, optimization, and control theory, with a strong emphasis on safety, robustness, and generalization.\nMy work centers around the following pillars:\n\nSafe & Trustworthy Reinforcement Learning: Designing agents that are robust to adversarial attacks, resilient to distributional shifts, and capable of safe exploration.\nPhysics-informed Deep Reinforcement Learning (DRL): Embedding physical laws and constraints into learning frameworks for stability, interpretability, and faster convergence.\nProbabilistic & Bayesian Modeling: Probabilistic & Bayesian Modeling: Capturing both epistemic and aleatoric uncertainties for reliable control in high-stakes, partially observable systems.\nLarge Language Models (LLMs) for autonomous reasoning: Leveraging large language models (LLMs) to enhance planning, explainability, and human-AI collaboration in control systems.\nVision-based simulation environments: Using platforms like CARLA and CityLearn to train agents in multimodal, visually rich, and interactive worlds.\n\nBy tightly integrating domain knowledge into learning frameworks, I aim to enable resilient, generalizable, and safe AI for critical applications including smart grids, autonomous systems, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/research1.html#research-vision",
    "href": "rpkg/research1.html#research-vision",
    "title": "Research",
    "section": "",
    "text": "I aim to develop safe, interpretable, and adaptive AI systems for real-world cyber-physical environments that operate under uncertainty, constraints, and adversarial conditions. My research bridges the domains of machine learning, optimization, and control theory, with a strong emphasis on safety, robustness, and generalization.\nMy work centers around the following pillars:\n\nSafe & Trustworthy Reinforcement Learning: Designing agents that are robust to adversarial attacks, resilient to distributional shifts, and capable of safe exploration.\nPhysics-informed Deep Reinforcement Learning (DRL): Embedding physical laws and constraints into learning frameworks for stability, interpretability, and faster convergence.\nProbabilistic & Bayesian Modeling: Probabilistic & Bayesian Modeling: Capturing both epistemic and aleatoric uncertainties for reliable control in high-stakes, partially observable systems.\nLarge Language Models (LLMs) for autonomous reasoning: Leveraging large language models (LLMs) to enhance planning, explainability, and human-AI collaboration in control systems.\nVision-based simulation environments: Using platforms like CARLA and CityLearn to train agents in multimodal, visually rich, and interactive worlds.\n\nBy tightly integrating domain knowledge into learning frameworks, I aim to enable resilient, generalizable, and safe AI for critical applications including smart grids, autonomous systems, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/research1.html#publications",
    "href": "rpkg/research1.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\nJournal PapersConference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \n\n\n\n\nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n\n\n Show More"
  },
  {
    "objectID": "rpkg/research1.html#conference-papers",
    "href": "rpkg/research1.html#conference-papers",
    "title": "Conference Papers",
    "section": "",
    "text": "&lt;div class=\"paper-card\"&gt;\n  &lt;strong&gt;Kundan Kumar&lt;/strong&gt;, Gelli Ravikumar&lt;br&gt;\n  &lt;em&gt;Transfer Learning Enhanced Deep Reinforcement Learning for Volt–Var Control in Smart Grids&lt;/em&gt;&lt;br&gt;\n  &lt;strong&gt;IEEE PES Grid Edge Technologies Conference & Exposition, 2025&lt;/strong&gt;\n  &lt;div class=\"paper-links\"&gt;\n    &lt;a class=\"btn btn-primary btn-sm\" href=\"https://arxiv.org/abs/2202.13541\"&gt;Paper&lt;/a&gt;\n    &lt;a class=\"btn btn-success btn-sm\" href=\"#\"&gt;Code&lt;/a&gt;\n    &lt;a class=\"btn btn-info btn-sm\" href=\"#\"&gt;Poster&lt;/a&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n\n&lt;div class=\"paper-card\"&gt;\n  &lt;strong&gt;Kundan Kumar&lt;/strong&gt;, Aditya Akilesh Mantha, Gelli Ravikumar&lt;br&gt;\n  &lt;em&gt;Bayesian Optimization for Deep Reinforcement Learning in Robust Volt–Var Control&lt;/em&gt;&lt;br&gt;\n  &lt;strong&gt;IEEE PES General Meeting, 2024&lt;/strong&gt;\n  &lt;div class=\"paper-links\"&gt;\n    &lt;a class=\"btn btn-primary btn-sm\" href=\"https://arxiv.org/abs/2202.13541\"&gt;Paper&lt;/a&gt;\n    &lt;a class=\"btn btn-success btn-sm\" href=\"#\"&gt;Code&lt;/a&gt;\n    &lt;a class=\"btn btn-info btn-sm\" href=\"#\"&gt;Poster&lt;/a&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n\n&lt;div class=\"paper-card\"&gt;\n  &lt;strong&gt;Kundan Kumar&lt;/strong&gt;, Gelli Ravikumar&lt;br&gt;\n  &lt;em&gt;Deep RL–based Volt–VAR Control and Attack Resiliency for DER–integrated Distribution Grids&lt;/em&gt;&lt;br&gt;\n  &lt;strong&gt;IEEE ISGT, 2024&lt;/strong&gt;\n  &lt;div class=\"paper-links\"&gt;\n    &lt;a class=\"btn btn-primary btn-sm\" href=\"https://ieeexplore.ieee.org/document/10454163\"&gt;Paper&lt;/a&gt;\n    &lt;a class=\"btn btn-success btn-sm\" href=\"#\"&gt;Code&lt;/a&gt;\n    &lt;a class=\"btn btn-info btn-sm\" href=\"#\"&gt;Poster&lt;/a&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n\n&lt;button onclick=\"showPage(1)\"&gt;1&lt;/button&gt;\n&lt;button onclick=\"showPage(2)\"&gt;2&lt;/button&gt;\n&lt;button onclick=\"showPage(3)\"&gt;3&lt;/button&gt;"
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#personal-advice",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "Personal Advice",
    "text": "Personal Advice\n\nFocus on clarity — aim to teach, not just impress.\nKeep your message grounded in the real-world impact of your work.\nBe honest about challenges or failures — show how you learned from them.\nShow excitement for the work and curiosity about new ideas.\nPractice regularly and iterate based on feedback."
  },
  {
    "objectID": "publication/ph_20230330_sp/index.html",
    "href": "publication/ph_20230330_sp/index.html",
    "title": "Public health surveillance and reporting",
    "section": "",
    "text": "Time and place: Mar. 30, 2023 12:00 PM–1:00 PM\nHybrid: Georg Sverdrups hus and Zoom\nEvent page"
  },
  {
    "objectID": "publication/ph_20230330_sp/index.html#about-the-topic",
    "href": "publication/ph_20230330_sp/index.html#about-the-topic",
    "title": "Public health surveillance and reporting",
    "section": "About the topic",
    "text": "About the topic\nSituational awareness is key to fast response during a public health emergency, such as COVID-19 pandemic. However, making disease surveillance reports that cover different geographical units for various metrics and data registries is both resource intensive and time consuming. Open source tools such as R packages, GitHub and Airflow can make this process automatic, reproducible and scalable.\nEvery day during the pandemic, Sykdomspulsen team at the Norwegian Institute of Public Health (FHI/NIPH) fetched data from more than 15 data sources, cleaned, censored datasets and carried out a wide range of statistical analyses. Over 1000 situational reports containing automated graphs and tables were produced before breakfast time.\nGrab you matpakke and join us for a presentation from Chi Zhang about how Sykdomspulsen team used and developed open source software to make public health surveillance and reporting more efficient, followed up by a discussion on the benefits and concerns of making these data public. We will end with an open Q&A session as usual!"
  },
  {
    "objectID": "publication/ehr_20240918_betterehr/index.html",
    "href": "publication/ehr_20240918_betterehr/index.html",
    "title": "One step closer to better Electronic Health Records data",
    "section": "",
    "text": "Real-World Data (RWD) like Electronic Health Records (EHR) is crucial for understanding drug usage and various treatments and generating Real-World Evidence (RWE). Risk prediction has been a major application where EHR is used, and there is now a shift towards causal inference, which requires data of even higher quality. Patients undergo treatments (drugs, procedures) at various times during their hospital stays, yet the data being recorded are messy and error-prone for various reasons. Analysts spend significant amount of time to sit together with clinicians to identify and understand abnormal records, and unfortunately this process is challenging to automate.\nThis talk will use an example on antibiotics prescription and use at a Nordic hospital to illustrate how some EHR systems can improve for better clinical decision-making and better data for research. I will also introduce a pilot R package (ggehr) that facilitates visual exploration of EHR data, and how it can help reconstruct patient journeys and enable analysts to perform effective quality control."
  },
  {
    "objectID": "publication/community_20240710_camis/index.html",
    "href": "publication/community_20240710_camis/index.html",
    "title": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations",
    "section": "",
    "text": "2024.7.8-11, Salzburg, Austria. Conference link: UseR!\nStatisticians using multiple softwares (SAS, R, Python) will have found differences in analysis results that warrant further justification. Whilst some industries may accept results not being the same as long as they are “close”, the highly regulated pharmaceutical industry would require an identical match in results. Yet, discrepancies might still occur, and knowing the reasons (different methods, options, algorithms etc) is critical to the modern statistician and subsequent regulatory submissions.\nIn this talk I will introduce CAMIS: Comparing Analysis Method Implementations in Software. https://psiaims.github.io/CAMIS/ It is a joint-project between PHUSE, the R Validation Hub, PSI AIMS, R consortium and openstatsware. The aim of CAMIS is to investigate and document differences and similarities between different statistical softwares such as SAS and R. We use Quarto and Github to document methods, algorithms and comparisons between softwares through small case studies, and all articles are contributed by the community. In the transition from proprietary to open source technology in the industry, CAMIS can serve as a guidebook to navigate this process.\n\nkeywords: cross industry collaboration, multi-lingua, open-source, quarto"
  },
  {
    "objectID": "publication/dummy_talk/index.html",
    "href": "publication/dummy_talk/index.html",
    "title": "My Dummy Talk",
    "section": "",
    "text": "This is a simple demo of a Quarto talk listing with an image, title, and subtitle."
  },
  {
    "objectID": "publication/ph_20220616_splverse/index.html",
    "href": "publication/ph_20220616_splverse/index.html",
    "title": "Sykdomspulsen: An automated public health surveillance platform",
    "section": "",
    "text": "About the talk\nWatch the talk on YouTube\nSykdomspulsen is a real-time analysis and disease surveillance system designed at developed at the Norwegian Institute of Public Health (FHI). Sykdomspulsen processes new data collected from 15 data sources (e.g., covid-19 cases), runs 1000.000+ statistical analysis automatically for all locations (nation, county, municipality) in Norway, produces 1000+ reports and alerts for public health authorities and shares data to the public on GitHub.\nSykdomspulsen runs on a collection of R packages, the {splverse}. {splverse} is an ecosystem for infectious disease surveillance, from analysis planning, statistical analysis to reporting via visualization, shiny website and Rmarkdown generated reports. In this talk, Chi will present how Sykdomspulsen does public health real-time surveillance during the pandemic using R. Chi will introduce some of the core packages and illustrate how they work together, with an example using real surveillance data published daily on GitHub.\n\n\nAbout the speaker\nChi is currently working at the Sykdomspulsen team as a researcher and R developer, at the Norwegian Institute of Public Health. Before she joined Sykdomspulsen in the middle of the pandemic (2020), she was a PhD student at the Department of Biostatistics at University of Oslo (OCBE), working on hospital EHR data."
  },
  {
    "objectID": "publication/ehr_20221013_ml_icu/index.html",
    "href": "publication/ehr_20221013_ml_icu/index.html",
    "title": "Machine Learning in Intensive Care Units",
    "section": "",
    "text": "A 45 minutes trial lecture to fulfill the requirement of my PhD degree."
  },
  {
    "objectID": "publication/rstats_20240613_teaching/index.html",
    "href": "publication/rstats_20240613_teaching/index.html",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "",
    "text": "Time and place: June 13 2024. Online\nSlides for this talk can be accessed here."
  },
  {
    "objectID": "publication/rstats_20240613_teaching/index.html#about-the-topic",
    "href": "publication/rstats_20240613_teaching/index.html#about-the-topic",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "About the topic",
    "text": "About the topic\nThe Department of Biostatistics at University of Oslo offer statistics courses at different levels for medical students and PhD candidates with clinical backgrounds. The courses were traditionally taught with a focus on theory instead of data analysis, where SPSS and STATA were the tools of choice.\nSince 2023 spring semester, we have been gradually transforming some of our statistics courses into R, using Quarto course websites and Carpentries style live-coding instruction. With new Quarto tools (such as WebR) we also added interactivity in the code blocks. So far we have transformed two courses with over 100 students who have almost no programming experience. We have observed impressive progress in the skill development, and received significantly more positive feedback when it comes to statistics education.\nIn this talk, I would like to share our experience on the successes and challenges throughout the process. Looking back, is it cost-effective? Definitely. Can we do better in the future? Almost surely. If you are also planning to adopt new technology in your teaching activities, join us to learn more about what you can do to make the transition happen!\nCourse website can be accessed here"
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "publication/index.html#upcoming",
    "href": "publication/index.html#upcoming",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "publication/index.html#selected-previous-talks",
    "href": "publication/index.html#selected-previous-talks",
    "title": "Talks",
    "section": "Selected previous talks",
    "text": "Selected previous talks\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\nDate\n\n\n\n\n\n\nUse Quarto, Make Friends\n\n\nKolkata UseR meetup\n\n\n2024-09-21\n\n\n\n\nOne step closer to better Electronic Health Records data\n\n\nPHUSE Single Day Event Basel\n\n\n2024-09-18\n\n\n\n\nCAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n\n\n\n\n\n2024-07-10\n\n\n\n\nA one year recap on teaching statistcis to medical students: how can R and Quarto help?\n\n\nR/Medicine 2024 - Online\n\n\n2024-06-13\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\nBasel R meeting\n\n\n2023-07-21\n\n\n\n\nMachine Learning in Intensive Care Units\n\n\nPhD defence trial lecture\n\n\n2022-10-13\n\n\n\n\nSykdomspulsen: An automated public health surveillance platform\n\n\nOslo UseR meetup \n\n\n2022-06-16\n\n\n\n\nNetwork Analysis of Hospital EHR data\n\n\nBig Insight Day, Oslo\n\n\n2021-02-18\n\n\n\n\nBuilding Website in R: Step by Step Introduction to blogdown\n\n\nTalk at Oslo UseR meetup\n\n\n2019-04-02\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication/ehr_20210218_biday/index.html",
    "href": "publication/ehr_20210218_biday/index.html",
    "title": "Network Analysis of Hospital EHR data",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "publication/community_20240921_quartofriends/index.html",
    "href": "publication/community_20240921_quartofriends/index.html",
    "title": "Use Quarto, Make Friends",
    "section": "",
    "text": "It has been two years since Quarto became the most popular reproducible publication tool in data science and R community. However Quarto is so much more than just a publication tool! I started using it since late 2022, and it has helped me become more organized, productive and connected with people in the data science community.\nIn this talk I will not focus on the technical aspects on ‘how’ to use this tool. In the first part of the talk, I would like to report the latest news and trends seen in the useR conference and Posit conf, the two biggest global R events. In the second part, I will share my own experience in using Quarto for my career: from learning new skills, collaborating with co-workers, teaching university courses to networking and building a community (CAMIS collaboration). It is a powerful tool to share your work, and make new connections - both for work and for fun! I hope this talk will provide you with some new ideas on how to use this fantastic technology to fulfill your goals."
  },
  {
    "objectID": "publication/rstats_20190402_blogdown/index.html",
    "href": "publication/rstats_20190402_blogdown/index.html",
    "title": "Building Website in R: Step by Step Introduction to blogdown",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "publication/rstats_20230721_teaching/index.html",
    "href": "publication/rstats_20230721_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Time and place: July 21, 2023 10AM. Roche office, Basel, Switzerland\nSlides for this talk can be accessed here."
  },
  {
    "objectID": "publication/rstats_20230721_teaching/index.html#about-the-topic",
    "href": "publication/rstats_20230721_teaching/index.html#about-the-topic",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "About the topic",
    "text": "About the topic\nThe 8 day introductory statistics course (MF9130) at the Faculty of Medicine, University of Oslo is designed for PhD students in medicine, biology, psychology and other health related fields. Similar to other conventional teaching methods, the course has been focusing largely on theory and hand calculation. The software has been Stata and SPSS, and data analysis was mostly left for the students to figure out on their own.\nThis year, we made an attempt to transform the course with R, and aimed to teach more practical data analysis skills. We added one session per day where the instructor guide students on R and project management, importing data , basic manipulation and statistical methods. The IT skills of the students vary greatly, and therefore we used the ‘sticky notes’ help system borrowed from the Carpentries to make sure everyone could get help in the first days. We have created a course website using Quarto, where all the material and R exercises (with rendered solution) are available for self-study. We have witnessed amazing progress - by the end of the first week, students with the least computer / data skills were able to work on dataframes, make basic plots and do a chi-squared test. This helps build students confidence in data and statistics, and as a result, they can start to work on their own datasets using the skills immediately."
  },
  {
    "objectID": "projects/ehr-title/index.html",
    "href": "projects/ehr-title/index.html",
    "title": "AI-Powered Patient Education System",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr"
  },
  {
    "objectID": "projects/drl_mdp/index.html",
    "href": "projects/drl_mdp/index.html",
    "title": "DRL",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "blog/talks/Rsearch_sci.html",
    "href": "blog/talks/Rsearch_sci.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/talks/Rsearch_sci.html#overview",
    "href": "blog/talks/Rsearch_sci.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/talks/Rsearch_sci.html#key-interview-components",
    "href": "blog/talks/Rsearch_sci.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1️⃣ Research Portfolio Deep Dive\n\nBe able to explain your core research contributions in detail.\nClearly articulate: problem definition, novelty, methods, results, and real-world impact.\nPrepare multiple levels of technical depth (5-min, 15-min, 30-min versions).\nPractice connecting your work to broader research trends and applications.\n\n\n\n2️⃣ Technical Machine Learning Knowledge\n\nReinforcement Learning: algorithms, policy gradients, actor-critic, safe RL.\nDeep Learning: optimization, architecture design, generalization, transformers.\nProbabilistic Modeling: Bayesian inference, uncertainty estimation, graphical models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: LLM scaling laws, prompting, fine-tuning, RAG architectures.\nVision: object detection, segmentation, multi-modal perception.\n\n\n\n3️⃣ System Design / Applied ML Problems\n\nBe able to discuss:\n\nEnd-to-end ML pipelines\nData challenges (imbalance, noisy labels, drift)\nModel serving and deployment challenges\nScalability, latency, interpretability\n\n\n\n\n4️⃣ Coding and Algorithmic Skills\n\nLeetcode-style DSA for research interviews (moderate level)\nData manipulation (pandas, numpy, SQL)\nModel prototyping (PyTorch, TensorFlow, JAX)\n\n\n\n5️⃣ Behavioral and Collaboration Skills\n\n“Tell me about a time…” questions.\nCollaboration across teams.\nHandling ambiguous open-ended research problems.\nCommunication with product teams or non-research stakeholders."
  },
  {
    "objectID": "blog/talks/Rsearch_sci.html#recommended-preparation-resources",
    "href": "blog/talks/Rsearch_sci.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nPapers: Read papers from top-tier conferences (NeurIPS, ICML, ICLR, CVPR, ACL).\nCoding: Leetcode (medium), ML system design problems.\nSystem Design: Read “Designing Machine Learning Systems” by Chip Huyen.\nMock Interviews: Practice mock sessions with peers or mentors.\nPresentation: Prepare 1-2 strong 20-minute research talks."
  },
  {
    "objectID": "blog/talks/Rsearch_sci.html#example-interview-questions",
    "href": "blog/talks/Rsearch_sci.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nHow does your research contribute to state-of-the-art methods?\nWalk me through one of your recent papers.\nHow would you apply your methods to X domain?\nWhat challenges remain in your area of research?\nHow do you evaluate safety, robustness, or uncertainty in your models?\nHow would you adapt your methods if labeled data was extremely limited?"
  },
  {
    "objectID": "blog/talks/Rsearch_sci.html#my-personal-advice",
    "href": "blog/talks/Rsearch_sci.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "My Personal Advice",
    "text": "My Personal Advice\n\nClarity beats complexity — explain ideas simply.\nBe enthusiastic about your work and its impact.\nConnect your strengths to the job’s mission.\nShow your ability to collaborate and iterate."
  },
  {
    "objectID": "blog/talks/Rsearch_sci.html#mentorship",
    "href": "blog/talks/Rsearch_sci.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’re preparing for Research Scientist interviews and would like advice or mentorship, feel free to reach out at cs.kundann@gmail.com."
  },
  {
    "objectID": "others/blog2.html",
    "href": "others/blog2.html",
    "title": "Blog and notes",
    "section": "",
    "text": "Blog\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nTechnical notes\nMost of the technical notes are in the newly built note repository, Data Apothecary’s Notes. Please feel free to reach out if you found any errors!\nI’d be glad if it helps you in some way.\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nReading notes\nThis section is constantly being updated.\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "others/Rsearch_sci.html",
    "href": "others/Rsearch_sci.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "others/Rsearch_sci.html#overview",
    "href": "others/Rsearch_sci.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "others/Rsearch_sci.html#key-interview-components",
    "href": "others/Rsearch_sci.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1️⃣ Research Portfolio Deep Dive\n\nBe able to explain your core research contributions in detail.\nClearly articulate: problem definition, novelty, methods, results, and real-world impact.\nPrepare multiple levels of technical depth (5-min, 15-min, 30-min versions).\nPractice connecting your work to broader research trends and applications.\n\n\n\n2️⃣ Technical Machine Learning Knowledge\n\nReinforcement Learning: algorithms, policy gradients, actor-critic, safe RL.\nDeep Learning: optimization, architecture design, generalization, transformers.\nProbabilistic Modeling: Bayesian inference, uncertainty estimation, graphical models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: LLM scaling laws, prompting, fine-tuning, RAG architectures.\nVision: object detection, segmentation, multi-modal perception.\n\n\n\n3️⃣ System Design / Applied ML Problems\n\nBe able to discuss:\n\nEnd-to-end ML pipelines\nData challenges (imbalance, noisy labels, drift)\nModel serving and deployment challenges\nScalability, latency, interpretability\n\n\n\n\n4️⃣ Coding and Algorithmic Skills\n\nLeetcode-style DSA for research interviews (moderate level)\nData manipulation (pandas, numpy, SQL)\nModel prototyping (PyTorch, TensorFlow, JAX)\n\n\n\n5️⃣ Behavioral and Collaboration Skills\n\n“Tell me about a time…” questions.\nCollaboration across teams.\nHandling ambiguous open-ended research problems.\nCommunication with product teams or non-research stakeholders."
  },
  {
    "objectID": "others/Rsearch_sci.html#recommended-preparation-resources",
    "href": "others/Rsearch_sci.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nPapers: Read papers from top-tier conferences (NeurIPS, ICML, ICLR, CVPR, ACL).\nCoding: Leetcode (medium), ML system design problems.\nSystem Design: Read “Designing Machine Learning Systems” by Chip Huyen.\nMock Interviews: Practice mock sessions with peers or mentors.\nPresentation: Prepare 1-2 strong 20-minute research talks."
  },
  {
    "objectID": "others/Rsearch_sci.html#example-interview-questions",
    "href": "others/Rsearch_sci.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nHow does your research contribute to state-of-the-art methods?\nWalk me through one of your recent papers.\nHow would you apply your methods to X domain?\nWhat challenges remain in your area of research?\nHow do you evaluate safety, robustness, or uncertainty in your models?\nHow would you adapt your methods if labeled data was extremely limited?"
  },
  {
    "objectID": "others/Rsearch_sci.html#my-personal-advice",
    "href": "others/Rsearch_sci.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "My Personal Advice",
    "text": "My Personal Advice\n\nClarity beats complexity — explain ideas simply.\nBe enthusiastic about your work and its impact.\nConnect your strengths to the job’s mission.\nShow your ability to collaborate and iterate."
  },
  {
    "objectID": "others/Rsearch_sci.html#mentorship",
    "href": "others/Rsearch_sci.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’re preparing for Research Scientist interviews and would like advice or mentorship, feel free to reach out at cs.kundann@gmail.com."
  },
  {
    "objectID": "blog/2023-06-13_wade_washi-wacse/index.html",
    "href": "blog/2023-06-13_wade_washi-wacse/index.html",
    "title": "Washington Soil Health Initative and Climate Smart Estimator",
    "section": "",
    "text": "Slides  Video \n\nDetails\n📆 June 13, 2023 // 1:30 pm - 2:20 pm PT\n🏨 Leavenworth, WA\n🌠 Washington Association of District Employees (WADE) conference\n\n\nAbstract\nWashington Soil Health Initiative overview and updates.\nHow to get the most of the Sustainable Farms and Fields Washington Climate Smart Estimator (WaCSE) tool.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Washington {Soil} {Health} {Initative} and {Climate} {Smart}\n    {Estimator}},\n  date = {2023-06-13},\n  url = {https://kundan-kumarr.github.io/blog/2023-06-13_wade_washi-wacse/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2023. “Washington\nSoil Health Initative and Climate Smart Estimator.” June 13,\n2023. https://kundan-kumarr.github.io/blog/2023-06-13_wade_washi-wacse/."
  },
  {
    "objectID": "blog/2023-09-25_posit_parameterized-quarto/index.html",
    "href": "blog/2023-09-25_posit_parameterized-quarto/index.html",
    "title": "Parameterized Quarto reports improve understanding of soil health",
    "section": "",
    "text": "Slides  Code  Video \n\nDetails\n📆 September 25, 2023 // 5:30 pm - 5:40 pm CDT 🏨 Chicago, IL\n🌠 posit::conf(2023)\n\n\nAbstract\nSoil sampling data are notoriously challenging to tidy and effectively communicate to farmers. We used functional programming with the tidyverse to reproducibly streamline data cleaning and summarization. To improve project outreach, we developed a Quarto project to dynamically create interactive HTML reports and printable PDFs. Custom to every farmer, reports include project goals, measured parameter descriptions, summary statistics, maps, tables, and graphs.\nOur case study presents a workflow for data preparation and parameterized reporting, with best practices for effective data visualization, interpretation, and accessibility.\nSee an example HTML report.\nLearn more about the Washington Soil Health Initiative State of the Soils Assessment.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and McIlquham, Molly and Sarpong, Kwabena and\n    Michel, Leslie and Potter, Teal and Griffin LaHue, Deirdre and\n    Gelardi, Dani},\n  title = {Parameterized {Quarto} Reports Improve Understanding of Soil\n    Health},\n  date = {2023-09-25},\n  url = {https://kundan-kumarr.github.io/blog/2023-09-25_posit_parameterized-quarto/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Molly McIlquham, Kwabena Sarpong, Leslie Michel, Teal\nPotter, Deirdre Griffin LaHue, and Dani Gelardi. 2023.\n“Parameterized Quarto Reports Improve Understanding of Soil\nHealth.” September 25, 2023. https://kundan-kumarr.github.io/blog/2023-09-25_posit_parameterized-quarto/."
  },
  {
    "objectID": "blog/2023-04-20_rladies_orcas-web-scraping/index.html",
    "href": "blog/2023-04-20_rladies_orcas-web-scraping/index.html",
    "title": "Web Scraping & Mapping {orcas} Encounters",
    "section": "",
    "text": "Slides  Code \n\nDetails\n📆 April 20, 2023 // 15-minute talk\n🏨 Seattle, WA\n🆓 R-Ladies Seattle and Seattle useR Group\n\n\nAbstract\nR-Ladies Seattle invited me to give a talk for the ‘R in the Outdoors’ meetup. This was my first in-person talk of my professional career! I used this as an opportunity to learn new skills through a personal project. I’ve always had an affinity for the Southern Resident Killer Whales in the Salish Sea. The Center for Whale Research does a lot of really fascinating and important work monitoring their population. They post their survey data on their website; each encounter with the orcas is a separate webpage. Lately, I’ve been curious and intimidated by web scraping so I decided this would make a great case study and personal project.\nI ended up also going to the Seattle useR Group lightning talks meetup afterwards and spontaneously gave the same presentation there!\n\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey},\n  title = {Web {Scraping} \\& {Mapping} \\{Orcas\\} {Encounters}},\n  date = {2023-04-20},\n  url = {https://kundan-kumarr.github.io/blog/2023-04-20_rladies_orcas-web-scraping/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2023. “Web Scraping & Mapping {Orcas}\nEncounters.” April 20, 2023. https://kundan-kumarr.github.io/blog/2023-04-20_rladies_orcas-web-scraping/."
  },
  {
    "objectID": "blog/2022-05-25_wagisa_arcgis-wacse/index.html",
    "href": "blog/2022-05-25_wagisa_arcgis-wacse/index.html",
    "title": "Washington Climate Smart Estimator: Using ArcGIS Dashboards and Experience Builder",
    "section": "",
    "text": "Slides  Recording \n\nDetails\n📆 May 25, 2022 // 2:30 pm - 3:00 pm PT\n🏨 Leavenworth, WA\n🌠 Washington GIS Association (WAGISA) conference\n\n\nAbstract\nWashington’s Sustainable Farms and Fields (SFF) program provides financial incentives to growers who implement climate-smart practices that sequester soil carbon or reduce greenhouse gas emissions. The climate change mitigation potential of different on-farm practices depends on many site-specific factors such as geography, climate, soil type, and management history.\nTo maximize the climate change mitigation potential of the SFF program, and to optimize the use of every dollar, a decision support tool is required. This presentation demonstrates how two existing spatial datasets (NRCS’ COMET-Planner and WSDA’s Agricultural Land Use) can be integrated into a decision support tool using ArcGIS Dashboards and Experience Builder. This tool, called the Washington Climate Smart Estimator (WaCSE), allows users to compare the climate benefits of different agricultural practices across different counties in Washington.\nBy utilizing the intuitive user interface of ArcGIS Dashboards, WaCSE enables swift, science-based estimates of climate benefits, while remaining accessible for audiences of varied technical backgrounds.\nSee the old app built with ArcGIS.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2022,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Washington {Climate} {Smart} {Estimator:} {Using} {ArcGIS}\n    {Dashboards} and {Experience} {Builder}},\n  date = {2022-05-25},\n  url = {https://kundan-kumarr.github.io/blog/2022-05-25_wagisa_arcgis-wacse/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2022. “Washington\nClimate Smart Estimator: Using ArcGIS Dashboards and Experience\nBuilder.” May 25, 2022. https://kundan-kumarr.github.io/blog/2022-05-25_wagisa_arcgis-wacse/."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#personal-advice",
    "href": "blog/technotes_20250703_research_guide/index.html#personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "Personal Advice",
    "text": "Personal Advice\n\nFocus on clarity — aim to teach, not just impress.\nKeep your message grounded in the real-world impact of your work.\nBe honest about challenges or failures — show how you learned from them.\nShow excitement for the work and curiosity about new ideas.\nPractice regularly and iterate based on feedback."
  },
  {
    "objectID": "blog/2021-11-09_awra_insecticides_water/index.html",
    "href": "blog/2021-11-09_awra_insecticides_water/index.html",
    "title": "Aquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters",
    "section": "",
    "text": "Slides\n\nDetails\n📆 November 9, 2021 // 1:30 pm - 1:50 pm PT\n🏨 Virtual\n🌠 American Water Resources Association (AWRA)\n\n\nAbstract\nEcological risk assessments often do not consider potential additive, synergistic, or antagonistic effects from mixtures of chemicals and instead typically base risk on a single chemical. In the last decade, more tools and models have been developed to consider the interactive effects of chemicals within a mixture when conducting risk assessments. Therefore, this study uses actual environmental concentrations measured in 2018 and 2019 from the Washington State Department of Agriculture’s Surface Water Monitoring Program. Aquatic risk from exposure was assessed from chlorpyrifos, diazinon, and malathion (as individual chemicals and as binary and ternary mixtures) using the concentration addition model. These pesticides were selected because they have a common mechanism of toxicity, are frequently detected in surface waters in Washington, and were recently evaluated in a biological opinion by the National Marine Fisheries Service.\nAll detected concentrations of chlorpyrifos and malathion, assessed as individual chemicals, exceeded the predicted no effect concentration, indicating potential for adverse effects on aquatic life. Further, risk quotients for all binary and ternary mixtures were greater than one, also indicative of potential for adverse effects on aquatic life. In all samples containing a mixture, the maximum cumulative ratio suggested that a single insecticide contributed &gt;50% of the overall toxicity of each mixture. Based on the individual and mixture risk quotients, chlorpyrifos and malathion were the primary drivers of the toxicity of each mixture.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2021,\n  author = {Ryan, Jadey},\n  title = {Aquatic {Risk} {Assessment:} {Organophosphate} Insecticide\n    Mixtures in {Washington} Surface Waters},\n  date = {2021-11-09},\n  url = {https://kundan-kumarr.github.io/blog/2021-11-09_awra_insecticides_water/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2021. “Aquatic Risk Assessment: Organophosphate\nInsecticide Mixtures in Washington Surface Waters.” November 9,\n2021. https://kundan-kumarr.github.io/blog/2021-11-09_awra_insecticides_water/."
  },
  {
    "objectID": "blog/2024-01-18_rladies-dc_quarto-params/index.html",
    "href": "blog/2024-01-18_rladies-dc_quarto-params/index.html",
    "title": "Parameterized Reports with Quarto: R-Ladies DC Workshop",
    "section": "",
    "text": "Course website  Slides  Code  Video \n\nDetails\n📆 January 18, 2024 // 6:30 pm - 8:30 pm EDT\n🏨 Virtual\n🆓 FREE with registration\n🏡 Workshop website\n🔖 Source tag\n\n\nAbstract\nTired of manually adjusting Quarto reports for different regions, time periods, or clients? Dreaming of using just one template to generate both interactive HTML and static Word/PDF versions of your reports?\nJoin our workshop to unlock the power of parameterized reporting with Quarto and leave with your own template and examples to modify for your own projects.\nGet a sneak preview of what you’ll learn by checking out the slides for my posit::conf(2023) talk.\nEveryone is welcome to attend. If you’re new to Quarto, we recommend watching Tom Mock’s excellent 2-hour introduction to Quarto.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2024,\n  author = {Ryan, Jadey},\n  title = {Parameterized {Reports} with {Quarto:} {R-Ladies} {DC}\n    {Workshop}},\n  date = {2024-01-18},\n  url = {https://kundan-kumarr.github.io/blog/2024-01-18_rladies-dc_quarto-params/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2024. “Parameterized Reports with Quarto: R-Ladies DC\nWorkshop.” January 18, 2024. https://kundan-kumarr.github.io/blog/2024-01-18_rladies-dc_quarto-params/."
  },
  {
    "objectID": "blog/2024-02-21_rladies-abuja-quarto-params/index.html",
    "href": "blog/2024-02-21_rladies-abuja-quarto-params/index.html",
    "title": "Parameterized Reports with Quarto: R-Ladies Abuja Workshop",
    "section": "",
    "text": "Course website  Slides  Code  Video \n\nDetails\n📆 February 21, 2024 // 4:30 pm - 6:30 pm WAT\n🏨 Virtual\n🆓 FREE with registration\n🎥 Recording\n🏡 Workshop website\n🔖 Code\n\n\nAbstract\nTired of manually adjusting Quarto reports for different regions, time periods, or clients? Dreaming of using just one template to generate both interactive HTML and static Word/PDF versions of your reports?\nJoin our workshop to unlock the power of parameterized reporting with Quarto and leave with your own template and examples to modify for your own projects.\nGet a sneak preview of what you’ll learn by checking out the slides from my posit::conf(2023) talk.\nWe welcome everyone! However, if you’re new to Quarto or functional programming with {purrr}, take a look at the pre-work for some background videos/tutorials.\n\n\nSlides\n\n\n\n\nRecording"
  },
  {
    "objectID": "blog/2021-11-14_setac_insecticides_water/index.html",
    "href": "blog/2021-11-14_setac_insecticides_water/index.html",
    "title": "Aquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters",
    "section": "",
    "text": "Slides\n\nDetails\n📆 November 14, 2021 // 12-minute recording\n🏨 Virtual\n🌠 Society of Environmental Toxicology and Chemistry (SETAC) North America 42nd Annual Meeting\n\n\nAbstract\nEcological risk assessments often do not consider potential additive, synergistic, or antagonistic effects from mixtures of chemicals and instead typically base risk on a single chemical. In the last decade, more tools and models have been developed to consider the interactive effects of chemicals within a mixture when conducting risk assessments. Therefore, this study uses actual environmental concentrations measured in 2018 and 2019 from the Washington State Department of Agriculture’s Surface Water Monitoring Program. Aquatic risk from exposure was assessed from chlorpyrifos, diazinon, and malathion (as individual chemicals and as binary and ternary mixtures) using the concentration addition model. These pesticides were selected because they have a common mechanism of toxicity, are frequently detected in surface waters in Washington, and were recently evaluated in a biological opinion by the National Marine Fisheries Service.\nAll detected concentrations of chlorpyrifos and malathion, assessed as individual chemicals, exceeded the predicted no effect concentration, indicating potential for adverse effects on aquatic life. Further, risk quotients for all binary and ternary mixtures were greater than one, also indicative of potential for adverse effects on aquatic life. In all samples containing a mixture, the maximum cumulative ratio suggested that a single insecticide contributed &gt;50% of the overall toxicity of each mixture. Based on the individual and mixture risk quotients, chlorpyrifos and malathion were the primary drivers of the toxicity of each mixture.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2021,\n  author = {Ryan, Jadey},\n  title = {Aquatic {Risk} {Assessment:} {Organophosphate} Insecticide\n    Mixtures in {Washington} Surface Waters},\n  date = {2021-11-14},\n  url = {https://kundan-kumarr.github.io/blog/2021-11-14_setac_insecticides_water/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2021. “Aquatic Risk Assessment: Organophosphate\nInsecticide Mixtures in Washington Surface Waters.” November 14,\n2021. https://kundan-kumarr.github.io/blog/2021-11-14_setac_insecticides_water/."
  },
  {
    "objectID": "blog/2023-08-19_cascadia_shiny-wacse/index.html",
    "href": "blog/2023-08-19_cascadia_shiny-wacse/index.html",
    "title": "Shiny optimization of climate benefits from a statewide agricultural grant program",
    "section": "",
    "text": "Slides  Code  Video \n\nDetails\n📆 August 19, 2023 // 2:05 pm - 2:20 pm PT\n🏨 Seattle, WA\n🌠 Cascadia R Conf\n\n\nAbstract\nWashington’s Sustainable Farms and Fields program provides grants to growers to increase soil carbon or reduce greenhouse gas (GHG) emissions on their farms. To optimize the climate benefits of the program, we developed the Washington Climate Smart Estimator {WaCSE} using R and Shiny.\nIntegrating national climate models and datasets, this intuitive, regionally specific user interface allows farmers and policymakers to compare the climate benefits of different agricultural practices across Washington’s diverse counties and farm sizes. Users can explore GHG estimates in interactive tables and plots, download results in spreadsheets and figures, and generate PDF reports. In this talk, we present the development process of {WaCSE} and discuss the lessons we learned from creating our first ever Shiny app.\n\n\nSlides\n\n\n\n\nRecording"
  },
  {
    "objectID": "about_me.html#projects",
    "href": "about_me.html#projects",
    "title": "Kundan Kumar",
    "section": " Projects",
    "text": "Projects\n\n\n   Fast Mixed‑Logit Estimation Fast estimation of mixed logit models with preference‑space utility.  \n   cbcTools Suite Designing choice‑based conjoint experiments and power/system analysis.  \n   LLM‑Powered Energy Optimizer Multi‑building energy optimization in CityLearn with LLM guidance.  \n\n\n 📁 Check My Projects \n\n\n\nExplore My Work\n\n\nBlog\n\n\n\n\n\n\n\n\nResearch Scientist Interview Guide\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\nMy Dummy Talk\n\n\nLearning How to Show a Card\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nPublications\n\n\n\n\n\n\n\n\n\n\nMy Dummy Talk\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\nDRL\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\n\n\nNews Highlights\n\n\n\n\n\n\n[Jul 2025]\n\n\nSelected for the Cohere Machine Learning Summer School, hosted by Cohere Labs.\n\n\n\n\n[Mar 2025]\n\n\nOur paper on Advanced Semi-Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems has been accepted to IEEE PES General Meeting 2025.\n\n\n\n\n[Jan 2025]\n\n\nOur paper on Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids has been accepted to IEEE PES Grid Edge Technologies Conference & Exposition 2025."
  },
  {
    "objectID": "others/CopyOfabout_me.html",
    "href": "others/CopyOfabout_me.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar\n  \n\n  \n  \nHi! I’m Kundan Kumar, a Ph.D. candidate and researcher focused on building intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work bridges deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), safe and explainable AI, and computer vision, with real-world applications in smart grids, autonomous vehicles, and critical infrastructure.\nMy Ph.D. research centers on physics-informed and safety-critical DRL frameworks that embed domain knowledge, safety constraints, and uncertainty into the learning process—enabling agents to make robust and interpretable decisions in dynamic, complex environments. My research within DRL focuses on techniques such as transfer learning, uncertainty quantification, and adversarial resilience to improve generalization, safety, and reliability across diverse tasks and environments.\nI also develop LLM-integrated simulation frameworks for robotics and autonomous systems, combining vision-based perception, trajectory planning, and natural language reasoning to support high-level control and human-AI collaboration.\nBeyond research, I enjoy sharing my insights through educational content on Substack and YouTube. Outside of work, I love cooking and Ice skating 🛼.\n\n\n\n\n\nOther Research Interests\n\n\n\n\n  \n    \n      Computer Vision\n      Visual perception, object detection, semantic segmentation, and sensor fusion for autonomous systems.\n    \n  \n\n  \n    \n      Statistical ML\n      Uncertainty quantification, probabilistic modeling, and data-driven inference in dynamic environments.\n    \n  \n\n  \n    \n      Self-Driving Systems\n      Learning-based control, trajectory planning, vision-based perception, and sensor fusion in autonomous driving environments.\n    \n  \n\n  \n    \n      LLM Reasoning\n      Multi-modal reasoning and control in autonomous environments using Large Language Models.\n    \n  \n\n\n\n\n\nExplore My Work\n\n\nBlog\n\n\n\n\n\n\n\n\nResearch Scientist Interview Guide\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\nMy Dummy Talk\n\n\nLearning How to Show a Card\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nPublications\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\nDRL\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\n\n\nNews Highlights\n\n\n\n\n\n\n[Jul 2025]\n\n\nSelected for the Cohere Machine Learning Summer School, hosted by Cohere Labs.\n\n\n\n\n[Mar 2025]\n\n\nOur paper on Advanced Semi-Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems has been accepted to IEEE PES General Meeting 2025.\n\n\n\n\n[Jan 2025]\n\n\nOur paper on Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids has been accepted to IEEE PES Grid Edge Technologies Conference & Exposition 2025."
  },
  {
    "objectID": "others/main.html",
    "href": "others/main.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar\n  \n\n  \n  \nHi! I’m Kundan Kumar, a Ph.D. candidate and researcher focused on building intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work bridges deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), safe and explainable AI, and computer vision, with real-world applications in smart grids, autonomous vehicles, and critical infrastructure.\nMy Ph.D. research centers on physics-informed and safety-critical DRL frameworks that embed domain knowledge, safety constraints, and uncertainty into the learning process—enabling agents to make robust and interpretable decisions in dynamic, complex environments. My research within DRL focuses on techniques such as transfer learning, uncertainty quantification, and adversarial resilience to improve generalization, safety, and reliability across diverse tasks and environments.\nI also develop LLM-integrated simulation frameworks for robotics and autonomous systems, combining vision-based perception, trajectory planning, and natural language reasoning to support high-level control and human-AI collaboration.\nBeyond research, I enjoy sharing my insights through educational content on Substack and YouTube. Outside of work, I love cooking and Ice skating 🛼.\n\n\n\n\n\nOther Research Interests\n\n\n\n\nComputer Vision\n\n\nVisual perception, object detection, semantic segmentation, and sensor fusion for autonomous systems.\n\n\n\n\n\nStatistical ML\n\n\nUncertainty quantification, probabilistic modeling, and data-driven inference in dynamic environments.\n\n\n\n\n\nSelf-Driving Systems\n\n\nLearning-based control, trajectory planning, vision-based perception, and sensor fusion in autonomous driving environments.\n\n\n\n\n\n\nExplore My Work\n\n\nBlogs\n\n\n\n\n\n\n\n\nResearch Scientist Interview Guide\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\nMy Dummy Talk\n\n\nLearning How to Show a Card\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nPublications\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\nDRL\n\n\n\n\n\n\n\nNo matching items\n\n\nSee all →\n\n\n\n\nNews Highlights\n\n\n\n\n\n\n[Jul 2025]\n\n\nSelected for the Cohere Machine Learning Summer School, hosted by Cohere Labs.\n\n\n\n\n[Mar 2025]\n\n\nOur paper on Advanced Semi-Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems has been accepted to IEEE PES General Meeting 2025.\n\n\n\n\n[Jan 2025]\n\n\nOur paper on Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids has been accepted to IEEE PES Grid Edge Technologies Conference & Exposition 2025."
  },
  {
    "objectID": "rpkg/rpkg1.html#research-vision",
    "href": "rpkg/rpkg1.html#research-vision",
    "title": "Research",
    "section": "",
    "text": "I aim to develop safe, interpretable, and adaptive AI systems for real-world cyber-physical environments that operate under uncertainty, constraints, and adversarial conditions. My research bridges the domains of machine learning, optimization, and control theory, with a strong emphasis on safety, robustness, and generalization.\nMy work centers around the following pillars:\n\nSafe & Trustworthy Reinforcement Learning: Designing agents that are robust to adversarial attacks, resilient to distributional shifts, and capable of safe exploration.\nPhysics-informed Deep Reinforcement Learning (DRL): Embedding physical laws and constraints into learning frameworks for stability, interpretability, and faster convergence.\nProbabilistic & Bayesian Modeling: Probabilistic & Bayesian Modeling: Capturing both epistemic and aleatoric uncertainties for reliable control in high-stakes, partially observable systems.\nLarge Language Models (LLMs) for autonomous reasoning: Leveraging large language models (LLMs) to enhance planning, explainability, and human-AI collaboration in control systems.\nVision-based simulation environments: Using platforms like CARLA and CityLearn to train agents in multimodal, visually rich, and interactive worlds.\n\nBy tightly integrating domain knowledge into learning frameworks, I aim to enable resilient, generalizable, and safe AI for critical applications including smart grids, autonomous systems, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg1.html#research-focus",
    "href": "rpkg/rpkg1.html#research-focus",
    "title": "Research",
    "section": "My Research Focus Areas",
    "text": "My Research Focus Areas\n\n\n\n\n\nDRL-based Control\n\n\n   DRL for Volt-VAR Design control agents for voltage regulation and reactive power optimization in smart distribution grids.  \n   Physics-Informed Actor-Critic Embed grid physics and control limits directly into the DRL learning loop for stable and efficient decisions.  \n   Sim-to-Real Transfer Train agents in simulated OpenDSS environments and deploy them on real-time OPAL-RT setups.  \n\n\n\n\nSafe & Trustworthy RL\n\n\n   Robust & Stable Learning Develop agents that ensure system safety, robustness, and interpretability under uncertainty.  \n   Uncertainty-Aware Policies Quantify epistemic and aleatoric uncertainty in high-stakes, partially observable settings.  \n   Physics-Informed DRL Incorporate physical constraints into DRL agents to ensure safe and interpretable control.  \n\n\n\nTransfer & Meta-Adaptation\n\n\n   Domain Adaptation Enable agents to generalize across grids with different topologies, dynamics, and loads.  \n   Meta-RL for Efficiency Leverage meta-reasoning to accelerate learning in low-data, high-variance scenarios.  \n\n\n\nVision-Simulation Integration\n\n\n   Perception-Control Fusion Use CARLA and AirSim to train end-to-end systems in visual RL tasks with sensors.  \n   Multi-modal Representations Combine visual, state, and contextual features for better decision-making.  \n\n\n\nLLM-Augmented Decision Systems\n\n\n\n\n   LLM-Guided Control Translate natural language into actionable policies for real-world environments."
  },
  {
    "objectID": "rpkg/rpkg1.html#application-domains",
    "href": "rpkg/rpkg1.html#application-domains",
    "title": "Research",
    "section": "Application Domains",
    "text": "Application Domains\n\n\n\n\n\n\n\nDomain\nDescription\n\n\n\n\n⚡ Smart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\n🚘 Autonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world environments\n\n\n🛡 Secure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/rpkg1.html#publications",
    "href": "rpkg/rpkg1.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\nJournal PapersConference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nAdvanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems\nIEEE PES General Meeting, 2025\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \n\n\n\n\nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n\n\n Show More"
  },
  {
    "objectID": "rpkg/rpkg1.html#ongoing-projects",
    "href": "rpkg/rpkg1.html#ongoing-projects",
    "title": "Research",
    "section": "Ongoing Projects",
    "text": "Ongoing Projects\n\nFederated DRL for Cyber-Resilient Volt-VAR Optimization\nDecentralized, communication-efficient control using LSTM-enhanced PPO agents across distributed DERs.\nOne-Shot Policy Transfer with Physics Priors\nTrain agents on small topologies and adapt to IEEE 123-bus, 8500-node networks in a few iterations.\nLLM-Guided Autonomous Planning for Smart Buildings\nConvert user prompts to interpretable control policies using LLMs (OpenAI, Claude) in CityLearn environments."
  },
  {
    "objectID": "rpkg/research1.html#research-focus",
    "href": "rpkg/research1.html#research-focus",
    "title": "Research",
    "section": "My Research Focus Areas",
    "text": "My Research Focus Areas\n\n\n\n\nDRL-based Control\n\n\n\n\n\nDRL for Volt-VAR\n\n\nDesign control agents for voltage regulation and reactive power optimization in smart distribution grids.\n\n\n\n\n\nPhysics-Informed Actor-Critic\n\n\nEmbed grid physics and control limits directly into the DRL learning loop for stable and efficient decisions.\n\n\n\n\n\nSim-to-Real Transfer\n\n\nTrain agents in simulated OpenDSS environments and deploy them on real-time OPAL-RT setups.\n\n\n\n\n\n\nSafe & Trustworthy RL\n\n\n\n\n\nRobust & Stable Learning\n\n\nDevelop agents that ensure system safety, robustness, and interpretability under uncertainty.\n\n\n\n\n\nUncertainty-Aware Policies\n\n\nQuantify epistemic and aleatoric uncertainty in high-stakes, partially observable settings.\n\n\n\n\n\nPhysics-Informed DRL\n\n\nIncorporate physical constraints into DRL agents to ensure safe and interpretable control.\n\n\n\n\n\nTransfer & Meta-Adaptation\n\n\n\n\n\nDomain Adaptation\n\n\nEnable agents to generalize across grids with different topologies, dynamics, and loads.\n\n\n\n\n\nMeta-RL for Efficiency\n\n\nLeverage meta-reasoning to accelerate learning in low-data, high-variance scenarios.\n\n\n\n\n\nVision-Simulation Integration\n\n\n\n\n\nPerception-Control Fusion\n\n\nUse CARLA and AirSim to train end-to-end systems in visual RL tasks with sensors.\n\n\n\n\n\nMulti-modal Representations\n\n\nCombine visual, state, and contextual features for better decision-making.\n\n\n\n\n\nLLM-Augmented Decision Systems\n\n\n\n\n\nLLM-Guided Control\n\n\nTranslate natural language into actionable policies for real-world environments."
  },
  {
    "objectID": "rpkg/research1.html#application-domains",
    "href": "rpkg/research1.html#application-domains",
    "title": "Research",
    "section": "Application Domains",
    "text": "Application Domains\n\n\n\n\n\n\n\nDomain\nDescription\n\n\n\n\n⚡ Smart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\n🚘 Autonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world environments\n\n\n🛡 Secure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/research1.html#ongoing-projects",
    "href": "rpkg/research1.html#ongoing-projects",
    "title": "Research",
    "section": "Ongoing Projects",
    "text": "Ongoing Projects\n\nFederated DRL for Cyber-Resilient Volt-VAR Optimization\nDecentralized, communication-efficient control using LSTM-enhanced PPO agents across distributed DERs.\nOne-Shot Policy Transfer with Physics Priors\nTrain agents on small topologies and adapt to IEEE 123-bus, 8500-node networks in a few iterations.\nLLM-Guided Autonomous Planning for Smart Buildings\nConvert user prompts to interpretable control policies using LLMs (OpenAI, Claude) in CityLearn environments."
  },
  {
    "objectID": "projects/robo/washi.html",
    "href": "projects/robo/washi.html",
    "title": "Survival oF Ventilated and Control Flies",
    "section": "",
    "text": "{pkgdown} site  Code  CRAN \nInspired by other branding R packages such as Code, Code, and Code, washi provides color palettes and themes consistent with Washington Soil Health Initiative (WaSHI) branding. This package is to be used only by direct collaborators within WaSHI, though you are welcome to adapt the package to suit your own organization’s branding."
  },
  {
    "objectID": "others/cv1.html",
    "href": "others/cv1.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "📄 Resume"
  },
  {
    "objectID": "others/cv1.html#professional-experience",
    "href": "others/cv1.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": " Professional Experience",
    "text": "Professional Experience\n\n\n\n\n\nNational Renewable Energy Laboratory (NREL)\n\n\n\n\n\nMachine Learning Engineer (Intern)\n\n\n May 2024  —  Jan 2025 \n\n\n\n\nDeveloped novel machine learning models for automated network topology inference and resilient control policy optimization for complex distributed systems under extreme scenarios\nDesigned and developed semi‑supervised learning approaches to tackle the challenge of limited labeled data in networks, achieving 98% improvement in model accuracy with varying labeled data.\nPaper ”Advanced Semi‑Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems” accepted at IEEE Power & Energy Society General Meeting (PES GM) 2025.\n\n\n\n\n\n\n\nComcast\n\n\n\n\n\nSoftware Engineer\n\n\n Jul 2019  —  Feb 2020 \n\n\n\n\nDesigned and implemented real‑time data processing pipelines using Amazon Kinesis and RabbitMQ, processing 1TB+ daily data for fraud detection and system monitoring.\nDeveloped machine learning models for anomaly detection and user behavior analysis, reducing fraudulent activities by 70% through predictive analytics.\nBuilt scalable Spring Boot microservices handling 10K+ concurrent requests, achieving 99.9% uptime for critical system components.\nCreated interactive dashboards using Presto DB and Python visualization tools, enabling real‑time monitoring of network performance metrics and fraud patterns.\n\n\n\n\n\n\n\nIBM\n\n\n\n\n\nSoftware Engineer\n\n\n Jan 2019  —  Jun 2019 \n\n\n\n\nLed cloud infrastructure optimization using OpenShift, implementing auto‑scaling solutions that reduced operational costs by 30%.\nDeveloped a comprehensive monitoring system using Grafana and Flask, providing real‑time visibility into 100+ cloud servers.\nImplemented automated performance monitoring and alerting system, reducing incident response time by 60%.\n\n\n\n\n\n\n\nHewlett Packard Enterprise (HPE)\n\n\n\n\n\nSoftware Engineer\n\n\n Apr 2017  —  Dec 2018 \n\n\n\n\n\nSpearheaded migration of critical applications from HPI to HPE domain, ensuring zero downtime during transition.\nImplemented OAuth 2.0 authentication system and RESTful services using Spring Boot, securing applications serving 50K+ users.\nDesigned and deployed microservices architecture on Apache/WebLogic servers, improving system response time by 40%.\n\n\n\n\n\n\n\nTata Consultancy Services (TCS)\n\n\n\n\n\nSystem Engineer\n\n\n Jul 2012  —  Dec 2015 \n\n\n\n\n\nEngineered high‑performance ETL pipelines for data warehouse integration, processing 100GB+ daily data volumes.\nOptimized database performance through advanced SQL tuning and indexing strategies, reducing query execution time by 70%.\nReceived excellence award for achieving $100K cost savings through database optimization initiatives."
  },
  {
    "objectID": "others/cv1.html#education",
    "href": "others/cv1.html#education",
    "title": "Curriculum Vitae",
    "section": " Education",
    "text": "Education\n\n\n\n\n\nIowa State University\n\n\n\n\n\nPh.D. in Computer Science (Minor: Statistics)\n\n\n 2020  —  2025 (Expected) \n\n\n\n\n\nResearch: Deep RL, Physics-Informed AI, Uncertainty Quantification\nCourses: Deep Learning, NLP, Statistical Theory, Empirical Methods, Algorithms\n\n\n\n\nMS in Computer Science\n\n\n Jan 2015  —  Dec 2016 \n\n\n\n\nFocus: Algorithms, Databases, Network Programming"
  },
  {
    "objectID": "others/cv1.html#teaching-experience",
    "href": "others/cv1.html#teaching-experience",
    "title": "Curriculum Vitae",
    "section": " Teaching Experience",
    "text": "Teaching Experience\n\n\n\n\n\nIowa State University\n\n\n\n\n\nTeaching Assistant\n\n\n 2020  —  2025 \n\n\n\nDepartment of Computer Science\n\nSupported undergraduate/graduate courses including Software Development Practices, Database Systems, and Spreadsheets.\nLed weekly lab sessions, assisted students with debugging and conceptual challenges, and held office hours.\nDesigned assignments and quizzes aligned with real-world workflows and agile development practices.\nMentored students on semester-long capstone projects simulating software engineering team experiences."
  },
  {
    "objectID": "others/cv1.html#research-experience",
    "href": "others/cv1.html#research-experience",
    "title": "Curriculum Vitae",
    "section": " Research Experience",
    "text": "Research Experience\n\n\n\n\n\nIowa State University\n\n\n\n\n\nResearch Assistant\n\n\n Aug 2022  —  Jul 2025 \n\n\n\n\nResearch on Physics‑Informed Deep Reinforcement Learning for Critical Infrastructure Systems, focusing on Intelligent Resource Management and Security in Large‑Scale Distributed Networks.\nApplied computational deep reinforcement learning algorithms in a Smart Energy System to analyze power simulation data, minimizing voltage violations, power loss, and control errors.\nDeveloped physics‑informed DRL algorithms incorporating domain‑specific physical constraints, achieving 30% improvement in resource allocation efficiency and reducing system violations in complex distributed networks.\nDesigned and implemented adversarial attack detection and mitigation frameworks for AI models in critical systems, enhancing robustness against security threats through systematic testing and defensive techniques.\nCreated novel transfer learning methodologies enabling DRL models to adapt across varying network sizes and topologies, reducing training time by 40% for new configurations.\nDeveloped Python‑based simulation and control framework integrating real‑time hardware (OPAL‑RT and OpenDSS) with distributed systems.\nLeveraged LLM‑driven reasoning and contextual understanding within simulation environments to support real‑time adaptive control, human‑AI collaboration, and predictive system optimization.\n\n\n\n\nResearch Assistant\n\n\n Aug 2020  —  Jul 2022 \n\n\n\n\nResearch on Deep Reinforcement Learning (DRL) and Safety‑Critical Learning for Autonomous Systems, with focus on perception, control, and decision‑making in high‑stakes environments.\nUtilized CARLA simulator for vision‑based autonomous driving tasks, including perception, object detection, trajectory planning, and policy learning in complex traffic scenarios.\nApplied deep computer vision models for object recognition, semantic segmentation, and sensor fusion, enabling robust situational awareness in autonomous driving and robotics."
  },
  {
    "objectID": "others/cv1.html#skills",
    "href": "others/cv1.html#skills",
    "title": "Curriculum Vitae",
    "section": " Skills",
    "text": "Skills\n\n\n\nProgramming Languages\n\n\nPython, R, Java, C++, SAS, MATLAB, SQL, HTML, JS, Node.js, React.js\n\n\n\n\nML & Data Analysis\n\n\nscikit-learn, TensorFlow, PyTorch, Pandas, Matplotlib, Seaborn, Gym, RLlib\n\n\n\n\nLLMs & NLP\n\n\nHugging Face Transformers, LangChain, RAG, Prompt Engineering\n\n\n\n\nHPC & Big Data\n\n\nHadoop, Hive, Spark, Kafka, Kinesis, SLURM, MPI, OpenMP\n\n\n\n\nSimulation & Modeling\n\n\nOpal-RT, OpenDSS (Power), Carla (Autonomous Driving)\n\n\n\n\nOptimization\n\n\nGurobi, Pyomo, BoTorch, Optuna, Hyperopt\n\n\n\n\nVisualization & GIS\n\n\nTableau, ArcGIS, Leaflet\n\n\n\n\nCloud & DevOps\n\n\nAWS (EC2, S3, Lambda), GCP, Docker, Kubernetes, Git, Terraform, Jenkins, CircleCI"
  },
  {
    "objectID": "others/cv1.html#honors-awards",
    "href": "others/cv1.html#honors-awards",
    "title": "Curriculum Vitae",
    "section": " Honors & Awards",
    "text": "Honors & Awards\n\nSelected, Seventh Workshop on Autonomous Energy Systems @ NREL (2024)\nSelected, ByteBoost Workshop on Accelerating HPC Research Skills (2024)\nSelected, Oxford Machine Learning Summer School (OxML) (2022)\nExcellence Award, Database Optimization @ TCS\n2nd Place, BAJA SAE India (Safest Terrain Vehicle Category, National Level)"
  },
  {
    "objectID": "others/cv1.html#service",
    "href": "others/cv1.html#service",
    "title": "Curriculum Vitae",
    "section": " Service",
    "text": "Service\n\nReviewer:\n\nIEEE Transactions on Industrial Informatics (2025)\nConference on Neural Information Processing Systems (Ethics)(2025)\nIEEE Transactions on Neural Networks and Learning Systems (2024)\nIEEE PES GM, Grid Edge & ISGT (2023, 2024)\n\nMock Interviewer: Supporting underrepresented minorities in tech.\nVolunteer, Prayaas India (BIT): NGO providing quality education to underprivileged children in slums and villages."
  },
  {
    "objectID": "others/cv1.html#projects",
    "href": "others/cv1.html#projects",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n\n\n\n\n   Fast Mixed‑Logit Estimation Fast estimation of mixed logit models with preference‑space utility.  \n   cbcTools Suite Designing choice‑based conjoint experiments and power and system analyses.  \n   LLM‑Powered Energy Optimizer Multi‑building energy optimization in CityLearn with LLM guidance.  \n\n\n 📁 Check My Projects"
  },
  {
    "objectID": "others/posts.html",
    "href": "others/posts.html",
    "title": "What’s New & Updated",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "others/projects1.html",
    "href": "others/projects1.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "llms/projects1.html",
    "href": "llms/projects1.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "llms/stat.html",
    "href": "llms/stat.html",
    "title": "Statistics",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "llms/sykdomspulsen/index.html",
    "href": "llms/sykdomspulsen/index.html",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to Code. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nPaper"
  },
  {
    "objectID": "llms/sykdomspulsen/index.html#overview",
    "href": "llms/sykdomspulsen/index.html#overview",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to Code. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nPaper"
  },
  {
    "objectID": "llms/projects.html",
    "href": "llms/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "llms/dan/index.html",
    "href": "llms/dan/index.html",
    "title": "Data Apothecary’s Notes",
    "section": "",
    "text": "(This is my own note-taking system using quarto)\n\nAbout the notes\nData Apothecary’s Notes is a note-taking repository for modern data science skills with a focus on drug development and clinical trials. Content will be gradually added while I learn the topics. Therefore, it is by no means a complete guide by the time you read it!\nI try to organize the content in a modular way. I think these should cover the important aspects in which a data scientist / modern statistician should know.\n\nstudy design\ninference\nmodels\nreporting\nprogramming\n\n\n\nWhy quarto\nIn short, quarto has the advantage of making a very well structured website with code chunks easy. No more worry about scattered notes in different folders - put them together, render it so you can find your notes quickly!"
  },
  {
    "objectID": "llms/dl.html",
    "href": "llms/dl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "llms/nor_mortality/index.html",
    "href": "llms/nor_mortality/index.html",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "llms/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "llms/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "llms/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "llms/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Mortality Surveillance in Norway",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "llms/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "llms/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Mortality Surveillance in Norway",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "llms/drl.html",
    "href": "llms/drl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "llms/prohect2.html",
    "href": "llms/prohect2.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "llms/os_teaching/index.html",
    "href": "llms/os_teaching/index.html",
    "title": "Teach in R and Quarto",
    "section": "",
    "text": "MF9130E - Introductory course in statistics\n8-day intensive course on introductory statistics. April 2023 we made it with R rather than propriety software, coupled with live-coding sessions to enhance understanding of basic concepts such as distribution and hypothesis tests.\nCode\nCourse website\nRead more about the experience in\n\nblogpost\npresentation"
  },
  {
    "objectID": "llms/robo.html",
    "href": "llms/robo.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "llms/ehr-title/index.html",
    "href": "llms/ehr-title/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr"
  },
  {
    "objectID": "llms/noreden/index.html",
    "href": "llms/noreden/index.html",
    "title": "Noreden",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "llms/phuse/index.html",
    "href": "llms/phuse/index.html",
    "title": "PHUSE - CAMIS",
    "section": "",
    "text": "I am contributing to two working groups at PHUSE: CAMIS, and RWD - Real World Data Guideline (early stage).\n\nCAMIS: Comparing Analysis Method Implementations in Software\nCAMIS is a cross-industry group formed of members from PHUSE, PSI and ASA.\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comparison and comprehensive explanations.\n\n\nRWD Working Group\nThis is a newly formed working group, working on statistical programming guidelines while working on RWD (read world data)."
  },
  {
    "objectID": "llms/cv.html",
    "href": "llms/cv.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "llms/index.html",
    "href": "llms/index.html",
    "title": "Large Language Models",
    "section": "",
    "text": "Deep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nThis porfolio includes openly available educational material I have created, along with works, software, and tools I have contributed to or designed.\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nThis porfolio includes openly available educational material I have created, along with works, software, and tools I have contributed to or designed.\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "llms/index.html#upcoming",
    "href": "llms/index.html#upcoming",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "llms/index.html#selected-previous-talks",
    "href": "llms/index.html#selected-previous-talks",
    "title": "Talks",
    "section": "Selected previous talks",
    "text": "Selected previous talks\n\n\n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\nDate\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drl.html",
    "href": "drl.html",
    "title": "DRL",
    "section": "",
    "text": "Doing real-world projects is, I think, the best way to learn and also to engage the world and find out what the world is all about.\n\n-Ray Kurzweil\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaboration and Competition\n\n\nTrain a pair of agents to play tennis.\n\n\n\n\n\n\n\n\n\n\n\n\nAI-Powered Patient Education System\n\n\nDevelop an AI agent to enhance patient education by delivering personalized, on-demand health information through summaries, comprehension checks, and quizzes about relevant…\n\n\n\n\n\n\n\n\n\n\n\n\nChatbot Design using Retrieval‑Augmented Generation (RAG)\n\n\nBuilt a domain‑specific chatbot integrating vector‑based retrieval with GPT models to provide accurate, context‑aware responses\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Agent Travel Assistant System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\n\n\n\n\n\nCongressional Policy Analysis using ML and HPCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt Injection Attacks on LLM Medical Diagnosis by Symptom Elaboration\n\n\n\nR package\n\n\n\nAttacker is to trick the LLM to generate inappropriate possible medical diagnosis which could mislead the end use\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Of Ventilated and Control Flies\n\n\n\nR package\n\n\n\nHypothesis Analysis of Life Expectancy of Flies in Normal vs ill Ventilated Bottles\n\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification of Foods Based on their Quality\n\n\n\nArcGIS\n\nPython\n\n\n\nML model to assess the quality of fruit from an data set, which could be integrated into a product for use in home kitchens\n\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nP2P File Sharing Protocol\n\n\n\nShiny app\n\n\n\nBuild a peer-to-peer file sharing protocol that keeps track of which peers are sharing and what files are being shared in the network\n\n\n\nJan 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotion Prediction and Detection for Autonomous Vehicles\n\n\n\nAutonomous Systems\n\nDeep Learning\n\nComputer Vision\n\n\n\nDevelop a framework for vehicle detection and motion planning of vehicles in complex driving scenarios\n\n\n\nDec 7, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/llm_noreden/index.html",
    "href": "projects/llm_noreden/index.html",
    "title": "Chatbot Design using Retrieval‑Augmented Generation (RAG)",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "projects/llm_nor_mortality/index.html",
    "href": "projects/llm_nor_mortality/index.html",
    "title": "Multi-Agent Travel Assistant System",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects/llm_nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "projects/llm_nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Multi-Agent Travel Assistant System",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects/llm_nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "projects/llm_nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Multi-Agent Travel Assistant System",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "projects/llm_nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "projects/llm_nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Multi-Agent Travel Assistant System",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "publications.html#journal-articles",
    "href": "publications.html#journal-articles",
    "title": "Publications",
    "section": "",
    "text": "Kumar, Kundan, Ravikumar Gelli, and Christopher Quinn. 2025.\n“Physics-Informed DRL for Smart Grids.”  IEEE Transactions on Smart Grid.  Paper Code Poster"
  },
  {
    "objectID": "publications.html#conference-papers",
    "href": "publications.html#conference-papers",
    "title": "Publications",
    "section": "Conference Papers",
    "text": "Conference Papers\n\n\n\nKumar, Kundan, and Christopher Quinn. 2024.\n“Federated DRL for Resilient Energy Systems.”  In Proceedings of ACM BuildSys.  Paper Code"
  },
  {
    "objectID": "publications.html#workshop-papers",
    "href": "publications.html#workshop-papers",
    "title": "Publications",
    "section": "Workshop Papers",
    "text": "Workshop Papers\n\n\n\nKumar, Kundan, and Nabila Masud. 2024.\n“One-Shot DRL for Low-Data Grid Settings.”  In NeurIPS Workshop on Energy Systems.  Paper Poster"
  },
  {
    "objectID": "publications.html#journal-articles-1",
    "href": "publications.html#journal-articles-1",
    "title": "Publications",
    "section": "📘 Journal Articles",
    "text": "📘 Journal Articles\n\n\n\nKumar, Gelli, and Quinn (2025)"
  },
  {
    "objectID": "publications1.html",
    "href": "publications1.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Home\n    Publications\n  \n\n\nPublications\nThe following is a list of my research publications, including journal articles, conference papers, Workshops papers and preprints.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n      \n        Publication\n      \n      \n        Year\n      \n    \n  \n    \n      \n      \n    \n\n\n\n  \n    A Multi-Objective Optimization Framework for Carbon-Aware Smart Energy Management\n    Kundan Kumar\n    57th North American Power Symposium\n    (2025)\n    \n\n    Details\n\n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems\n    Kundan Kumar\n    IEEE Power & Energy Society General Meeting\n    (2025)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Bayesian-Optimized Bidirectional Long-Short-Term Memory network for Wind Power Forecasting with Uncertainty Quantification\n    Kundan Kumar\n    Electric Power Systems Research (under Review)\n    (2025)\n    \n\n    Details\n\n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\n    Kundan Kumar.\n    IEEE\n    (2025)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample\n    Sprunger, Girard, & Chard\n    Journal of Traumatic Stress\n    (2024)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Bayesian Optimization for Deep Reinforcement Learning for Robust Volt-Var Control\n    Kundan Kumar\n    IEEE\n    (2024)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Deep Rl-based volt-var control and attack resiliency for der-integrated distribution grids\n    Kundan Kumar\n    IEEE\n    (2024)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Deep learning and pattern-based methodology for multivariable sensor data regression\n    Kundan Kumar\n    IEEE International Conference on Machine Learning and Applications (ICMLA)\n    (2024)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics\n    Rincon Caicedo, Girard, et al.\n    Journal of Latinx Psychology\n    (2024)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis\n    Girard, Tie, & Liebenthal\n    ACII\n    (2023)\n    \n\n    Details\n\n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth\n    Swartz, Bylsma, Fournier, et al.\n    Journal of Affective Disorders\n    (2023)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Computational analysis of spoken language in acute psychosis and mania\n    Girard, Vail, Liebenthal, et al.\n    Schizophrenia Research\n    (2022)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment\n    Vail, Girard, Bylsma, et al.\n    ICMI\n    (2022)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement\n    Van Oest & Girard\n    Psychological Methods\n    (2022)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder\n    Sewall, Girard, Merranko, et al.\n    The Journal of Child Psychology and Psychiatry\n    (2021)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion\n    Girard, Cohn, Yin, & Morency\n    Affective Science\n    (2021)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    To rate or not to rate: Investigating evaluation methods for generated co-speech gestures\n    Wolfert, Girard, Kucherenko, & Belpaeme\n    ICMI\n    (2021)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Context-Dependent Models for Predicting and Characterizing Facial Expressiveness\n    Lin, Girard, & Morency\n    WACA\n    (2020)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict\n    Hopwood, Harrison, Amole, et al.\n    Assessment\n    (2020)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Toward Multimodal Modeling of Emotional Expressiveness\n    Lin, Girard, Sayette, & Morency\n    ICMI\n    (2020)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Democratizing psychological insights from analysis of nonverbal behavior\n    McDuff & Girard\n    ACII\n    (2019)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Narcissistic admiration and rivalry: An interpersonal approach to construct validation\n    Grove, Smith, Girard, & Wright\n    Journal of Personality Disorders\n    (2019)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?\n    Girard, Shandar, Liu, et al.\n    ACII\n    (2019)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    DARMA: Software for dual axis rating and media annotation\n    Girard & Wright\n    Behavior Research Methods\n    (2018)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study\n    Pacella, Girard, Wright, et al.\n    Academic Emergency Medicine\n    (2018)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents\n    Shepherd, Sly, & Girard\n    Journal of Adolescence\n    (2017)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    FERA 2017 - Addressing head pose in the third facial expression recognition and analysis challenge\n    Valstar, Sánchez-Lozano, Cohn, et al.\n    FG\n    (2017)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses\n    Girard & McDuff\n    FG\n    (2017)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Interpersonal problems across levels of the psychopathology hierarchy\n    Girard, Wright, Beeney, et al.\n    Comprehensive Psychiatry\n    (2017)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Large-scale observational evidence of cross-cultural differences in facial behavior\n    McDuff, Girard, & el Kaliouby\n    Journal of Nonverbal Behavior\n    (2017)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment\n    Ross, Girard, Wright, et al.\n    Psychological Assessment\n    (2017)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Sayette group formation task (GFT) spontaneous facial expression database\n    Girard, Chu, Jeni, et al.\n    FG\n    (2017)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    A primer on observational measurement\n    Girard & Cohn\n    Assessment\n    (2016)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Deep value of information estimators for collaborative human-machine information gathering\n     Kundan Kumar\n    ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS\n    (2016)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Multimodal spontaneous emotion corpus for human behavior analysis\n    Zhang, Girard, Wu, et al.\n    CVPR\n    (2016)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Automated audiovisual depression analysis\n    Girard & Cohn\n    Current Opinion in Psychology\n    (2015)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Estimating smile intensity: A better way\n    Girard, Cohn, & De la Torre\n    Pattern Recognition Letters\n    (2015)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    FERA 2015 - Second Facial Expression Recognition and Analysis challenge\n    Valstar, Almaev, Girard, et al.\n    FG\n    (2015)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    How much training data for facial action unit detection?\n    Girard, Cohn, Jeni, et al.\n    FG\n    (2015)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Spontaneous facial expression in unscripted social interactions can be measured automatically\n    Girard, Cohn, Jeni, et al.\n    Behavior Research Methods\n    (2015)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database\n    Zhang, Yin, Cohn, et al.\n    Image and Vision Computing\n    (2014)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    CARMA: Software for continuous affect rating and media annotation\n    Girard \n    Journal of Open Research Software\n    (2014)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses\n    Girard, Cohn, Mahoor, et al.\n    Image and Vision Computing\n    (2014)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Continuous AU intensity estimation using localized, sparse facial feature space\n    Jeni, Girard, Cohn, & De la Torre\n    FG\n    (2013)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Social risk and depression: Evidence from manual and automatic facial expression analysis\n    Girard, Cohn, Mahoor, et al.\n    FG\n    (2013)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Criteria and metrics for thresholded AU detection\n    Girard & Cohn\n    ICCV\n    (2011)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publicationx/rstats_20230721_teaching/index.html",
    "href": "publicationx/rstats_20230721_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Time and place: July 21, 2023 10AM. Roche office, Basel, Switzerland\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "publicationx/rstats_20230721_teaching/index.html#about-the-topic",
    "href": "publicationx/rstats_20230721_teaching/index.html#about-the-topic",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "About the topic",
    "text": "About the topic\nThe 8 day introductory statistics course (MF9130) at the Faculty of Medicine, University of Oslo is designed for PhD students in medicine, biology, psychology and other health related fields. Similar to other conventional teaching methods, the course has been focusing largely on theory and hand calculation. The software has been Stata and SPSS, and data analysis was mostly left for the students to figure out on their own.\nThis year, we made an attempt to transform the course with R, and aimed to teach more practical data analysis skills. We added one session per day where the instructor guide students on R and project management, importing data , basic manipulation and statistical methods. The IT skills of the students vary greatly, and therefore we used the ‘sticky notes’ help system borrowed from the Carpentries to make sure everyone could get help in the first days. We have created a course website using Quarto, where all the material and R exercises (with rendered solution) are available for self-study. We have witnessed amazing progress - by the end of the first week, students with the least computer / data skills were able to work on dataframes, make basic plots and do a chi-squared test. This helps build students confidence in data and statistics, and as a result, they can start to work on their own datasets using the skills immediately."
  },
  {
    "objectID": "publicationx/rstats_20190402_blogdown/index.html",
    "href": "publicationx/rstats_20190402_blogdown/index.html",
    "title": "Building Website in R: Step by Step Introduction to blogdown",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "publicationx/community_20240921_quartofriends/index.html",
    "href": "publicationx/community_20240921_quartofriends/index.html",
    "title": "Use Quarto, Make Friends",
    "section": "",
    "text": "It has been two years since Quarto became the most popular reproducible publication tool in data science and R community. However Quarto is so much more than just a publication tool! I started using it since late 2022, and it has helped me become more organized, productive and connected with people in the data science community.\nIn this talk I will not focus on the technical aspects on ‘how’ to use this tool. In the first part of the talk, I would like to report the latest news and trends seen in the useR conference and Posit conf, the two biggest global R events. In the second part, I will share my own experience in using Quarto for my career: from learning new skills, collaborating with co-workers, teaching university courses to networking and building a community (CAMIS collaboration). It is a powerful tool to share your work, and make new connections - both for work and for fun! I hope this talk will provide you with some new ideas on how to use this fantastic technology to fulfill your goals."
  },
  {
    "objectID": "publicationx/ehr_20210218_biday/index.html",
    "href": "publicationx/ehr_20210218_biday/index.html",
    "title": "Network Analysis of Hospital EHR data",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "publicationx/index.html",
    "href": "publicationx/index.html",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "publicationx/index.html#upcoming",
    "href": "publicationx/index.html#upcoming",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "publicationx/index.html#selected-previous-talks",
    "href": "publicationx/index.html#selected-previous-talks",
    "title": "Talks",
    "section": "Selected previous talks",
    "text": "Selected previous talks\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nDate\n\n\n\n\n\n\n\n\nUse Quarto, Make Friends\n\n\nKolkata UseR meetup\n\n\n2024-09-21\n\n\n\n\n\n\nOne step closer to better Electronic Health Records data\n\n\nPHUSE Single Day Event Basel\n\n\n2024-09-18\n\n\n\n\n\n\nCAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n\n\n\n\n\n2024-07-10\n\n\n\n\n\n\nA one year recap on teaching statistcis to medical students: how can R and Quarto help?\n\n\nR/Medicine 2024 - Online\n\n\n2024-06-13\n\n\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\nBasel R meeting\n\n\n2023-07-21\n\n\n\n\n\n\nMachine Learning in Intensive Care Units\n\n\nPhD defence trial lecture\n\n\n2022-10-13\n\n\n\n\n\n\nSykdomspulsen: An automated public health surveillance platform\n\n\nOslo UseR meetup \n\n\n2022-06-16\n\n\n\n\n\n\nNetwork Analysis of Hospital EHR data\n\n\nBig Insight Day, Oslo\n\n\n2021-02-18\n\n\n\n\n\n\nBuilding Website in R: Step by Step Introduction to blogdown\n\n\nTalk at Oslo UseR meetup\n\n\n2019-04-02\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publicationx/rstats_20240613_teaching/index.html",
    "href": "publicationx/rstats_20240613_teaching/index.html",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "",
    "text": "Time and place: June 13 2024. Online\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "publicationx/rstats_20240613_teaching/index.html#about-the-topic",
    "href": "publicationx/rstats_20240613_teaching/index.html#about-the-topic",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "About the topic",
    "text": "About the topic\nThe Department of Biostatistics at University of Oslo offer statistics courses at different levels for medical students and PhD candidates with clinical backgrounds. The courses were traditionally taught with a focus on theory instead of data analysis, where SPSS and STATA were the tools of choice.\nSince 2023 spring semester, we have been gradually transforming some of our statistics courses into R, using Quarto course websites and Carpentries style live-coding instruction. With new Quarto tools (such as WebR) we also added interactivity in the code blocks. So far we have transformed two courses with over 100 students who have almost no programming experience. We have observed impressive progress in the skill development, and received significantly more positive feedback when it comes to statistics education.\nIn this talk, I would like to share our experience on the successes and challenges throughout the process. Looking back, is it cost-effective? Definitely. Can we do better in the future? Almost surely. If you are also planning to adopt new technology in your teaching activities, join us to learn more about what you can do to make the transition happen!\nCourse website can be accessed here"
  },
  {
    "objectID": "publicationx/ehr_20221013_ml_icu/index.html",
    "href": "publicationx/ehr_20221013_ml_icu/index.html",
    "title": "Machine Learning in Intensive Care Units",
    "section": "",
    "text": "A 45 minutes trial lecture to fulfill the requirement of my PhD degree."
  },
  {
    "objectID": "publications/articles/pacella2018.html",
    "href": "publications/articles/pacella2018.html",
    "title": "The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study",
    "section": "",
    "text": "Pacella, M. L., Girard, J. M., Wright, A. G. C., Suffoletto, B., & Callaway, C. W. (2018). The association between daily posttraumatic stress symptoms and pain over the first 14 days after injury: An experience sampling study. Academic Emergency Medicine, 25(8), 844–855."
  },
  {
    "objectID": "publications/articles/pacella2018.html#citation-apa-7",
    "href": "publications/articles/pacella2018.html#citation-apa-7",
    "title": "The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study",
    "section": "",
    "text": "Pacella, M. L., Girard, J. M., Wright, A. G. C., Suffoletto, B., & Callaway, C. W. (2018). The association between daily posttraumatic stress symptoms and pain over the first 14 days after injury: An experience sampling study. Academic Emergency Medicine, 25(8), 844–855."
  },
  {
    "objectID": "publications/articles/pacella2018.html#abstract",
    "href": "publications/articles/pacella2018.html#abstract",
    "title": "The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nPsychosocial factors and responses to injury modify the transition from acute to chronic pain. Specifically, posttraumatic stress disorder (PTSD) symptoms (reexperiencing, avoidance, and hyperarousal symptoms) exacerbate and cooccur with chronic pain. Yet no study has prospectively considered the associations among these psychological processes and pain reports using experience sampling methods (ESMs) during the acute aftermath of injury. This study applied ESM via daily text messaging to monitor and detect relationships among psychosocial factors and postinjury pain across the first 14 days after emergency department (ED) discharge.\n\n\nMethods\nWe recruited 75 adults (59% male; mean ± SD age = 34 ± 11.73 years) who experienced a potentially traumatic injury (i.e., involving life threat or serious injury) in the past 24 hours from the EDs of two Level I trauma centers. Participants received five questions per day via text messaging from Day 1 to Day 14 post–ED discharge; three questions measured PTSD symptoms, one question measured perceived social support, and one question measured physical pain.\n\n\nResults\nSixty-seven participants provided sufficient data for inclusion in the final analyses, and the average response rate per subject was 86%. Pain severity score decreased from a mean ± SD of 7.2 ± 2.0 to 4.4 ± 2.69 over 14 days and 50% of the variance in daily pain scores was within person. In multilevel structural equation models, pain scores decreased over time, and daily fluctuations of hyperarousal (B = 0.22, 95% confidetnce interval = 0.08–0.36) were uniquely associated with daily fluctuations in reported pain level within each person.\n\n\nConclusions\nDaily hyperarousal symptoms predict same-day pain severity over the acute postinjury recovery period. We also demonstrated feasibility to screen and identify patients at risk for pain chronicity in the acute aftermath of injury. Early interventions aimed at addressing hyperarousal (e.g., anxiolytics) could potentially aid in reducing experience of pain."
  },
  {
    "objectID": "publications/articles/ross2017.html",
    "href": "publications/articles/ross2017.html",
    "title": "Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment",
    "section": "",
    "text": "Ross, J. M., Girard, J. M., Wright, A. G. C., Beeney, J. E., Scott, L. N., Hallquist, M. N., Lazarus, S. A., Stepp, S. D., & Pilkonis, P. A. (2017). Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment. Psychological Assessment, 29(2), 123–134."
  },
  {
    "objectID": "publications/articles/ross2017.html#citation-apa-7",
    "href": "publications/articles/ross2017.html#citation-apa-7",
    "title": "Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment",
    "section": "",
    "text": "Ross, J. M., Girard, J. M., Wright, A. G. C., Beeney, J. E., Scott, L. N., Hallquist, M. N., Lazarus, S. A., Stepp, S. D., & Pilkonis, P. A. (2017). Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment. Psychological Assessment, 29(2), 123–134."
  },
  {
    "objectID": "publications/articles/ross2017.html#abstract",
    "href": "publications/articles/ross2017.html#abstract",
    "title": "Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment",
    "section": "Abstract",
    "text": "Abstract\nRelationships are among the most salient factors affecting happiness and wellbeing for individuals and families. Relationship science has identified the study of dyadic behavioral patterns between couple members during conflict as an important window in to relational functioning with both short-term and long-term consequences. Several methods have been developed for the momentary assessment of behavior during interpersonal transactions. Among these, the most popular is the Specific Affect Coding System (SPAFF), which organizes social behavior into a set of discrete behavioral constructs. This study examines the interpersonal meaning of the SPAFF codes through the lens of interpersonal theory, which uses the fundamental dimensions of Dominance and Affiliation to organize interpersonal behavior. A sample of 67 couples completed a conflict task, which was video recorded and coded using SPAFF and a method for rating momentary interpersonal behavior, the Continuous Assessment of Interpersonal Dynamics (CAID). Actor partner interdependence models in a multilevel structural equation modeling framework were used to study the covariation of SPAFF codes and CAID ratings. Results showed that a number of SPAFF codes had clear interpersonal signatures, but many did not. Additionally, actor and partner effects for the same codes were strongly consistent with interpersonal theory’s principle of complementarity. Thus, findings reveal points of convergence and divergence in the 2 systems and provide support for central tenets of interpersonal theory. Future directions based on these initial findings are discussed."
  },
  {
    "objectID": "publications/articles/sprunger2024.html",
    "href": "publications/articles/sprunger2024.html",
    "title": "Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample",
    "section": "",
    "text": "Sprunger, J. G., Girard, J. M., & Chard, K. M. (2024). Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample. Journal of Traumatic Stress."
  },
  {
    "objectID": "publications/articles/sprunger2024.html#citation-apa-7",
    "href": "publications/articles/sprunger2024.html#citation-apa-7",
    "title": "Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample",
    "section": "",
    "text": "Sprunger, J. G., Girard, J. M., & Chard, K. M. (2024). Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample. Journal of Traumatic Stress."
  },
  {
    "objectID": "publications/articles/sprunger2024.html#abstract",
    "href": "publications/articles/sprunger2024.html#abstract",
    "title": "Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample",
    "section": "Abstract",
    "text": "Abstract\nDimensional conceptualizations of psychopathology hold promise for understanding the high rates of comorbidity with posttraumatic stress disorder (PTSD). Linking PTSD symptoms to transdiagnostic dimensions of psychopathology may enable researchers and clinicians to understand the patterns and breadth of behavioral sequelae following traumatic experiences that may be shared with other psychiatric disorders. To explore this premise, we recruited a trauma-exposed online community sample \\((N = 462)\\) and measured dimensional transdiagnostic traits of psychopathology using parceled facets derived from the Personality Inventory for DSM-5 Faceted–Short Form. PTSD symptom factors were measured using the PTSD Checklist for DSM-5 and derived using confirmatory factor analysis according to the seven-factor hybrid model (i.e., Intrusions, Avoidance, Negative Affect, Anhedonia, Externalizing Behaviors, Anxious Arousal, And Dysphoric Arousal). We observed hypothesized associations between PTSD factors and transdiagnostic traits indicating that some transdiagnostic dimensions were associated with nearly all PTSD symptom factors (e.g., emotional lability: rmean = .35), whereas others showed more unique relationships (e.g., hostility–Externalizing Behavior: \\(r = .60\\); hostility with other PTSD factors: \\(r = .12–.31\\)). All PTSD factors were correlated with traits beyond those that would appear to be construct-relevant, suggesting the possibility of indirect associations that should be explicated in future research. The results indicate the breadth of trait-like consequences associated with PTSD symptom exacerbation, with implications for case conceptualization and treatment planning. Although PTSD is not a personality disorder, the findings indicate that increased PTSD factor severity is moderately associated with different patterns of trait-like disruptions in many areas of functioning."
  },
  {
    "objectID": "publications/articles/adaryukov2024.html",
    "href": "publications/articles/adaryukov2024.html",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "",
    "text": "K. Kumar, K. Utkarsh, J. Wang, and H. V. Padullaparti, “Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems,” Proc. IEEE Power & Energy Society General Meeting (PESGM), 2025."
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#citation-apa-7",
    "href": "publications/articles/adaryukov2024.html#citation-apa-7",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "",
    "text": "Adaryukov, J., Biernat, M., Girard, J. M., Villicana, A. J., & Pleskac, T. J. (in press). Worth the weight: An examination of unstructured and structured data in graduate admissions. (Accepted)"
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#abstract",
    "href": "publications/articles/adaryukov2024.html#abstract",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Abstract",
    "text": "Abstract\nThe integration of advanced metering infrastructure (AMI) into power distribution networks generates valuable data for tasks such as phase identification; however, the limited and unreliable availability of labeled data in the form of customer phase connectivity presents challenges. To address this issue, we propose a semi-supervised learning (SSL) framework that effectively leverages both labeled and unlabeled data.\n\nWhy Phase Identification Needs a New Approach ?\n\n\n\n\nFig. 1: Illustration of Semi-Supervised Learning Techniques\n\n\nProblem: Utilities don’t know which phase customers are connected to — this affects voltage regulation, DER integration, and fault localization. • Challenge: Ground truth phase data is scarce, unreliable, and costly to collect. • Supervised learning ML methods need lots of labeled data – often unavailable or unreliable. • Motivation: How do we scale phase identification without needing tons of labeled data?\nadd techniques"
  },
  {
    "objectID": "publications/articles/girard2017a.html",
    "href": "publications/articles/girard2017a.html",
    "title": "Interpersonal problems across levels of the psychopathology hierarchy",
    "section": "",
    "text": "Girard, J. M., Wright, A. G. C., Beeney, J. E., Lazarus, S. A., Scott, L. N., Stepp, S. D., & Pilkonis, P. A. (2017). Interpersonal problems across levels of the psychopathology hierarchy. Comprehensive Psychiatry, 79, 53–69."
  },
  {
    "objectID": "publications/articles/girard2017a.html#citation-apa-7",
    "href": "publications/articles/girard2017a.html#citation-apa-7",
    "title": "Interpersonal problems across levels of the psychopathology hierarchy",
    "section": "",
    "text": "Girard, J. M., Wright, A. G. C., Beeney, J. E., Lazarus, S. A., Scott, L. N., Stepp, S. D., & Pilkonis, P. A. (2017). Interpersonal problems across levels of the psychopathology hierarchy. Comprehensive Psychiatry, 79, 53–69."
  },
  {
    "objectID": "publications/articles/girard2017a.html#abstract",
    "href": "publications/articles/girard2017a.html#abstract",
    "title": "Interpersonal problems across levels of the psychopathology hierarchy",
    "section": "Abstract",
    "text": "Abstract\nWe examined the relationship between psychopathology and interpersonal problems in a sample of 825 clinical and community participants. Sixteen psychiatric diagnoses and five transdiagnostic dimensions were examined in relation to self-reported interpersonal problems. The structural summary method was used with the Inventory of Interpersonal Problems Circumplex Scales to examine interpersonal problem profiles for each diagnosis and dimension. We built a structural model of mental disorders including factors corresponding to detachment (avoidant personality, social phobia, major depression), internalizing (dependent personality, borderline personality, panic disorder, posttraumatic stress, major depression), disinhibition (antisocial personality, drug dependence, alcohol dependence, borderline personality), dominance (histrionic personality, narcissistic personality, paranoid personality), and compulsivity (obsessive-compulsive personality). All dimensions showed good interpersonal prototypicality (e.g., detachment was defined by a socially avoidant/nonassertive interpersonal profile) except for internalizing, which was diffusely associated with elevated interpersonal distress. The findings for individual disorders were largely consistent with the dimension that each disorder loaded on, with the exception of the internalizing and dominance disorders, which were interpersonally heterogeneous. These results replicate previous findings and provide novel insights into social dysfunction in psychopathology by wedding the power of hierarchical (i.e., dimensional) modeling and interpersonal circumplex assessment."
  },
  {
    "objectID": "publications/articles/bowdring2021.html",
    "href": "publications/articles/bowdring2021.html",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "",
    "text": "Bowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259."
  },
  {
    "objectID": "publications/articles/bowdring2021.html#citation-apa-7",
    "href": "publications/articles/bowdring2021.html#citation-apa-7",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "",
    "text": "Bowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259."
  },
  {
    "objectID": "publications/articles/bowdring2021.html#abstract",
    "href": "publications/articles/bowdring2021.html#abstract",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "Abstract",
    "text": "Abstract\nPhysical attractiveness plays a central role in psychosocial experiences. One of the top research priorities has been to identify factors affecting perceptions of physical attractiveness (PPA). Recent work suggests PPA derives from different sources (e.g., target, perceiver, stimulus type). Although smiles in particular are believed to enhance PPA, support has been surprisingly limited. This study comprehensively examines the effect of smiles on PPA and, more broadly, evaluates the roles of target, perceiver, and stimulus type in PPA variation. Perceivers (n=181) rated both static images and 5-s videos of targets displaying smiling and neutral-expressions. Smiling images were rated as more attractive than neutral-expression images (regardless of stimulus motion format). Interestingly, perceptions of physical attractiveness were based more on the perceiver than on either the target or format in which the target was presented. Results clarify the effect of smiles, and highlight the significant role of the perceiver, in PPA."
  },
  {
    "objectID": "publications/articles/baber2024.html",
    "href": "publications/articles/baber2024.html",
    "title": "It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports",
    "section": "",
    "text": "Baber, G. R., Hamilton, N. A., Girard, J. M., Cohen, J. M., Gratton, M. K. P., Ellis, S., & Hemmer, E. (in press). It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports. Sleep."
  },
  {
    "objectID": "publications/articles/baber2024.html#citation-apa-7",
    "href": "publications/articles/baber2024.html#citation-apa-7",
    "title": "It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports",
    "section": "",
    "text": "Baber, G. R., Hamilton, N. A., Girard, J. M., Cohen, J. M., Gratton, M. K. P., Ellis, S., & Hemmer, E. (in press). It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports. Sleep."
  },
  {
    "objectID": "publications/articles/baber2024.html#abstract",
    "href": "publications/articles/baber2024.html#abstract",
    "title": "It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports",
    "section": "Abstract",
    "text": "Abstract\nTBA"
  },
  {
    "objectID": "publications/articles/vanoest2022.html",
    "href": "publications/articles/vanoest2022.html",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "",
    "text": "van Oest, R., & Girard, J. M. (2022). Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement. Psychological Methods, 27(6), 1069–1088."
  },
  {
    "objectID": "publications/articles/vanoest2022.html#citation-apa-7",
    "href": "publications/articles/vanoest2022.html#citation-apa-7",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "",
    "text": "van Oest, R., & Girard, J. M. (2022). Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement. Psychological Methods, 27(6), 1069–1088."
  },
  {
    "objectID": "publications/articles/vanoest2022.html#abstract",
    "href": "publications/articles/vanoest2022.html#abstract",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "Abstract",
    "text": "Abstract\nVan Oest (2019) developed a framework to assess interrater agreement for nominal categories and complete data. We generalize this framework to all four situations of nominal or ordinal categories and complete or incomplete data. The mathematical solution yields a chance-corrected agreement coefficient that accommodates any weighting scheme for penalizing rater disagreements and any number of raters and categories. By incorporating Bayesian estimates of the category proportions, the generalized coefficient also captures situations in which raters classify only subsets of items; that is, incomplete data. Furthermore, this coefficient encompasses existing chance-corrected agreement coefficients: the S-coefficient, Scott’s pi, Fleiss’ kappa, and Van Oest’s uniform prior coefficient, all augmented with a weighting scheme and the option of incomplete data. We use simulation to compare these nested coefficients. The uniform prior coefficient tends to perform best, in particular, if one category has a much larger proportion than others. The gap with Scott’s pi and Fleiss’ kappa widens if the weighting scheme becomes more lenient to small disagreements and often if more item classifications are missing; missingness biases play a moderating role. The uniform prior coefficient often performs much better than the S-coefficient, but the S-coefficient sometimes performs best for small samples, missing data, and lenient weighting schemes. The generalized framework implies a new interpretation of chance-corrected weighted agreement coefficients: These coefficients estimate the probability that both raters in a pair assign an item to its correct category without guessing. Whereas Van Oest showed this interpretation for unweighted agreement, we generalize to weighted agreement."
  },
  {
    "objectID": "publications/articles/vanoest2022.html#translational-abstract",
    "href": "publications/articles/vanoest2022.html#translational-abstract",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "Translational Abstract",
    "text": "Translational Abstract\nMany studies and assessments require classification of subjective items (e.g., text) into categories (e.g., based on content). To assess whether the results are reproducible, it is good practice to let two or more raters independently classify the items, compute the proportion of pairwise rater agreement, and adjust for agreement expected by chance. Most chance-corrected agreement coefficients assume nominal categories and include only full agreements in which raters choose the same category. However, many situations (e.g., point scales) imply ordinal categories, where raters may receive partial credit for disagreements, based on the distance of their chosen categories and captured by a weighting scheme. Furthermore, raters often classify only subsets of items, where the missing data occur either by accident or by design. The present study develops a framework to estimate chance-corrected agreement for all four combinations of nominal or ordinal categories and complete or incomplete data. The resulting coefficient requires only a few lines of programming code and captures several existing coefficients via different values of its input parameters; it augments all nested coefficients with a weighting scheme and the option of missing item classifications. We use simulation to compare the coefficient performances for different weighting schemes, missing data mechanisms, and category proportions: The so-called uniform prior coefficient often (but not always) performs best. Furthermore, our framework implies that chance-corrected agreement coefficients, both unweighted and weighted, estimate the probability that both raters in a pair assign an item to its correct category without guessing."
  },
  {
    "objectID": "publications/articles/mcduff2017.html",
    "href": "publications/articles/mcduff2017.html",
    "title": "Large-scale observational evidence of cross-cultural differences in facial behavior",
    "section": "",
    "text": "McDuff, D., Girard, J. M., & El Kaliouby, R. (2017). Large-scale observational evidence of cross-cultural differences in facial behavior. Journal of Nonverbal Behavior, 41(1), 1–19."
  },
  {
    "objectID": "publications/articles/mcduff2017.html#citation-apa-7",
    "href": "publications/articles/mcduff2017.html#citation-apa-7",
    "title": "Large-scale observational evidence of cross-cultural differences in facial behavior",
    "section": "",
    "text": "McDuff, D., Girard, J. M., & El Kaliouby, R. (2017). Large-scale observational evidence of cross-cultural differences in facial behavior. Journal of Nonverbal Behavior, 41(1), 1–19."
  },
  {
    "objectID": "publications/articles/mcduff2017.html#abstract",
    "href": "publications/articles/mcduff2017.html#abstract",
    "title": "Large-scale observational evidence of cross-cultural differences in facial behavior",
    "section": "Abstract",
    "text": "Abstract\nSelf-report studies have found evidence that cultures differ in the display rules they have for facial expressions (i.e., for what is appropriate for different people at different times). However, observational studies of actual patterns of facial behavior have been rare and typically limited to the analysis of dozens of participants from two or three regions. We present the first large-scale evidence of cultural differences in observed facial behavior, including 740,984 participants from 12 countries around the world. We used an Internet-based framework to collect video data of participants in two different settings: in their homes and in market research facilities. Using computer vision algorithms designed for this dataset, we measured smiling and brow furrowing expressions as participants watched television ads. Our results reveal novel findings and provide empirical evidence to support theories about cultural and gender differences in display rules. Participants from more individualist cultures displayed more brow furrowing overall, whereas smiling depended on both culture and setting. Specifically, participants from more individualist countries were more expressive in the facility setting, while participants from more collectivist countries were more expressive in the home setting. Female participants displayed more smiling and less brow furrowing than male participants overall, with the latter difference being more pronounced in more individualist countries. This is the first study to leverage advances in computer science to enable large-scale observational research that would not have been possible using traditional methods."
  },
  {
    "objectID": "publications/articles/hopwood2020.html",
    "href": "publications/articles/hopwood2020.html",
    "title": "Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict",
    "section": "",
    "text": "Hopwood, C. J., Harrison, A. L., Amole, M. C., Girard, J. M., Wright, A. G. C., Thomas, K. M., Sadler, P., Ansell, E. B., Chaplin, T. M., Morey, L. C., Crowley, M. J., Durbin, C. E., & Kashy, D. A. (2020). Properties of the continuous assessment of interpersonal dynamics across sex, level of familiarity, and interpersonal conflict. Assessment, 27(1), 40–56."
  },
  {
    "objectID": "publications/articles/hopwood2020.html#citation-apa-7",
    "href": "publications/articles/hopwood2020.html#citation-apa-7",
    "title": "Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict",
    "section": "",
    "text": "Hopwood, C. J., Harrison, A. L., Amole, M. C., Girard, J. M., Wright, A. G. C., Thomas, K. M., Sadler, P., Ansell, E. B., Chaplin, T. M., Morey, L. C., Crowley, M. J., Durbin, C. E., & Kashy, D. A. (2020). Properties of the continuous assessment of interpersonal dynamics across sex, level of familiarity, and interpersonal conflict. Assessment, 27(1), 40–56."
  },
  {
    "objectID": "publications/articles/hopwood2020.html#abstract",
    "href": "publications/articles/hopwood2020.html#abstract",
    "title": "Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict",
    "section": "Abstract",
    "text": "Abstract\nThe Continuous Assessment of Interpersonal Dynamics (CAID) is a method in which trained observers code individuals’ dominance and warmth continuously while they interact in dyads. This method has significant promise for assessing dynamic interpersonal processes. The purpose of this study was to examine the impact of individual sex, dyadic familiarity, and situational conflict on patterns of interpersonal warmth, dominance, and complementarity as assessed via CAID. We used six samples with 603 dyads, including 2 samples of unacquainted mixed-sex undergraduates interacting in a collaborative task, 2 samples of couples interacting in both collaborative and conflict tasks, and 2 samples of mothers and children interacting in both collaborative and conflict tasks. Complementarity effects were robust across all samples, and individuals tended to be relatively warm and dominant. Results from multilevel models indicated that women were slightly warmer than men whereas there were no sex differences in dominance. Unfamiliar dyads and dyads interacting in more collaborative tasks were relatively warmer, more submissive, and more complementary on warmth but less complementary on dominance. These findings speak to the utility of the CAID method for assessing interpersonal dynamics and provide norms for researchers who use the method for different types of samples and applications."
  },
  {
    "objectID": "publications/articles/girard2022a.html",
    "href": "publications/articles/girard2022a.html",
    "title": "Computational analysis of spoken language in acute psychosis and mania",
    "section": "",
    "text": "Girard, J. M., Vail, A. K., Liebenthal, E., Brown, K., Kilciksiz, C. M., Pennant, L., Liebson, E., Öngür, D., Morency, L.-P., & Baker, J. T. (2022). Computational analysis of spoken language in acute psychosis and mania. Schizophrenia Research, 245, 97–115."
  },
  {
    "objectID": "publications/articles/girard2022a.html#citation-apa-7",
    "href": "publications/articles/girard2022a.html#citation-apa-7",
    "title": "Computational analysis of spoken language in acute psychosis and mania",
    "section": "",
    "text": "Girard, J. M., Vail, A. K., Liebenthal, E., Brown, K., Kilciksiz, C. M., Pennant, L., Liebson, E., Öngür, D., Morency, L.-P., & Baker, J. T. (2022). Computational analysis of spoken language in acute psychosis and mania. Schizophrenia Research, 245, 97–115."
  },
  {
    "objectID": "publications/articles/girard2022a.html#abstract",
    "href": "publications/articles/girard2022a.html#abstract",
    "title": "Computational analysis of spoken language in acute psychosis and mania",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nThis study aimed to (1) determine the feasibility of collecting behavioral data from participants hospitalized with acute psychosis and (2) begin to evaluate the clinical information that can be computationally derived from such data.\n\n\nMethods\nBehavioral data was collected across 99 sessions from 38 participants recruited from an inpatient psychiatric unit. Each session started with a semi-structured interview modeled on a typical “clinical rounds” encounter and included administration of the Positive and Negative Syndrome Scale (PANSS).\n\n\nAnalysis\nWe quantified aspects of participants’ verbal behavior during the interview using lexical, coherence, and disfluency features. We then used two complementary approaches to explore our second objective. The first approach used predictive models to estimate participants’ PANSS scores from their language features. Our second approach used inferential models to quantify the relationships between individual language features and symptom measures.\n\n\nResults\nOur predictive models showed promise but lacked sufficient data to achieve clinically useful accuracy. Our inferential models identified statistically significant relationships between numerous language features and symptom domains.\n\n\nConclusion\nOur interview recording procedures were well-tolerated and produced adequate data for transcription and analysis. The results of our inferential modeling suggest that automatic measurements of expressive language contain signals highly relevant to the assessment of psychosis. These findings establish the potential of measuring language during a clinical interview in a naturalistic setting and generate specific hypotheses that can be tested in future studies. This, in turn, will lead to more accurate modeling and better understanding of the relationships between expressive language and psychosis."
  },
  {
    "objectID": "publications/articles/girard2018a.html",
    "href": "publications/articles/girard2018a.html",
    "title": "DARMA: Software for dual axis rating and media annotation",
    "section": "",
    "text": "Girard, J. M., & Wright, A. G. C. (2018). DARMA: Software for Dual Axis Rating and Media Annotation. Behavior Research Methods, 50(3), 902–909."
  },
  {
    "objectID": "publications/articles/girard2018a.html#citation-apa-7",
    "href": "publications/articles/girard2018a.html#citation-apa-7",
    "title": "DARMA: Software for dual axis rating and media annotation",
    "section": "",
    "text": "Girard, J. M., & Wright, A. G. C. (2018). DARMA: Software for Dual Axis Rating and Media Annotation. Behavior Research Methods, 50(3), 902–909."
  },
  {
    "objectID": "publications/articles/girard2018a.html#abstract",
    "href": "publications/articles/girard2018a.html#abstract",
    "title": "DARMA: Software for dual axis rating and media annotation",
    "section": "Abstract",
    "text": "Abstract\nContinuous measurement systems provide a means of measuring dynamic behavioral and experiential processes as they play out over time. DARMA is a modernized continuous measurement system that synchronizes media playback and the continuous recording of two-dimensional measurements. These measurements can be observational or self-reported and are provided in real-time through the manipulation of a computer joystick. DARMA also provides tools for reviewing and comparing collected measurements and for customizing various settings. DARMA is a domain-independent software tool that was designed to aid researchers who are interested in gaining a deeper understanding of behavior and experience. It is especially well-suited to the study of affective and interpersonal processes, such as the perception and expression of emotional states and the communication of social signals. DARMA is open-source using the GNU General Public License (GPL) and is available for free download from http://darma.jmgirard.com."
  },
  {
    "objectID": "publications/articles/chung2024.html",
    "href": "publications/articles/chung2024.html",
    "title": "Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders",
    "section": "",
    "text": "Chung, Y., Girard, J. M., Ravichandran, C., Ongur, D., Cohen, B. M., Baker, J. T. (in press). Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders. Journal of Psychopathology and Clinical Science."
  },
  {
    "objectID": "publications/articles/chung2024.html#citation-apa-7",
    "href": "publications/articles/chung2024.html#citation-apa-7",
    "title": "Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders",
    "section": "",
    "text": "Chung, Y., Girard, J. M., Ravichandran, C., Ongur, D., Cohen, B. M., Baker, J. T. (in press). Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders. Journal of Psychopathology and Clinical Science."
  },
  {
    "objectID": "publications/articles/chung2024.html#abstract",
    "href": "publications/articles/chung2024.html#abstract",
    "title": "Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders",
    "section": "Abstract",
    "text": "Abstract\nPrevailing factor models of psychosis are centered on schizophrenia-related disorders defined by the DSM and ICD, restricting generalizability to other clinical presentations featuring psychosis, even though affective psychoses are more common. This study aims to bridge this gap by conducting exploratory and confirmatory factor analyses, utilizing clinical ratings collected from patients with either affective or non-affective psychoses (n = 1042). Drawing from established clinical instruments, such as the Positive and Negative Syndrome Scale, Young Mania Rating Scale, and Montgomery-Åsberg Depression Rating Scale, a broad spectrum of core psychotic symptoms was considered for the model development. Among the candidate models considered, including correlated factors and multifactor models, a model with seven correlated factors encompassing positive symptoms, negative symptoms, depression, mania, disorganization, hostility, and anxiety was most interpretable with acceptable fit. The seven factors exhibited expected associations with external validators, were replicable through cross- validation, and were generalizable across affective and non-affective psychoses."
  },
  {
    "objectID": "publications/articles/girard2015c.html",
    "href": "publications/articles/girard2015c.html",
    "title": "Automated audiovisual depression analysis",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2015). Automated audiovisual depression analysis. Current Opinion in Psychology, 4, 75–79."
  },
  {
    "objectID": "publications/articles/girard2015c.html#citation-apa-7",
    "href": "publications/articles/girard2015c.html#citation-apa-7",
    "title": "Automated audiovisual depression analysis",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2015). Automated audiovisual depression analysis. Current Opinion in Psychology, 4, 75–79."
  },
  {
    "objectID": "publications/articles/girard2015c.html#abstract",
    "href": "publications/articles/girard2015c.html#abstract",
    "title": "Automated audiovisual depression analysis",
    "section": "Abstract",
    "text": "Abstract\nAnalysis of observable behavior in depression primarily relies on subjective measures. New computational approaches make possible automated audiovisual measurement of behaviors that humans struggle to quantify (e.g., movement velocity and voice inflection). These tools have the potential to improve screening and diagnosis, identify new behavioral indicators of depression, measure response to clinical intervention, and test clinical theories about underlying mechanisms. Highlights include a study that measured the temporal coordination of vocal tract and facial movements, a study that predicted which adolescents would go on to develop depression based on their voice qualities, and a study that tested the behavioral predictions of clinical theories using automated measures of facial actions and head motion."
  },
  {
    "objectID": "publications/articles/butler2024.html",
    "href": "publications/articles/butler2024.html",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "",
    "text": "Butler, R. M., Christian, C., Girard, J. M., Vanzhula, I. A., & Levinson, C. A. (2024). Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of imaginal exposure for eating disorders. Behavior Research and Therapy, 180, 104577."
  },
  {
    "objectID": "publications/articles/butler2024.html#citation-apa-7",
    "href": "publications/articles/butler2024.html#citation-apa-7",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "",
    "text": "Butler, R. M., Christian, C., Girard, J. M., Vanzhula, I. A., & Levinson, C. A. (2024). Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of imaginal exposure for eating disorders. Behavior Research and Therapy, 180, 104577."
  },
  {
    "objectID": "publications/articles/butler2024.html#abstract",
    "href": "publications/articles/butler2024.html#abstract",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "Abstract",
    "text": "Abstract\nObjective: Imaginal exposure is a novel intervention for eating disorders (EDs) that has been investigated as a method for targeting ED symptoms and fears. Research is needed to understand mechanisms of change during imaginal exposure for EDs, including whether within- and between-session distress reduction is related to treatment outcomes.\nMethod: Study 1 tested four sessions of online imaginal exposure (N = 143). Study 2 examined combined imaginal and in vivo exposure, comprising six imaginal exposure sessions (N = 26). ED symptoms and fears were assessed pre- and posttreatment, and subjective distress and state anxiety were collected during sessions.\nResults: Subjective distress tended to increase within-session in both studies, and within-session reduction was not associated with change in ED symptoms or fears. In Study 1, between-session reduction of distress and state anxiety was associated with greater decreases in ED symptoms and fears pre-to posttreatment. In Study 2, between-session distress reduction occurred but was not related to outcomes.\nConclusions: Within-session distress reduction may not promote change during exposure for EDs, whereas between-session distress reduction may be associated with better treatment outcomes. These findings corroborate research on distress reduction during exposure for anxiety disorders. Clinicians might consider approaches to exposure-based treatment that focus on distress tolerance and promote between-session distress reduction."
  },
  {
    "objectID": "publications/proceedings/girard2017b.html",
    "href": "publications/proceedings/girard2017b.html",
    "title": "Sayette group formation task (GFT) spontaneous facial expression database",
    "section": "",
    "text": "Girard, J. M., Chu, W.-S., Jeni, L. A., Cohn, J. F., De La Torre, F., & Sayette, M. A. (2017). Sayette Group Formation Task (GFT) Spontaneous Facial Expression Database. Proceedings of the 12th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 581–588."
  },
  {
    "objectID": "publications/proceedings/girard2017b.html#citation-apa-7",
    "href": "publications/proceedings/girard2017b.html#citation-apa-7",
    "title": "Sayette group formation task (GFT) spontaneous facial expression database",
    "section": "",
    "text": "Girard, J. M., Chu, W.-S., Jeni, L. A., Cohn, J. F., De La Torre, F., & Sayette, M. A. (2017). Sayette Group Formation Task (GFT) Spontaneous Facial Expression Database. Proceedings of the 12th IEEE International Conference on Automatic Face and Gesture Recognition (FG), 581–588."
  },
  {
    "objectID": "publications/proceedings/girard2017b.html#abstract",
    "href": "publications/proceedings/girard2017b.html#abstract",
    "title": "Sayette group formation task (GFT) spontaneous facial expression database",
    "section": "Abstract",
    "text": "Abstract\nDespite the important role that facial expressions play in interpersonal communication and our knowledge that interpersonal behavior is influenced by social context, no currently available facial expression database includes multiple interacting participants. The Sayette Group Formation Task (GFT) database addresses the need for well-annotated video of multiple participants during unscripted interactions. The database includes 172,800 video frames from 96 participants in 32 three-person groups. To aid in the development of automated facial expression analysis systems, GFT includes expert annotations of FACS occurrence and intensity, facial landmark tracking, and baseline results for linear SVM, deep learning, active patch learning, and personalized classification. Baseline performance is quantified and compared using identical partitioning and a variety of metrics (including means and confidence intervals). The highest performance scores were found for the deep learning and active patch learning methods. Learn more at http://osf.io/7wcyz"
  },
  {
    "objectID": "publications/proceedings/zhang2016.html",
    "href": "publications/proceedings/zhang2016.html",
    "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
    "section": "",
    "text": "Zhang, Z., Girard, J. M., Wu, Y., Zhang, X., Liu, P., Ciftci, U., Canavan, S., Reale, M., Horowitz, A., Yang, H., Cohn, J. F., Ji, Q., & Yin, L. (2016). Multimodal spontaneous emotion corpus for human behavior analysis. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3438–3446."
  },
  {
    "objectID": "publications/proceedings/zhang2016.html#citation-apa-7",
    "href": "publications/proceedings/zhang2016.html#citation-apa-7",
    "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
    "section": "",
    "text": "Zhang, Z., Girard, J. M., Wu, Y., Zhang, X., Liu, P., Ciftci, U., Canavan, S., Reale, M., Horowitz, A., Yang, H., Cohn, J. F., Ji, Q., & Yin, L. (2016). Multimodal spontaneous emotion corpus for human behavior analysis. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3438–3446."
  },
  {
    "objectID": "publications/proceedings/zhang2016.html#abstract",
    "href": "publications/proceedings/zhang2016.html#abstract",
    "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
    "section": "Abstract",
    "text": "Abstract\nEmotion is expressed in multiple modalities, yet most research has considered at most one or two. This stems in part from the lack of large, diverse, well-annotated, multimodal databases with which to develop and test algorithms. We present a well-annotated, multimodal, multidimensional spontaneous emotion corpus of 140 participants. Emotion inductions were highly varied. Data were acquired from a variety of sensors of the face that included high-resolution 3D dynamic imaging, high-resolution 2D video, and thermal (infrared) sensing, and contact physiological sensors that included electrical conductivity of the skin, respiration, blood pressure, and heart rate. Facial expression was annotated for both the occurrence and intensity of facial action units from 2D video by experts in the Facial Action Coding System (FACS). The corpus further includes derived features from 3D, 2D, and IR (infrared) sensors and baseline results for facial expression and action unit detection. The entire corpus will be made available to the research community."
  },
  {
    "objectID": "publications/proceedings/lin2020b.html",
    "href": "publications/proceedings/lin2020b.html",
    "title": "Toward Multimodal Modeling of Emotional Expressiveness",
    "section": "",
    "text": "Lin, V., Girard, J. M., Sayette, M. A., & Morency, L.-P. (2020). Toward Multimodal Modeling of Emotional Expressiveness. Proceedings of the 22nd International Conference on Multimodal Interaction, 548–557."
  },
  {
    "objectID": "publications/proceedings/lin2020b.html#citation-apa-7",
    "href": "publications/proceedings/lin2020b.html#citation-apa-7",
    "title": "Toward Multimodal Modeling of Emotional Expressiveness",
    "section": "",
    "text": "Lin, V., Girard, J. M., Sayette, M. A., & Morency, L.-P. (2020). Toward Multimodal Modeling of Emotional Expressiveness. Proceedings of the 22nd International Conference on Multimodal Interaction, 548–557."
  },
  {
    "objectID": "publications/proceedings/lin2020b.html#abstract",
    "href": "publications/proceedings/lin2020b.html#abstract",
    "title": "Toward Multimodal Modeling of Emotional Expressiveness",
    "section": "Abstract",
    "text": "Abstract\nEmotional expressiveness captures the extent to which a person tends to outwardly display their emotions through behavior. Due to the close relationship between emotional expressiveness and behavioral health, as well as the crucial role that it plays in social interaction, the ability to automatically predict emotional expressiveness stands to spur advances in science, medicine, and industry. In this paper, we explore three related research questions. First, how well can emotional expressiveness be predicted from visual,linguistic, and multimodal behavioral signals? Second, how important is each behavioral modality to the prediction of emotional expressiveness? Third, which behavioral signals are reliably related to emotional expressiveness? To answer these questions, we add highly reliable transcripts and human ratings of perceived emotional expressiveness to an existing video database and use this data to train, validate, and test predictive models. Our best model shows promising predictive performance on this dataset (RMSE=0.65,R2=0.45,r=0.74). Multimodal models tend to perform best overall, and models trained on the linguistic modality tend to outperform models trained on the visual modality. Finally,examination of our interpretable models’ coefficients reveals a number of visual and linguistic behavioral signals—such as facial action unit intensity, overall word count, and use of words related to social processes—that reliably predict emotional expressiveness."
  },
  {
    "objectID": "publications/proceedings/lin2020b.html#awards",
    "href": "publications/proceedings/lin2020b.html#awards",
    "title": "Toward Multimodal Modeling of Emotional Expressiveness",
    "section": "Awards",
    "text": "Awards\nThis paper was nominated for Best Paper at ICMI 2020."
  },
  {
    "objectID": "publications/proceedings/valstar2017.html",
    "href": "publications/proceedings/valstar2017.html",
    "title": "FERA 2017 - Addressing head pose in the third facial expression recognition and analysis challenge",
    "section": "",
    "text": "Valstar, M. F., Sanchez-Lozano, E., Cohn, J. F., Jeni, L. A., Girard, J. M., Zhang, Z., Yin, L., & Pantic, M. (2017). FERA 2017—Addressing head pose in the third facial expression recognition and analysis challenge. Proceedings of the 12th IEEE Conference on Automatic Face & Gesture Recognition (FG), 839–847."
  },
  {
    "objectID": "publications/proceedings/valstar2017.html#citation-apa-7",
    "href": "publications/proceedings/valstar2017.html#citation-apa-7",
    "title": "FERA 2017 - Addressing head pose in the third facial expression recognition and analysis challenge",
    "section": "",
    "text": "Valstar, M. F., Sanchez-Lozano, E., Cohn, J. F., Jeni, L. A., Girard, J. M., Zhang, Z., Yin, L., & Pantic, M. (2017). FERA 2017—Addressing head pose in the third facial expression recognition and analysis challenge. Proceedings of the 12th IEEE Conference on Automatic Face & Gesture Recognition (FG), 839–847."
  },
  {
    "objectID": "publications/proceedings/valstar2017.html#abstract",
    "href": "publications/proceedings/valstar2017.html#abstract",
    "title": "FERA 2017 - Addressing head pose in the third facial expression recognition and analysis challenge",
    "section": "Abstract",
    "text": "Abstract\nThe field of Automatic Facial Expression Analysis has grown rapidly in recent years. However, despite progress in new approaches as well as benchmarking efforts, most evaluations still focus on either posed expressions, near-frontal recordings, or both. This makes it hard to tell how existing expression recognition approaches perform under conditions where faces appear in a wide range of poses (or camera views), displaying ecologically valid expressions. The main obstacle for assessing this is the availability of suitable data, and the challenge proposed here addresses this limitation. The FG 2017 Facial Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to the estimation of Action Units occurrence and intensity under different camera views. In this paper we present the third challenge in automatic recognition of facial expressions, to be held in conjunction with the 12th IEEE conference on Face and Gesture Recognition, May 2017, in Washington, United States. Two sub-challenges are defined: the detection of AU occurrence, and the estimation of AU intensity. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for both sub-challenges."
  },
  {
    "objectID": "publications/proceedings/valstar2015.html",
    "href": "publications/proceedings/valstar2015.html",
    "title": "FERA 2015 - Second Facial Expression Recognition and Analysis challenge",
    "section": "",
    "text": "Valstar, M. F., Almaev, T., Girard, J. M., Mckeown, G., Mehu, M., Yin, L., Pantic, M., & Cohn, J. F. (2015). FERA 2015—Second Facial Expression Recognition and Analysis Challenge. Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/valstar2015.html#citation-apa-7",
    "href": "publications/proceedings/valstar2015.html#citation-apa-7",
    "title": "FERA 2015 - Second Facial Expression Recognition and Analysis challenge",
    "section": "",
    "text": "Valstar, M. F., Almaev, T., Girard, J. M., Mckeown, G., Mehu, M., Yin, L., Pantic, M., & Cohn, J. F. (2015). FERA 2015—Second Facial Expression Recognition and Analysis Challenge. Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/valstar2015.html#abstract",
    "href": "publications/proceedings/valstar2015.html#abstract",
    "title": "FERA 2015 - Second Facial Expression Recognition and Analysis challenge",
    "section": "Abstract",
    "text": "Abstract\nDespite efforts towards evaluation standards in facial expression analysis (e.g. FERA 2011), there is a need for up-to-date standardised evaluation procedures, focusing in particular on current challenges in the field. One of the challenges that is actively being addressed is the automatic estimation of expression intensities. To continue to provide a standardisation platform and to help the field progress beyond its current limitations, the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015) will challenge participants to estimate FACS Action Unit (AU) intensity as well as AU occurrence on a common benchmark dataset with reliable manual annotations. Evaluation will be done using a clear and well-defined protocol. In this paper we present the second such challenge in automatic recognition of facial expressions, to be held in conjunction with the 11 IEEE conference on Face and Gesture Recognition, May 2015, in Ljubljana, Slovenia. Three sub-challenges are defined: the detection of AU occurrence, the estimation of AU intensity for pre-segmented data, and fully automatic AU intensity estimation. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for the three sub-challenges."
  },
  {
    "objectID": "publications/proceedings/girard2011.html",
    "href": "publications/proceedings/girard2011.html",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/girard2011.html#citation-apa-7",
    "href": "publications/proceedings/girard2011.html#citation-apa-7",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/girard2011.html#abstract",
    "href": "publications/proceedings/girard2011.html#abstract",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "Abstract",
    "text": "Abstract\nImplementing a computerized facial expression analysis system for automatic coding requires that a threshold for the system’s classifier outputs be selected. However, there are many potential ways to select a threshold. How do different criteria and metrics compare? Manually FACS coded video of 45 clinical interviews (Spectrum dataset) were processed using person-specific active appearance models (AAM). Support vector machine (SVM) classifiers were trained using an independent dataset (RU-FACS). Spectrum sessions were randomly assigned to training (n=32) and testing sets (n=13). Six different threshold selection criteria were compared for automatic AU coding. Three major findings emerged: 1) Thresholds that attempt to balance the confusion matrix (using kappa, \\(F_1\\), or MCC) performed significantly better on all metrics than thresholds that select arbitrary error or accuracy rates (such as TPR, FPR, or EER). 2) AU detection scores for kappa, \\(F_1\\), and MCC were highly intercorrelated; accuracy was uncorrelated with the others. And 3) Kappa, MCC, and \\(F_1\\) were all positively correlated with base rate. They increased with increases in AU base rates. Accuracy, by contrast, showed the opposite pattern. It was strongly negatively correlated with base rate. These findings suggest that better automatic coding can be obtained by using threshold-selection criteria that balance the confusion matrix and benefit from increased AU base rates in the training data."
  },
  {
    "objectID": "publications/proceedings/girard2011.html#author-note",
    "href": "publications/proceedings/girard2011.html#author-note",
    "title": "Criteria and metrics for thresholded AU detection",
    "section": "Author Note",
    "text": "Author Note\n\nWhen I wrote this paper back in 2011, I was just learning about performance evaluation. This was a first, and rather naive attempt at understanding the connection between agreement, prevalence, and threshold selection. Readers interested in more sophisticated approaches to these issues are encouraged to look up Guangchao Charles Feng, who has done nice work in this area.\nr tufte::quote_footer('--- Jeffrey Girard, 2018-06-14')"
  },
  {
    "objectID": "publications/proceedings/girard2015d.html",
    "href": "publications/proceedings/girard2015d.html",
    "title": "How much training data for facial action unit detection?",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Lucey, S., & De la Torre, F. (2015). How much training data for facial action unit detection? Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2015d.html#citation-apa-7",
    "href": "publications/proceedings/girard2015d.html#citation-apa-7",
    "title": "How much training data for facial action unit detection?",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Lucey, S., & De la Torre, F. (2015). How much training data for facial action unit detection? Proceedings of the 11th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2015d.html#abstract",
    "href": "publications/proceedings/girard2015d.html#abstract",
    "title": "How much training data for facial action unit detection?",
    "section": "Abstract",
    "text": "Abstract\nBy systematically varying the number of subjects and the number of frames per subject, we explored the influence of training set size on appearance and shape-based approaches to facial action unit (AU) detection. Digital video and expert coding of spontaneous facial activity from 80 subjects (over 350,000 frames) were used to train and test support vector machine classifiers. Appearance features were shape-normalized SIFT descriptors and shape features were 66 facial landmarks. Ten-fold cross-validation was used in all evaluations. Number of subjects and number of frames per subject differentially affected appearance and shape-based classifiers. For appearance features, which are high-dimensional, increasing the number of training subjects from 8 to 64 incrementally improved performance, regardless of the number of frames taken from each subject (ranging from 450 through 3600). In contrast, for shape features, increases in the number of training subjects and frames were associated with mixed results. In summary, maximal performance was attained using appearance features from large numbers of subjects with as few as 450 frames per subject. These findings suggest that variation in the number of subjects rather than number of frames per subject yields most efficient performance."
  },
  {
    "objectID": "publications/proceedings/girard2023a.html",
    "href": "publications/proceedings/girard2023a.html",
    "title": "DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis",
    "section": "",
    "text": "Girard, J. M., Tie, Y., & Liebenthal, E. (2023). DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis. Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)."
  },
  {
    "objectID": "publications/proceedings/girard2023a.html#citation-apa-7",
    "href": "publications/proceedings/girard2023a.html#citation-apa-7",
    "title": "DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis",
    "section": "",
    "text": "Girard, J. M., Tie, Y., & Liebenthal, E. (2023). DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis. Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)."
  },
  {
    "objectID": "publications/proceedings/girard2023a.html#abstract",
    "href": "publications/proceedings/girard2023a.html#abstract",
    "title": "DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis",
    "section": "Abstract",
    "text": "Abstract\nIn this paper, we describe the design, collection, and validation of a new video database that includes holistic and dynamic emotion ratings from 83 participants watching 22 affective movie clips. In contrast to previous work in Affective Computing, which pursued a single “ground truth” label for the affective content of each moment of each video (e.g., by averaging the ratings of 2 to 7 trained participants), we embrace the subjectivity inherent to emotional experiences and provide the full distribution of all participants’ ratings (with an average of 76.7 raters per video). We argue that this choice represents a paradigm shift with the potential to unlock new research directions, generate new hypotheses, and inspire novel methods in the Affective Computing community. We also describe several interdisciplinary use cases for the database: to provide dynamic norms for emotion elicitation studies (e.g., in psychology, medicine, and neuroscience), to train and test affective content analysis algorithms (e.g., for dynamic emotion recognition, video summarization, and movie recommendation), and to study subjectivity in emotional reactions (e.g., to identify moments of emotional ambiguity or ambivalence within movies, identify predictors of subjectivity, and develop personalized affective content analysis algorithms). The database is made freely available to researchers for noncommercial use at https://dynamos.mgb.org."
  },
  {
    "objectID": "publications/proceedings/girard2019a.html",
    "href": "publications/proceedings/girard2019a.html",
    "title": "Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?",
    "section": "",
    "text": "Girard, J. M., Shandar, G., Liu, Z., Cohn, J. F., Yin, L., & Morency, L.-P. (2019). Reconsidering the Duchenne smile: Indicator of positive emotion or artifact of smile intensity? Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 594–599."
  },
  {
    "objectID": "publications/proceedings/girard2019a.html#citation-apa-7",
    "href": "publications/proceedings/girard2019a.html#citation-apa-7",
    "title": "Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?",
    "section": "",
    "text": "Girard, J. M., Shandar, G., Liu, Z., Cohn, J. F., Yin, L., & Morency, L.-P. (2019). Reconsidering the Duchenne smile: Indicator of positive emotion or artifact of smile intensity? Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 594–599."
  },
  {
    "objectID": "publications/proceedings/girard2019a.html#abstract",
    "href": "publications/proceedings/girard2019a.html#abstract",
    "title": "Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?",
    "section": "Abstract",
    "text": "Abstract\nThe Duchenne smile hypothesis is that smiles that include eye constriction (AU6) are the product of genuine positive emotion, whereas smiles that do not are either falsified or related to negative emotion. This hypothesis has become very influential and is often used in scientific and applied settings to justify the inference that a smile is either true or false. However, empirical support for this hypothesis has been equivocal and some researchers have proposed that, rather than being a reliable indicator of positive emotion, AU6 may just be an artifact produced by intense smiles. Initial support for this proposal has been found when comparing smiles related to genuine and feigned positive emotion; however, it has not yet been examined when comparing smiles related to genuine positive and negative emotion. The current study addressed this gap in the literature by examining spontaneous smiles from 136 participants during the elicitation of amusement, embarrassment, fear, and pain (from the BP4D+ dataset). Bayesian multilevel regression models were used to quantify the associations between AU6 and self-reported amusement while controlling for smile intensity. Models were estimated to infer amusement from AU6 and to explain the intensity of AU6 using amusement. In both cases, controlling for smile intensity substantially reduced the hypothesized association, whereas the effect of smile intensity itself was quite large and reliable. These results provide further evidence that the Duchenne smile is likely an artifact of smile intensity rather than a reliable and unique indicator of genuine positive emotion."
  },
  {
    "objectID": "publications/proceedings/girard2013.html",
    "href": "publications/proceedings/girard2013.html",
    "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., & Rosenwald, D. P. (2013). Social risk and depression: Evidence from manual and automatic facial expression analysis. Proceedings of the 10th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2013.html#citation-apa-7",
    "href": "publications/proceedings/girard2013.html#citation-apa-7",
    "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., & Rosenwald, D. P. (2013). Social risk and depression: Evidence from manual and automatic facial expression analysis. Proceedings of the 10th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 1–8."
  },
  {
    "objectID": "publications/proceedings/girard2013.html#abstract",
    "href": "publications/proceedings/girard2013.html#abstract",
    "title": "Social risk and depression: Evidence from manual and automatic facial expression analysis",
    "section": "Abstract",
    "text": "Abstract\nInvestigated the relationship between change over time in severity of depression symptoms and facial expression. Depressed participants were followed over the course of treatment and video recorded during a series of clinical interviews. Facial expressions were analyzed from the video using both manual and automatic systems. Automatic and manual coding were highly consistent for FACS action units, and showed similar effects for change over time in depression severity. For both systems, when symptom severity was high, participants made more facial expressions associated with contempt, smiled less, and those smiles that occurred were more likely to be accompanied by facial actions associated with contempt. These results are consistent with the “social risk hypothesis” of depression. According to this hypothesis, when symptoms are severe, depressed participants withdraw from other people in order to protect themselves from anticipated rejection, scorn, and social exclusion. As their symptoms fade, participants send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and produced the same pattern of depression effects suggests that automatic facial expression analysis may be ready for use in behavioral and clinical science."
  },
  {
    "objectID": "publications/proceedings/jeni2013.html",
    "href": "publications/proceedings/jeni2013.html",
    "title": "Continuous AU intensity estimation using localized, sparse facial feature space",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/jeni2013.html#citation-apa-7",
    "href": "publications/proceedings/jeni2013.html#citation-apa-7",
    "title": "Continuous AU intensity estimation using localized, sparse facial feature space",
    "section": "",
    "text": "Jeni, L. A., Girard, J. M., Cohn, J. F., & De la Torre, F. (2013). Continuous AU intensity estimation using localized, sparse facial feature space. Proceedings of the 10th IEEE International Conference on Automated Face & Gesture Recognition (FG), 1–7."
  },
  {
    "objectID": "publications/proceedings/jeni2013.html#abstract",
    "href": "publications/proceedings/jeni2013.html#abstract",
    "title": "Continuous AU intensity estimation using localized, sparse facial feature space",
    "section": "Abstract",
    "text": "Abstract\nMost work in automatic facial expression analysis seeks to detect discrete facial actions. Yet, the meaning and function of facial actions often depends in part on their intensity. We propose a part-based, sparse representation for automated measurement of continuous variation in AU intensity. We evaluated its effectiveness in two publically available databases, CK+ and the soon to be released Binghamton high-resolution spontaneous 3D dyadic facial expression database. The former consists of posed facial expressions and ordinal level intensity (absent, low, and high). The latter consists of spontaneous facial expression in response to diverse, well-validated emotion inductions, and 6 ordinal levels of AU intensity. In a preliminary test, we started from discrete emotion labels and ordinal-scale intensity annotation in the CK+ dataset. The algorithm achieved state-of-the-art performance. These preliminary results supported the utility of the part-based, sparse representation. Second, we applied the algorithm to the more demanding task of continuous AU intensity estimation in spontaneous facial behavior in the Binghamton database. Manual 6-point ordinal coding and continuous measurement were highly consistent. Visual analysis of the overlay of continuous measurement by the algorithm and manual ordinal coding strongly supported the representational power of the proposed method to smoothly interpolate across the full range of AU intensity."
  },
  {
    "objectID": "publications/proceedings/wolfert2021.html",
    "href": "publications/proceedings/wolfert2021.html",
    "title": "To rate or not to rate: Investigating evaluation methods for generated co-speech gestures",
    "section": "",
    "text": "Wolfert, P., Girard, J. M., Kucherenko, T., & Belpaeme, T. (2021). To Rate or Not To Rate: Investigating Evaluation Methods for Generated Co-Speech Gestures. In Proceedings of the 23rd International Conference on Multimodal Interaction (pp. 494–502). Association for Computing Machinery."
  },
  {
    "objectID": "publications/proceedings/wolfert2021.html#citation-apa-7",
    "href": "publications/proceedings/wolfert2021.html#citation-apa-7",
    "title": "To rate or not to rate: Investigating evaluation methods for generated co-speech gestures",
    "section": "",
    "text": "Wolfert, P., Girard, J. M., Kucherenko, T., & Belpaeme, T. (2021). To Rate or Not To Rate: Investigating Evaluation Methods for Generated Co-Speech Gestures. In Proceedings of the 23rd International Conference on Multimodal Interaction (pp. 494–502). Association for Computing Machinery."
  },
  {
    "objectID": "publications/proceedings/wolfert2021.html#abstract",
    "href": "publications/proceedings/wolfert2021.html#abstract",
    "title": "To rate or not to rate: Investigating evaluation methods for generated co-speech gestures",
    "section": "Abstract",
    "text": "Abstract\nWhile automatic performance metrics are crucial for machine learning of artificial human-like behaviour, the gold standard for evaluation remains human judgement. The subjective evaluation of artificial human-like behaviour in embodied conversational agents is however expensive and little is known about the quality of the data it returns. Two approaches to subjective evaluation can be largely distinguished, one relying on ratings, the other on pairwise comparisons. In this study we use co-speech gestures to compare the two against each other and answer questions about their appropriateness for evaluation of artificial behaviour. We consider their ability to rate quality, but also aspects pertaining to the effort of use and the time required to collect subjective data.We use crowd sourcing to rate the quality of co-speech gestures in avatars, assessing which method picks up more detail in subjective assessments. We compared gestures generated by three different machine learning models with various level of behavioural quality. We found that both approaches were able to rank the videos according to quality and that the ranking significantly correlated, showing that in terms of quality there is no preference of one method over the other. We also found that pairwise comparisons were slightly faster and came with improved inter-rater reliability, suggesting that for small-scale studies pairwise comparisons are to be favoured over ratings."
  },
  {
    "objectID": "publications/proceedings/lin2020a.html",
    "href": "publications/proceedings/lin2020a.html",
    "title": "Context-Dependent Models for Predicting and Characterizing Facial Expressiveness",
    "section": "",
    "text": "Lin, V., Girard, J. M., & Morency, L.-P. (2020). Context-dependent models for predicting and characterizing facial expressiveness. Proceedings of the 3rd Workshop on Affective Content Analysis Co-Located with the 34th AAAI Conference on Artificial Intelligence, 2614, 11–28."
  },
  {
    "objectID": "publications/proceedings/lin2020a.html#citation-apa-7",
    "href": "publications/proceedings/lin2020a.html#citation-apa-7",
    "title": "Context-Dependent Models for Predicting and Characterizing Facial Expressiveness",
    "section": "",
    "text": "Lin, V., Girard, J. M., & Morency, L.-P. (2020). Context-dependent models for predicting and characterizing facial expressiveness. Proceedings of the 3rd Workshop on Affective Content Analysis Co-Located with the 34th AAAI Conference on Artificial Intelligence, 2614, 11–28."
  },
  {
    "objectID": "publications/proceedings/lin2020a.html#abstract",
    "href": "publications/proceedings/lin2020a.html#abstract",
    "title": "Context-Dependent Models for Predicting and Characterizing Facial Expressiveness",
    "section": "Abstract",
    "text": "Abstract\nIn recent years, extensive research has emerged in affective computing on topics like automatic emotion recognition and determining the signals that characterize individual emotions. Much less studied, however, is expressiveness—the extent to which someone shows any feeling or emotion. Expressiveness is related to personality and mental health and plays a crucial role in social interaction. As such, the ability to automatically detect or predict expressiveness can facilitate significant advancements in areas ranging from psychiatric care to artificial social intelligence. Motivated by these potential applications, we present an extension of the BP4D+ dataset with human ratings of expressiveness and develop methods for (1) automatically predicting expressiveness from visual data and (2) defining relationships between interpretable visual signals and expressiveness. In addition, we study the emotional context in which expressiveness occurs and hypothesize that different sets of signals are indicative of expressiveness in different con-texts (e.g., in response to surprise or in response to pain). Analysis of our statistical models confirms our hypothesis. Consequently, by looking at expressiveness separately in distinct emotional contexts, our predictive models show significant improvements over baselines and achieve com-parable results to human performance in terms of correlation with the ground truth."
  },
  {
    "objectID": "publications/proceedings/lin2020a.html#awards",
    "href": "publications/proceedings/lin2020a.html#awards",
    "title": "Context-Dependent Models for Predicting and Characterizing Facial Expressiveness",
    "section": "Awards",
    "text": "Awards\nThis paper won the Best Paper Award at the Workshop on Affective Content Analysis."
  },
  {
    "objectID": "publications/proceedings/mcduff2019.html",
    "href": "publications/proceedings/mcduff2019.html",
    "title": "Democratizing psychological insights from analysis of nonverbal behavior",
    "section": "",
    "text": "McDuff, D., & Girard, J. M. (2019). Democratizing psychological insights from analysis of nonverbal behavior. Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 220–226."
  },
  {
    "objectID": "publications/proceedings/mcduff2019.html#citation-apa-7",
    "href": "publications/proceedings/mcduff2019.html#citation-apa-7",
    "title": "Democratizing psychological insights from analysis of nonverbal behavior",
    "section": "",
    "text": "McDuff, D., & Girard, J. M. (2019). Democratizing psychological insights from analysis of nonverbal behavior. Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction (ACII), 220–226."
  },
  {
    "objectID": "publications/proceedings/mcduff2019.html#abstract",
    "href": "publications/proceedings/mcduff2019.html#abstract",
    "title": "Democratizing psychological insights from analysis of nonverbal behavior",
    "section": "Abstract",
    "text": "Abstract\nThe affective computing community has invested heavily in building automated tools for the analysis of facial behavior and the expression of emotion. These tools present a valuable, but largely untapped, opportunity for social scientists to perform observational analyses of nonverbal behavior at very large scale. Various tech companies are collecting huge corpora of images and videos from around the world that could be used to study important scientific questions. However, privacy restrictions and intellectual property concerns render these data inaccessible to most academics. Unfortunately, this limits the potential for scientific advancement and leads to the consolidation of data and opportunity into the hands of a few powerful institutions. In this paper, we ask whether similar psychological insights can be gained by analyzing smaller, public datasets that are more within reach for academic researchers. As a proof-of-concept for this idea, we gather, analyze, and release a corpus of public images and metadata and use it to replicate recent psychological findings about smiling, gender, and culture. In so doing, we provide evidence that psychological insights can indeed by democratized through the automated analysis of nonverbal behavior."
  },
  {
    "objectID": "publications/proceedings/vail2022.html",
    "href": "publications/proceedings/vail2022.html",
    "title": "Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment",
    "section": "",
    "text": "Vail, A. K., Girard, J. M., Bylsma, L. M., Cohn, J. F., Fournier, J., Swartz, H. A., & Morency, L.-P. (2022). Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment. Proceedings of the 24th ACM International Conference on Multimodal Interaction (ICMI), 487–494."
  },
  {
    "objectID": "publications/proceedings/vail2022.html#citation-apa-7",
    "href": "publications/proceedings/vail2022.html#citation-apa-7",
    "title": "Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment",
    "section": "",
    "text": "Vail, A. K., Girard, J. M., Bylsma, L. M., Cohn, J. F., Fournier, J., Swartz, H. A., & Morency, L.-P. (2022). Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment. Proceedings of the 24th ACM International Conference on Multimodal Interaction (ICMI), 487–494."
  },
  {
    "objectID": "publications/proceedings/vail2022.html#abstract",
    "href": "publications/proceedings/vail2022.html#abstract",
    "title": "Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment",
    "section": "Abstract",
    "text": "Abstract\nThe relationship between a therapist and their client is one of the most critical determinants of successful therapy. The working alliance is a multifaceted concept capturing the collaborative aspect of the therapist-client relationship; a strong working alliance has been extensively linked to many positive therapeutic outcomes. Although therapy sessions are decidedly multimodal interactions, the language modality is of particular interest given its recognized relationship to similar dyadic concepts such as rapport, cooperation, and affiliation. Specifically, in this work we study language entrainment, which measures how much the therapist and client adapt toward each other’s use of language over time. Despite the growing body of work in this area, however, relatively few studies examine causal relationships between human behavior and these relationship metrics: does an individual’s perception of their partner affect how they speak, or does how they speak affect their perception? We explore these questions in this work through the use of structural equation modeling (SEM) techniques, which allow for both multilevel and temporal modeling of the relationship between the quality of the therapist-client working alliance and the participants’ language entrainment. In our first experiment, we demonstrate that these techniques perform well in comparison to other common machine learning models, with the added benefits of interpretability and causal analysis. In our second analysis, we interpret the learned models to examine the relationship between working alliance and language entrainment and address our exploratory research questions. The results reveal that a therapist’s language entrainment can have a significant impact on the client’s perception of the working alliance, and that the client’s language entrainment is a strong indicator of their perception of the working alliance. We discuss the implications of these results and consider several directions for future work in multimodality."
  },
  {
    "objectID": "publications/proceedings/girard2017c.html",
    "href": "publications/proceedings/girard2017c.html",
    "title": "Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses",
    "section": "",
    "text": "Girard, J. M., & McDuff, D. (2017). Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses. Proceedings of the 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 719–726."
  },
  {
    "objectID": "publications/proceedings/girard2017c.html#citation-apa-7",
    "href": "publications/proceedings/girard2017c.html#citation-apa-7",
    "title": "Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses",
    "section": "",
    "text": "Girard, J. M., & McDuff, D. (2017). Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses. Proceedings of the 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 719–726."
  },
  {
    "objectID": "publications/proceedings/girard2017c.html#abstract",
    "href": "publications/proceedings/girard2017c.html#abstract",
    "title": "Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses",
    "section": "Abstract",
    "text": "Abstract\nFacial behavior is a valuable source of information about an individual’s feelings and intentions. However, many factors combine to influence and moderate facial behavior including personality, gender, context, and culture. Due to the high cost of traditional observational methods, the relationship between culture and facial behavior is not well-understood. In the current study, we explored the sociocultural factors that influence facial behavior using large-scale observational analyses. We developed and implemented an algorithm to automatically analyze the smiling of 866,726 participants across 31 different countries. We found that participants smiled more when from a country that is higher in individualism, has a lower population density, and has a long history of immigration diversity (i.e., historical heterogeneity). Our findings provide the first evidence that historical heterogeneity predicts actual smiling behavior. Furthermore, they converge with previous findings using selfreport methods. Taken together, these findings support the theory that historical heterogeneity explains, and may even contribute to the development of, permissive cultural display rules that encourage the open expression of emotion."
  },
  {
    "objectID": "publications/articles/girard2015a.html",
    "href": "publications/articles/girard2015a.html",
    "title": "Spontaneous facial expression in unscripted social interactions can be measured automatically",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Sayette, M. A., & De la Torre, F. (2015). Spontaneous facial expression in unscripted social interactions can be measured automatically. Behavior Research Methods, 47(4), 1136–1147."
  },
  {
    "objectID": "publications/articles/girard2015a.html#citation-apa-7",
    "href": "publications/articles/girard2015a.html#citation-apa-7",
    "title": "Spontaneous facial expression in unscripted social interactions can be measured automatically",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Sayette, M. A., & De la Torre, F. (2015). Spontaneous facial expression in unscripted social interactions can be measured automatically. Behavior Research Methods, 47(4), 1136–1147."
  },
  {
    "objectID": "publications/articles/girard2015a.html#abstract",
    "href": "publications/articles/girard2015a.html#abstract",
    "title": "Spontaneous facial expression in unscripted social interactions can be measured automatically",
    "section": "Abstract",
    "text": "Abstract\nMethods to assess individual facial actions have potential to shed light on important behavioral phenomena ranging from emotion and social interaction to psychological disorders and health. However, manual coding of such actions is labor intensive and requires extensive training. To date, establishing reliable automated coding of unscripted facial actions has been a daunting challenge impeding development of psychological theories and applications requiring facial expression assessment. It is therefore essential that automated coding systems be developed with enough precision and robustness to ease the burden of manual coding in challenging data involving variation in participant gender, ethnicity, head pose, speech, and occlusion. We report a major advance in automated coding of spontaneous facial actions during an unscripted social interaction involving three strangers. For each participant (n = 80, 47 % women, 15 % Nonwhite), 25 facial action units (AUs) were manually coded from video using the Facial Action Coding System. Twelve AUs occurred more than 3 % of the time and were processed using automated FACS coding. Automated coding showed very strong reliability for the proportion of time that each AU occurred (mean intraclass correlation = 0.89), and the more stringent criterion of frame-by-frame reliability was moderate to strong (mean Matthew’s correlation = 0.61). With few exceptions, differences in AU detection related to gender, ethnicity, pose, and average pixel intensity were small. Fewer than 6 % of frames could be coded manually but not automatically. These findings suggest automated FACS coding has progressed sufficiently to be applied to observational research in emotion and related areas of study."
  },
  {
    "objectID": "publications/articles/girard2015b.html",
    "href": "publications/articles/girard2015b.html",
    "title": "Estimating smile intensity: A better way",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., & De la Torre, F. (2015). Estimating smile intensity: A better way. Pattern Recognition Letters, 66, 13–21."
  },
  {
    "objectID": "publications/articles/girard2015b.html#citation-apa-7",
    "href": "publications/articles/girard2015b.html#citation-apa-7",
    "title": "Estimating smile intensity: A better way",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., & De la Torre, F. (2015). Estimating smile intensity: A better way. Pattern Recognition Letters, 66, 13–21."
  },
  {
    "objectID": "publications/articles/girard2015b.html#abstract",
    "href": "publications/articles/girard2015b.html#abstract",
    "title": "Estimating smile intensity: A better way",
    "section": "Abstract",
    "text": "Abstract\nBoth the occurrence and intensity of facial expressions are critical to what the face reveals. While much progress has been made towards the automatic detection of facial expression occurrence, controversy exists about how to estimate expression intensity. The most straight-forward approach is to train multiclass or regression models using intensity ground truth. However, collecting intensity ground truth is even more time consuming and expensive than collecting binary ground truth. As a shortcut, some researchers have proposed using the decision values of binary-trained maximum margin classifiers as a proxy for expression intensity. We provide empirical evidence that this heuristic is flawed in practice as well as in theory. Unfortunately, there are no shortcuts when it comes to estimating smile intensity: researchers must take the time to collect and train on intensity ground truth. However, if they do so, high reliability with expert human coders can be achieved. Intensity-trained multiclass and regression models outperformed binary-trained classifier decision values on smile intensity estimation across multiple databases and methods for feature extraction and dimensionality reduction. Multiclass models even outperformed binary–trained classifiers on smile occurrence detection."
  },
  {
    "objectID": "publications/articles/girard2014b.html",
    "href": "publications/articles/girard2014b.html",
    "title": "CARMA: Software for continuous affect rating and media annotation",
    "section": "",
    "text": "Girard, J. M. (2014). CARMA: Software for continuous affect rating and media annotation. Journal of Open Research Software, 2(1), e5–e5."
  },
  {
    "objectID": "publications/articles/girard2014b.html#citation-apa-7",
    "href": "publications/articles/girard2014b.html#citation-apa-7",
    "title": "CARMA: Software for continuous affect rating and media annotation",
    "section": "",
    "text": "Girard, J. M. (2014). CARMA: Software for continuous affect rating and media annotation. Journal of Open Research Software, 2(1), e5–e5."
  },
  {
    "objectID": "publications/articles/girard2014b.html#abstract",
    "href": "publications/articles/girard2014b.html#abstract",
    "title": "CARMA: Software for continuous affect rating and media annotation",
    "section": "Abstract",
    "text": "Abstract\nCARMA is a media annotation program that collects continuous ratings while displaying audio and video files. It is designed to be highly user-friendly and easily customizable. Based on Gottman and Levenson’s affect rating dial, CARMA enables researchers and study participants to provide moment-by-moment ratings of multimedia files using a computer mouse or keyboard. The rating scale can be configured on a number of parameters including the labels for its upper and lower bounds, its numerical range, and its visual representation. Annotations can be displayed alongside the multimedia file and saved for easy import into statistical analysis software. CARMA provides a tool for researchers in affective computing, human-computer interaction, and the social sciences who need to capture the unfolding of subjective experience and observable behavior over time."
  },
  {
    "objectID": "publications/articles/girard2014c.html",
    "href": "publications/articles/girard2014c.html",
    "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
    "section": "",
    "text": "Zhang, X., Yin, L., Cohn, J. F., Canavan, S., Reale, M., Horowitz, A., Liu, P., & Girard, J. M. (2014). BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database. Image and Vision Computing, 32(10), 692–706."
  },
  {
    "objectID": "publications/articles/girard2014c.html#citation-apa-7",
    "href": "publications/articles/girard2014c.html#citation-apa-7",
    "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
    "section": "",
    "text": "Zhang, X., Yin, L., Cohn, J. F., Canavan, S., Reale, M., Horowitz, A., Liu, P., & Girard, J. M. (2014). BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database. Image and Vision Computing, 32(10), 692–706."
  },
  {
    "objectID": "publications/articles/girard2014c.html#abstract",
    "href": "publications/articles/girard2014c.html#abstract",
    "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
    "section": "Abstract",
    "text": "Abstract\nFacial expression is central to human experience. Its efficiency and valid measurement are challenges that automated facial image analysis seeks to address. Most publically available databases are limited to 2D static images or video of posed facial behavior. Because posed and un-posed (aka “spontaneous”) facial expressions differ along several dimensions including complexity and timing, well-annotated video of un-posed facial behavior is needed. Moreover, because the face is a three-dimensional deformable object, 2D video may be insufficient, and therefore 3D video archives are required. We present a newly developed 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains. To the best of our knowledge, this new database is the first of its kind for the public. The work promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action."
  },
  {
    "objectID": "publications/articles/girard2014a.html",
    "href": "publications/articles/girard2014a.html",
    "title": "Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., Hammal, Z., & Rosenwald, D. P. (2014). Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses. Image and Vision Computing, 32(10), 641–647."
  },
  {
    "objectID": "publications/articles/girard2014a.html#citation-apa-7",
    "href": "publications/articles/girard2014a.html#citation-apa-7",
    "title": "Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., Hammal, Z., & Rosenwald, D. P. (2014). Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses. Image and Vision Computing, 32(10), 641–647."
  },
  {
    "objectID": "publications/articles/girard2014a.html#abstract",
    "href": "publications/articles/girard2014a.html#abstract",
    "title": "Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses",
    "section": "Abstract",
    "text": "Abstract\nThe relationship between nonverbal behavior and severity of depression was investigated by following depressed participants over the course of treatment and video recording a series of clinical interviews. Facial expressions and head pose were analyzed from video using manual and automatic systems. Both systems were highly consistent for FACS action units (AUs) and showed similar effects for change over time in depression severity. When symptom severity was high, participants made fewer affiliative facial expressions (AUs 12 and 15) and more non-affiliative facial expressions (AU 14). Participants also exhibited diminished head motion (i.e., amplitude and velocity) when symptom severity was high. These results are consistent with the Social Withdrawal hypothesis: that depressed individuals use nonverbal behavior to maintain or increase interpersonal distance. As individuals recover, they send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and revealed the same pattern of findings suggests that automatic facial expression analysis may be ready to relieve the burden of manual coding in behavioral and clinical science."
  },
  {
    "objectID": "publications/articles/caumiant2025.html",
    "href": "publications/articles/caumiant2025.html",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "",
    "text": "Caumiant, E. P., Kang, D., Girard, J. M., & Fairbairn, C. E. (in press). Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication. Psychology of Addictive Behaviors."
  },
  {
    "objectID": "publications/articles/caumiant2025.html#citation-apa-7",
    "href": "publications/articles/caumiant2025.html#citation-apa-7",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "",
    "text": "Caumiant, E. P., Kang, D., Girard, J. M., & Fairbairn, C. E. (in press). Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication. Psychology of Addictive Behaviors."
  },
  {
    "objectID": "publications/articles/caumiant2025.html#abstract",
    "href": "publications/articles/caumiant2025.html#abstract",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "Abstract",
    "text": "Abstract\nObjective: Emotion measurement is central to capturing acute alcohol reinforcement and so to informing models of alcohol use disorder etiology. Yet our understanding of how alcohol impacts emotion as assessed across diverse response modalities remains incomplete. The present study leverages a social alcohol-administration paradigm to assess drinking-related emotions, aiming to elucidate impacts of intoxication on self-reported versus behaviorally expressed emotion. Method: Participants (N = 60; Mage = 22.5; 50% male; 55% White) attended two counterbalanced laboratory sessions, on one of which they were administered an alcoholic beverage (target blood alcohol content .08%) and on the other a nonalcoholic control beverage. Participants in both conditions were accurately informed of beverage contents and consumed study beverages in assigned groups of three while their behavior was videotaped. Emotion was assessed via self-report as well as continuous coding of facial muscle movements. Results: The relationship between self-reported and behaviorally expressed emotion diverged significantly across beverage conditions: positive affect: b = −0.174, t = −2.36, p = .022; negative affect, b = 0.4319, t = 2.37, p = .021. Specifically, self-reports and behavioral displays converged among sober but not intoxicated participants. Further, alcohol’s effects on positive facial displays remained significant in models controlling for self-reported positive and negative emotion, with alcohol enhancing Duchenne smiles 20% beyond effects captured via self-reports, pointing to unique effects of alcohol on behavioral indicators of positive emotion. Conclusions: Findings highlight effects of acute intoxication on the convergence and divergence of emotion measures, thus informing our understanding of measures for capturing emotions that are most proximal to drinking and thus most immediately reinforcing of alcohol consumption."
  },
  {
    "objectID": "publications/articles/caumiant2025.html#impact-statement",
    "href": "publications/articles/caumiant2025.html#impact-statement",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "Impact Statement",
    "text": "Impact Statement\nThis study indicates that, while participants’ self-reported and behaviorally expressed emotion held consistent when not drinking, these measures diverged following alcohol consumption. Results further indicated alcohol’s effects on positive emotion were not fully captured by self-report, a tool that has been relied on throughout addiction research. As a result, this study highlights the importance of using multiple methods of capturing emotion, including behavioral and self-report methods, when studying alcohol’s effects on emotion and drinking behaviors."
  },
  {
    "objectID": "publications/articles/swartz2023.html",
    "href": "publications/articles/swartz2023.html",
    "title": "Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth",
    "section": "",
    "text": "Swartz, H. A., Bylsma, L. M., Fournier, J. C., Girard, J. M., Spotts, C., Cohn, J. F., & Morency, L.-P. (2023). Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth. Journal of Affective Disorders, 333, 543–552."
  },
  {
    "objectID": "publications/articles/swartz2023.html#citation-apa-7",
    "href": "publications/articles/swartz2023.html#citation-apa-7",
    "title": "Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth",
    "section": "",
    "text": "Swartz, H. A., Bylsma, L. M., Fournier, J. C., Girard, J. M., Spotts, C., Cohn, J. F., & Morency, L.-P. (2023). Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth. Journal of Affective Disorders, 333, 543–552."
  },
  {
    "objectID": "publications/articles/swartz2023.html#abstract",
    "href": "publications/articles/swartz2023.html#abstract",
    "title": "Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth",
    "section": "Abstract",
    "text": "Abstract\nBackground: Expert consensus guidelines recommend Cognitive Behavioral Therapy (CBT) and Interpersonal Psychotherapy (IPT), interventions that were historically delivered face-to-face, as first-line treatments for Major Depressive Disorder (MDD). Despite the ubiquity of telehealth following the COVID-19 pandemic, little is known about differential outcomes with CBT versus IPT delivered in-person (IP) or via telehealth (TH) or whether working alliance is affected.\nMethods: Adults meeting DSM-5 criteria for MDD were randomly assigned to either 8 sessions of IPT or CBT (group). Mid-trial, COVID-19 forced a change of therapy delivery from IP to TH (study phase). We compared changes in Hamilton Rating Scale for Depression (HRSD-17) and Working Alliance Inventory (WAI) scores for individuals by group and phase: CBT-IP (n = 24), CBT-TH (n = 11), IPT-IP (n = 25) and IPT-TH (n = 17).\nResults: HRSD-17 scores declined significantly from pre to post treatment (pre: M = 17.7, SD = 4.4 vs. post: M = 11.7, SD = 5.9; p &lt; .001; d = 1.45) without significant group or phase effects. WAI scores did not differ by group or phase. Number of completed therapy sessions was greater for TH (M = 7.8, SD = 1.2) relative to IP (M = 7.2, SD = 1.6) (Mann-Whitney U = 387.50, z = 2.24, p = .025).\nLimitations: Participants were not randomly assigned to IP versus TH. Sample size is small.\nConclusions: This study provides preliminary evidence supporting the efficacy of both brief IPT and CBT, delivered by either TH or IP, for depression. It showed that working alliance is preserved in TH, and delivery via TH may improve therapy adherence. Prospective, randomized controlled trials are needed to definitively test efficacy of brief IPT and CBT delivered via TH versus IP."
  },
  {
    "objectID": "publications/articles/shepherd2017.html",
    "href": "publications/articles/shepherd2017.html",
    "title": "Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents",
    "section": "",
    "text": "Shepherd, L. M., Sly, K. F., & Girard, J. M. (2017). Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents. Journal of Adolescence, 61, 50–63."
  },
  {
    "objectID": "publications/articles/shepherd2017.html#citation-apa-7",
    "href": "publications/articles/shepherd2017.html#citation-apa-7",
    "title": "Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents",
    "section": "",
    "text": "Shepherd, L. M., Sly, K. F., & Girard, J. M. (2017). Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents. Journal of Adolescence, 61, 50–63."
  },
  {
    "objectID": "publications/articles/shepherd2017.html#abstract",
    "href": "publications/articles/shepherd2017.html#abstract",
    "title": "Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents",
    "section": "Abstract",
    "text": "Abstract\nThe purpose of this study was to identify predictors of sexual behavior and condom use in African American adolescents, as well as to evaluate the effectiveness of comprehensive sexuality and abstinence-only education to reduce adolescent sexual behavior and increase condom use. Participants included 450 adolescents aged 12-14 years in the southern United States. Regression analyses showed favorable attitudes toward sexual behavior and social norms significantly predicted recent sexual behavior, and favorable attitudes toward condoms significantly predicted condom usage. Self-efficacy was not found to be predictive of adolescents’ sexual behavior or condom use. There were no significant differences in recent sexual behavior based on type of sexuality education. Adolescents who received abstinence-only education had reduced favorable attitudes toward condom use, and were more likely to have unprotected sex than the comparison group. Findings suggest that adolescents who receive abstinence-only education are at greater risk of engaging in unprotected sex."
  },
  {
    "objectID": "publications/articles/grove2019.html",
    "href": "publications/articles/grove2019.html",
    "title": "Narcissistic admiration and rivalry: An interpersonal approach to construct validation",
    "section": "",
    "text": "Grove, J. L., Smith, T. W., Girard, J. M., & Wright, A. G. (2019). Narcissistic admiration and rivalry: An interpersonal approach to construct validation. Journal of Personality Disorders, 33(6), 751–775."
  },
  {
    "objectID": "publications/articles/grove2019.html#citation-apa-7",
    "href": "publications/articles/grove2019.html#citation-apa-7",
    "title": "Narcissistic admiration and rivalry: An interpersonal approach to construct validation",
    "section": "",
    "text": "Grove, J. L., Smith, T. W., Girard, J. M., & Wright, A. G. (2019). Narcissistic admiration and rivalry: An interpersonal approach to construct validation. Journal of Personality Disorders, 33(6), 751–775."
  },
  {
    "objectID": "publications/articles/grove2019.html#abstract",
    "href": "publications/articles/grove2019.html#abstract",
    "title": "Narcissistic admiration and rivalry: An interpersonal approach to construct validation",
    "section": "Abstract",
    "text": "Abstract\nThe present study applied the interpersonal perspective in testing the narcissistic admiration and rivalry concept (NARC) and examining the construct validity of the corresponding Narcissistic Admiration and Rivalry Questionnaire (NARQ). Two undergraduate samples (Sample 1: N = 290; Sample 2: N = 188) completed self-report measures of interpersonal processes based in the interpersonal circumplex (IPC), as well as measures of related constructs. In examining IPC correlates, we used a novel bootstrapping approach to determine if admiration and rivalry related to differing interpersonal profiles. Consistent with our hypotheses, admiration was distinctly related to generally agentic (i.e., dominant) interpersonal processes, whereas rivalry generally reflected (low) communal (i.e., hostile) interpersonal processes. Furthermore, NARQ-admiration and NARQ-rivalry related to generally adaptive and maladaptive aspects of status-related constructs, emotional, personality, and social adjustment, respectively. This research provides further support for the NARC, as well as construct validation for the NARQ."
  },
  {
    "objectID": "publications/articles/gerber2019.html",
    "href": "publications/articles/gerber2019.html",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "",
    "text": "Gerber, A. H., Girard, J. M., Scott, S. B., & Lerner, M. D. (2019). Alexithymia – Not autism – is associated with frequency of social interactions in adults. Behaviour Research and Therapy, 123, 103477."
  },
  {
    "objectID": "publications/articles/gerber2019.html#citation-apa-7",
    "href": "publications/articles/gerber2019.html#citation-apa-7",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "",
    "text": "Gerber, A. H., Girard, J. M., Scott, S. B., & Lerner, M. D. (2019). Alexithymia – Not autism – is associated with frequency of social interactions in adults. Behaviour Research and Therapy, 123, 103477."
  },
  {
    "objectID": "publications/articles/gerber2019.html#abstract",
    "href": "publications/articles/gerber2019.html#abstract",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "Abstract",
    "text": "Abstract\n\nObjective\nWhile much is known about the quality of social behavior among neurotypical individuals and those with autism spectrum disorder (ASD), little work has evaluated quantity of social interactions. This study used ecological momentary assessment (EMA) to quantify in vivo daily patterns of social interaction in adults as a function of demographic and clinical factors.\n\n\nMethod\nAdults with and without ASD (\\(N_{ASD}=23\\), \\(N_{NT}=52\\)) were trained in an EMA protocol to report their social interactions via smartphone over one week. Participants completed measures of IQ, ASD symptom severity and alexithymia symptom severity.\n\n\nResults\nCyclical multilevel models were used to account for nesting of observations. Results suggest a daily cyclical pattern of social interaction that was robust to ASD and alexithymia symptoms. Adults with ASD did not have fewer social interactions than neurotypical peers; however, severity of alexithymia symptoms predicted fewer social interactions regardless of ASD status.\n\n\nConclusions\nThese findings suggest that alexithymia, not ASD severity, may drive social isolation and highlight the need to reevaluate previously accepted notions regarding differences in social behavior utilizing modern methods."
  },
  {
    "objectID": "publications/articles/girard2021a.html",
    "href": "publications/articles/girard2021a.html",
    "title": "Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Yin, L., & Morency, L.-P. (2021). Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion. Affective Science, 2(1), 32–47."
  },
  {
    "objectID": "publications/articles/girard2021a.html#citation-apa-7",
    "href": "publications/articles/girard2021a.html#citation-apa-7",
    "title": "Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Yin, L., & Morency, L.-P. (2021). Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion. Affective Science, 2(1), 32–47."
  },
  {
    "objectID": "publications/articles/girard2021a.html#abstract",
    "href": "publications/articles/girard2021a.html#abstract",
    "title": "Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion",
    "section": "Abstract",
    "text": "Abstract\nThe common view of emotional expressions is that certain configurations of facial-muscle movements reliably reveal certain categories of emotion. The principal exemplar of this view is the Duchenne smile, a configuration of facial-muscle movements (i.e., smiling with eye constriction) that has been argued to reliably reveal genuine positive emotion. In this paper, we formalized a list of hypotheses that have been proposed regarding the Duchenne smile, briefly reviewed the literature weighing on these hypotheses, identified limitations and unanswered questions, and conducted two empirical studies to begin addressing these limitations and answering these questions. Both studies analyzed a database of 751 smiles observed while 136 participants completed experimental tasks designed to elicit amusement, embarrassment, fear, and physical pain. Study 1 focused on participants’ self-reported positive emotion and Study 2 focused on how third-party observers would perceive videos of these smiles. Most of the hypotheses that have been proposed about the Duchenne smile were either contradicted by or only weakly supported by our data. Eye constriction did provide some information about experienced positive emotion, but this information was lacking in specificity, already provided by other smile characteristics, and highly dependent on context. Eye constriction provided more information about perceived positive emotion, including some unique information over other smile characteristics, but context was also important here as well. Overall, our results suggest that accurately inferring positive emotion from a smile requires more sophisticated methods than simply looking for the presence/absence (or even the intensity) of eye constriction."
  },
  {
    "objectID": "publications/articles/fairbairn2015.html",
    "href": "publications/articles/fairbairn2015.html",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "",
    "text": "Fairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental & Clinical Psychopharmacology, 23(4), 255–264."
  },
  {
    "objectID": "publications/articles/fairbairn2015.html#citation-apa-7",
    "href": "publications/articles/fairbairn2015.html#citation-apa-7",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "",
    "text": "Fairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental & Clinical Psychopharmacology, 23(4), 255–264."
  },
  {
    "objectID": "publications/articles/fairbairn2015.html#abstract",
    "href": "publications/articles/fairbairn2015.html#abstract",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "Abstract",
    "text": "Abstract\nMen and women differ dramatically in their rates of alcohol use disorder (AUD), and researchers have long been interested in identifying mechanisms underlying male vulnerability to problem drinking. Surveys suggest that social processes underlie sex differences in drinking patterns, with men reporting greater social enhancement from alcohol than women, and all-male social drinking contexts being associated with particularly high rates of hazardous drinking. But experimental evidence for sex differences in social-emotional response to alcohol has heretofore been lacking. Research using larger sample sizes, a social context, and more sensitive measures of alcohol’s rewarding effects may be necessary to better understand sex differences in the etiology of AUD. This study explored the acute effects of alcohol during social exchange on speech volume –an objective measure of social-emotional experience that was reliably captured at the group level. Social drinkers (360 male; 360 female) consumed alcohol (.82g/kg males; .74g/kg females), placebo, or a no-alcohol control beverage in groups of three over 36-minutes. Within each of the three beverage conditions, equal numbers of groups consisted of all males, all females, 2 females and 1 male, and 1 female and 2 males. Speech volume was monitored continuously throughout the drink period, and group volume emerged as a robust correlate of self-report and facial indexes of social reward. Notably, alcohol-related increases in group volume were observed selectively in all-male groups but not in groups containing any females. Results point to social enhancement as a promising direction for research exploring factors underlying sex differences in problem drinking."
  },
  {
    "objectID": "publications/articles/sewall2021.html",
    "href": "publications/articles/sewall2021.html",
    "title": "A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder",
    "section": "",
    "text": "Sewall, C. J. R., Girard, J. M., Merranko, J., Hafeman, D., Goldstein, B. I., Strober, M., Hower, H., Weinstock, L. M., Yen, S., Ryan, N. D., Keller, M. B., Liao, F., Diler, R. S., Gill, M. K., Axelson, D., Birmaher, B., & Goldstein, T. R. (2021). A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder. The Journal of Child Psychology and Psychiatry, 62(7), 905–9115."
  },
  {
    "objectID": "publications/articles/sewall2021.html#citation-apa-7",
    "href": "publications/articles/sewall2021.html#citation-apa-7",
    "title": "A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder",
    "section": "",
    "text": "Sewall, C. J. R., Girard, J. M., Merranko, J., Hafeman, D., Goldstein, B. I., Strober, M., Hower, H., Weinstock, L. M., Yen, S., Ryan, N. D., Keller, M. B., Liao, F., Diler, R. S., Gill, M. K., Axelson, D., Birmaher, B., & Goldstein, T. R. (2021). A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder. The Journal of Child Psychology and Psychiatry, 62(7), 905–9115."
  },
  {
    "objectID": "publications/articles/sewall2021.html#abstract",
    "href": "publications/articles/sewall2021.html#abstract",
    "title": "A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nYouth with bipolar disorder (BD) are at high risk for suicidal thoughts and behaviors and frequently experience interpersonal impairment, which is a risk factor for suicide. Yet, no study to date has examined the longitudinal associations between relationship quality in family/peer domains and suicidal thoughts and behaviors among youth with BD. Thus, we investigated how between-person differences – reflecting the average relationship quality across time – and within-person changes, reflecting recent fluctuations in relationship quality, act as distal and/or proximal risk factors for suicidal ideation (SI) and suicide attempts.\n\n\nMethods\nWe used longitudinal data from the Course and Outcome of Bipolar Youth Study (N = 413). Relationship quality variables were decomposed into stable (i.e., average) and varying (i.e., recent) components and entered, along with major clinical covariates, into separate Bayesian multilevel models predicting SI and suicide attempt. We also examined how the relationship quality effects interacted with age and sex.\n\n\nResults\nPoorer average relationship quality with parents (β = −.33, 95% Bayesian highest density interval (HDI) [−0.54, −0.11]) or friends (β = −.33, 95% HDI [−0.55, −0.11]) was longitudinally associated with increased risk of SI but not suicide attempt. Worsening recent relationship quality with parents (β = −.10, 95% HDI [−0.19, −0.03]) and, to a lesser extent, friends (β = −.06, 95% HDI [−0.15, 0.03]) was longitudinally associated with increased risk of SI, but only worsening recent relationship quality with parents was also associated with increased risk of suicide attempt (β = −.15, 95% HDI [−0.31, 0.01]). The effects of certain relationship quality variables were moderated by gender but not age.\n\n\nConclusions\nAmong youth with BD, having poorer average relationship quality with peers and/or parents represents a distal risk factor for SI but not suicide attempts. Additionally, worsening recent relationship quality with parents may be a time-sensitive indicator of increased risk for SI or suicide attempt."
  },
  {
    "objectID": "publications/articles/girard2016a.html",
    "href": "publications/articles/girard2016a.html",
    "title": "A primer on observational measurement",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2016). A primer on observational measurement. Assessment, 23(4), 404–413."
  },
  {
    "objectID": "publications/articles/girard2016a.html#citation-apa-7",
    "href": "publications/articles/girard2016a.html#citation-apa-7",
    "title": "A primer on observational measurement",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2016). A primer on observational measurement. Assessment, 23(4), 404–413."
  },
  {
    "objectID": "publications/articles/girard2016a.html#abstract",
    "href": "publications/articles/girard2016a.html#abstract",
    "title": "A primer on observational measurement",
    "section": "Abstract",
    "text": "Abstract\nObservational measurement plays an integral role in a variety of scientific endeavors within biology, psychology, sociology, education, medicine, and marketing. The current article provides an interdisciplinary primer on observational measurement; in particular, it highlights recent advances in observational methodology and the challenges that accompany such growth. First, we detail the various types of instrument that can be used to standardize measurements across observers. Second, we argue for the importance of validity in observational measurement and provide several approaches to validation based on contemporary validity theory. Third, we outline the challenges currently faced by observational researchers pertaining to measurement drift, observer reactivity, reliability analysis, and time/expense. Fourth, we describe recent advances in computer-assisted measurement, fully automated measurement, and statistical data analysis. Finally, we identify several key directions for future observational research to explore."
  },
  {
    "objectID": "publications/articles/rincon2024.html",
    "href": "publications/articles/rincon2024.html",
    "title": "Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics",
    "section": "",
    "text": "Rincon Caicedo, M., Girard, J. M., Punt, S. E., Giovanetti, A. K., & Ilardi, S. S. (in press). Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics. Journal of Latinx Psychology."
  },
  {
    "objectID": "publications/articles/rincon2024.html#citation-apa-7",
    "href": "publications/articles/rincon2024.html#citation-apa-7",
    "title": "Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics",
    "section": "",
    "text": "Rincon Caicedo, M., Girard, J. M., Punt, S. E., Giovanetti, A. K., & Ilardi, S. S. (in press). Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics. Journal of Latinx Psychology."
  },
  {
    "objectID": "publications/articles/rincon2024.html#abstract",
    "href": "publications/articles/rincon2024.html#abstract",
    "title": "Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics",
    "section": "Abstract",
    "text": "Abstract\nAs with many other racial and ethnic minorities, Hispanic Americans face substantial disparities in health care access and disease prevalence. The published literature on mental health disorders among Hispanic individuals, however, is not robust, and their experience of depressive disorders remains poorly understood. The construct of acculturation may help elucidate the risk of depression among Hispanic Americans and may inform the development of appropriate policy and treatment resources. We examined the degree to which acculturation may interact with key demographic variables (sex, age, socioeconomic status [SES], and Mexican ancestry) in accounting for depressive symptomatology among Hispanic Americans. We conducted a series of Bayesian generalized linear mixed models using data from the National Health and Nutrition Examination Survey to investigate the self-reported depressive symptomatology (measured by the Patient Health Questionnaire−9) of Mexican Americans and other Hispanic individuals and to examine possible effects of acculturation and demographics on depressive symptomatology in this population. Mexican Americans had substantially lower levels of depression than other Hispanic individuals. Acculturation was positively associated with depression severity, but this effect was moderated by sex and SES. High acculturation was more strongly linked to depression among men and those of high SES. Acculturation and several demographic factors were associated with depressive symptomatology among Hispanic individuals. Acculturation can be useful in understanding risk, developing culturally informed interventions, and implementing community-level changes to address the burden of Hispanic depression."
  },
  {
    "objectID": "publicationx/ph_20220616_splverse/index.html",
    "href": "publicationx/ph_20220616_splverse/index.html",
    "title": "Sykdomspulsen: An automated public health surveillance platform",
    "section": "",
    "text": "About the talk\nWatch the talk on YouTube\nSykdomspulsen is a real-time analysis and disease surveillance system designed at developed at the Norwegian Institute of Public Health (FHI). Sykdomspulsen processes new data collected from 15 data sources (e.g., covid-19 cases), runs 1000.000+ statistical analysis automatically for all locations (nation, county, municipality) in Norway, produces 1000+ reports and alerts for public health authorities and shares data to the public on GitHub.\nSykdomspulsen runs on a collection of R packages, the {splverse}. {splverse} is an ecosystem for infectious disease surveillance, from analysis planning, statistical analysis to reporting via visualization, shiny website and Rmarkdown generated reports. In this talk, Chi will present how Sykdomspulsen does public health real-time surveillance during the pandemic using R. Chi will introduce some of the core packages and illustrate how they work together, with an example using real surveillance data published daily on GitHub.\n\n\nAbout the speaker\nChi is currently working at the Sykdomspulsen team as a researcher and R developer, at the Norwegian Institute of Public Health. Before she joined Sykdomspulsen in the middle of the pandemic (2020), she was a PhD student at the Department of Biostatistics at University of Oslo (OCBE), working on hospital EHR data."
  },
  {
    "objectID": "publicationx/dummy_talk/index.html",
    "href": "publicationx/dummy_talk/index.html",
    "title": "My Dummy Talk",
    "section": "",
    "text": "This is a simple demo of a Quarto talk listing with an image, title, and subtitle."
  },
  {
    "objectID": "publicationx/community_20240710_camis/index.html",
    "href": "publicationx/community_20240710_camis/index.html",
    "title": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations",
    "section": "",
    "text": "2024.7.8-11, Salzburg, Austria. Conference link: UseR!\nStatisticians using multiple softwares (SAS, R, Python) will have found differences in analysis results that warrant further justification. Whilst some industries may accept results not being the same as long as they are “close”, the highly regulated pharmaceutical industry would require an identical match in results. Yet, discrepancies might still occur, and knowing the reasons (different methods, options, algorithms etc) is critical to the modern statistician and subsequent regulatory submissions.\nIn this talk I will introduce CAMIS: Comparing Analysis Method Implementations in Software. https://psiaims.github.io/CAMIS/ It is a joint-project between PHUSE, the R Validation Hub, PSI AIMS, R consortium and openstatsware. The aim of CAMIS is to investigate and document differences and similarities between different statistical softwares such as SAS and R. We use Quarto and Github to document methods, algorithms and comparisons between softwares through small case studies, and all articles are contributed by the community. In the transition from proprietary to open source technology in the industry, CAMIS can serve as a guidebook to navigate this process.\n\nkeywords: cross industry collaboration, multi-lingua, open-source, quarto"
  },
  {
    "objectID": "publicationx/ehr_20240918_betterehr/index.html",
    "href": "publicationx/ehr_20240918_betterehr/index.html",
    "title": "One step closer to better Electronic Health Records data",
    "section": "",
    "text": "Real-World Data (RWD) like Electronic Health Records (EHR) is crucial for understanding drug usage and various treatments and generating Real-World Evidence (RWE). Risk prediction has been a major application where EHR is used, and there is now a shift towards causal inference, which requires data of even higher quality. Patients undergo treatments (drugs, procedures) at various times during their hospital stays, yet the data being recorded are messy and error-prone for various reasons. Analysts spend significant amount of time to sit together with clinicians to identify and understand abnormal records, and unfortunately this process is challenging to automate.\nThis talk will use an example on antibiotics prescription and use at a Nordic hospital to illustrate how some EHR systems can improve for better clinical decision-making and better data for research. I will also introduce a pilot R package (ggehr) that facilitates visual exploration of EHR data, and how it can help reconstruct patient journeys and enable analysts to perform effective quality control."
  },
  {
    "objectID": "publicationx/ph_20230330_sp/index.html",
    "href": "publicationx/ph_20230330_sp/index.html",
    "title": "Public health surveillance and reporting",
    "section": "",
    "text": "Time and place: Mar. 30, 2023 12:00 PM–1:00 PM\nHybrid: Georg Sverdrups hus and Zoom\nEvent page"
  },
  {
    "objectID": "publicationx/ph_20230330_sp/index.html#about-the-topic",
    "href": "publicationx/ph_20230330_sp/index.html#about-the-topic",
    "title": "Public health surveillance and reporting",
    "section": "About the topic",
    "text": "About the topic\nSituational awareness is key to fast response during a public health emergency, such as COVID-19 pandemic. However, making disease surveillance reports that cover different geographical units for various metrics and data registries is both resource intensive and time consuming. Open source tools such as R packages, GitHub and Airflow can make this process automatic, reproducible and scalable.\nEvery day during the pandemic, Sykdomspulsen team at the Norwegian Institute of Public Health (FHI/NIPH) fetched data from more than 15 data sources, cleaned, censored datasets and carried out a wide range of statistical analyses. Over 1000 situational reports containing automated graphs and tables were produced before breakfast time.\nGrab you matpakke and join us for a presentation from Chi Zhang about how Sykdomspulsen team used and developed open source software to make public health surveillance and reporting more efficient, followed up by a discussion on the benefits and concerns of making these data public. We will end with an open Q&A session as usual!"
  },
  {
    "objectID": "publications1.html#upcoming",
    "href": "publications1.html#upcoming",
    "title": "Kundan Kumar",
    "section": "Upcoming",
    "text": "Upcoming\n\nR in Pharma\nExplore Real-World hospital Electronic Health Records data with ggehr\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n         \n          Publication\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n\n  \n    Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication\n    Caumiant, Kang, Girard, & Fairbairn\n    Psychology of Addictive Behaviors\n    (2025)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders\n    Butler, Christian, et al.\n    Behavior Research and Therapy\n    (2024)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample\n    Sprunger, Girard, & Chard\n    Journal of Traumatic Stress\n    (2024)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics\n    Rincon Caicedo, Girard, et al.\n    Journal of Latinx Psychology\n    (2024)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports\n    Baber, Hamilton, et al.\n    Sleep\n    (2024)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders\n    Chung, Girard, et al.\n    Journal of Psychopathology and Clinical Science\n    (2024)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Worth the weight: An examination of unstructured and structured data in graduate admissions\n    Adaryukov, Biernat, et al.\n    Decision\n    (2024)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    DynAMoS: The Dynamic Affective Movie Clip Database for Subjectivity Analysis\n    Girard, Tie, & Liebenthal\n    ACII\n    (2023)\n\n    Details\n\n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth\n    Swartz, Bylsma, Fournier, et al.\n    Journal of Affective Disorders\n    (2023)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Computational analysis of spoken language in acute psychosis and mania\n    Girard, Vail, Liebenthal, et al.\n    Schizophrenia Research\n    (2022)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Toward causal understanding of therapist-client relationships: A study of language modality and social entrainment\n    Vail, Girard, Bylsma, et al.\n    ICMI\n    (2022)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement\n    Van Oest & Girard\n    Psychological Methods\n    (2022)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder\n    Sewall, Girard, Merranko, et al.\n    The Journal of Child Psychology and Psychiatry\n    (2021)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions\n    Bowdring, Sayette, Girard, & Woods\n    Journal of Nonverbal Behavior\n    (2021)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion\n    Girard, Cohn, Yin, & Morency\n    Affective Science\n    (2021)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    To rate or not to rate: Investigating evaluation methods for generated co-speech gestures\n    Wolfert, Girard, Kucherenko, & Belpaeme\n    ICMI\n    (2021)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Context-Dependent Models for Predicting and Characterizing Facial Expressiveness\n    Lin, Girard, & Morency\n    WACA\n    (2020)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict\n    Hopwood, Harrison, Amole, et al.\n    Assessment\n    (2020)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Toward Multimodal Modeling of Emotional Expressiveness\n    Lin, Girard, Sayette, & Morency\n    ICMI\n    (2020)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Alexithymia – not autism – is associated with frequency of social interactions in adults\n    Gerber, Girard, Scott, & Lerner\n    Behaviour Research and Therapy\n    (2019)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Democratizing psychological insights from analysis of nonverbal behavior\n    McDuff & Girard\n    ACII\n    (2019)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Narcissistic admiration and rivalry: An interpersonal approach to construct validation\n    Grove, Smith, Girard, & Wright\n    Journal of Personality Disorders\n    (2019)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?\n    Girard, Shandar, Liu, et al.\n    ACII\n    (2019)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    DARMA: Software for dual axis rating and media annotation\n    Girard & Wright\n    Behavior Research Methods\n    (2018)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study\n    Pacella, Girard, Wright, et al.\n    Academic Emergency Medicine\n    (2018)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents\n    Shepherd, Sly, & Girard\n    Journal of Adolescence\n    (2017)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    FERA 2017 - Addressing head pose in the third facial expression recognition and analysis challenge\n    Valstar, Sánchez-Lozano, Cohn, et al.\n    FG\n    (2017)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Historical heterogeneity predicts smiling: Evidence from large-scale observational analyses\n    Girard & McDuff\n    FG\n    (2017)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Interpersonal problems across levels of the psychopathology hierarchy\n    Girard, Wright, Beeney, et al.\n    Comprehensive Psychiatry\n    (2017)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Large-scale observational evidence of cross-cultural differences in facial behavior\n    McDuff, Girard, & el Kaliouby\n    Journal of Nonverbal Behavior\n    (2017)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment\n    Ross, Girard, Wright, et al.\n    Psychological Assessment\n    (2017)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Sayette group formation task (GFT) spontaneous facial expression database\n    Girard, Chu, Jeni, et al.\n    FG\n    (2017)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    A primer on observational measurement\n    Girard & Cohn\n    Assessment\n    (2016)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Multimodal spontaneous emotion corpus for human behavior analysis\n    Zhang, Girard, Wu, et al.\n    CVPR\n    (2016)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Automated audiovisual depression analysis\n    Girard & Cohn\n    Current Opinion in Psychology\n    (2015)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Estimating smile intensity: A better way\n    Girard, Cohn, & De la Torre\n    Pattern Recognition Letters\n    (2015)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    FERA 2015 - Second Facial Expression Recognition and Analysis challenge\n    Valstar, Almaev, Girard, et al.\n    FG\n    (2015)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    How much training data for facial action unit detection?\n    Girard, Cohn, Jeni, et al.\n    FG\n    (2015)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Speech volume indexes gender differences in the social-emotional effects of alcohol\n    Fairbairn, Sayette, Amole, et al.\n    Experimental & Clinical Psychopharmacology\n    (2015)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Spontaneous facial expression in unscripted social interactions can be measured automatically\n    Girard, Cohn, Jeni, et al.\n    Behavior Research Methods\n    (2015)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database\n    Zhang, Yin, Cohn, et al.\n    Image and Vision Computing\n    (2014)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    CARMA: Software for continuous affect rating and media annotation\n    Girard \n    Journal of Open Research Software\n    (2014)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n      \n        Code\n      \n    \n\n    \n\n  \n\n  \n    Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses\n    Girard, Cohn, Mahoor, et al.\n    Image and Vision Computing\n    (2014)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n      \n        Paper\n      \n    \n\n    \n\n    \n\n  \n\n  \n    Continuous AU intensity estimation using localized, sparse facial feature space\n    Jeni, Girard, Cohn, & De la Torre\n    FG\n    (2013)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Social risk and depression: Evidence from manual and automatic facial expression analysis\n    Girard, Cohn, Mahoor, et al.\n    FG\n    (2013)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Criteria and metrics for thresholded AU detection\n    Girard & Cohn\n    ICCV\n    (2011)\n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Home\n    Publications\n  \n\n\nPublications\nThe following is a list of my research publications, including journal articles, conference papers, Workshops papers and preprints.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n      \n        Publication\n      \n      \n        Year\n      \n    \n  \n    \n      \n      \n    \n\n\n\n  \n    A Multi-Objective Optimization Framework for Carbon-Aware Smart Energy Management\n    Kundan Kumar\n    57th North American Power Symposium\n    (2025)\n    \n\n    Details\n\n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems\n    Kundan Kumar\n    IEEE Power & Energy Society General Meeting\n    (2025)\n    \n\n    Details\n\n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Bayesian-Optimized Bidirectional Long-Short-Term Memory network for Wind Power Forecasting with Uncertainty Quantification\n    Kundan Kumar\n    Electric Power Systems Research (under Review)\n    (2025)\n    \n\n    Details\n\n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\n    Kundan Kumar.\n    IEEE\n    (2025)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Bayesian Optimization for Deep Reinforcement Learning for Robust Volt-Var Control\n    Kundan Kumar\n    IEEE\n    (2024)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Deep Rl-based volt-var control and attack resiliency for der-integrated distribution grids\n    Kundan Kumar\n    IEEE\n    (2024)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Deep learning and pattern-based methodology for multivariable sensor data regression\n    Kundan Kumar\n    IEEE International Conference on Machine Learning and Applications (ICMLA)\n    (2024)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n  \n    Deep value of information estimators for collaborative human-machine information gathering\n     Kundan Kumar\n    ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS\n    (2016)\n    \n\n    Details\n\n    \n      \n        DOI\n      \n    \n\n    \n\n    \n\n    \n\n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/ph_20230330_sp/index.html",
    "href": "publications/ph_20230330_sp/index.html",
    "title": "Public health surveillance and reporting",
    "section": "",
    "text": "Time and place: Mar. 30, 2023 12:00 PM–1:00 PM\nHybrid: Georg Sverdrups hus and Zoom\nEvent page"
  },
  {
    "objectID": "publications/ph_20230330_sp/index.html#about-the-topic",
    "href": "publications/ph_20230330_sp/index.html#about-the-topic",
    "title": "Public health surveillance and reporting",
    "section": "About the topic",
    "text": "About the topic\nSituational awareness is key to fast response during a public health emergency, such as COVID-19 pandemic. However, making disease surveillance reports that cover different geographical units for various metrics and data registries is both resource intensive and time consuming. Open source tools such as R packages, GitHub and Airflow can make this process automatic, reproducible and scalable.\nEvery day during the pandemic, Sykdomspulsen team at the Norwegian Institute of Public Health (FHI/NIPH) fetched data from more than 15 data sources, cleaned, censored datasets and carried out a wide range of statistical analyses. Over 1000 situational reports containing automated graphs and tables were produced before breakfast time.\nGrab you matpakke and join us for a presentation from Chi Zhang about how Sykdomspulsen team used and developed open source software to make public health surveillance and reporting more efficient, followed up by a discussion on the benefits and concerns of making these data public. We will end with an open Q&A session as usual!"
  },
  {
    "objectID": "publications/ehr_20240918_betterehr/index.html",
    "href": "publications/ehr_20240918_betterehr/index.html",
    "title": "One step closer to better Electronic Health Records data",
    "section": "",
    "text": "Real-World Data (RWD) like Electronic Health Records (EHR) is crucial for understanding drug usage and various treatments and generating Real-World Evidence (RWE). Risk prediction has been a major application where EHR is used, and there is now a shift towards causal inference, which requires data of even higher quality. Patients undergo treatments (drugs, procedures) at various times during their hospital stays, yet the data being recorded are messy and error-prone for various reasons. Analysts spend significant amount of time to sit together with clinicians to identify and understand abnormal records, and unfortunately this process is challenging to automate.\nThis talk will use an example on antibiotics prescription and use at a Nordic hospital to illustrate how some EHR systems can improve for better clinical decision-making and better data for research. I will also introduce a pilot R package (ggehr) that facilitates visual exploration of EHR data, and how it can help reconstruct patient journeys and enable analysts to perform effective quality control."
  },
  {
    "objectID": "publications/community_20240710_camis/index.html",
    "href": "publications/community_20240710_camis/index.html",
    "title": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations",
    "section": "",
    "text": "2024.7.8-11, Salzburg, Austria. Conference link: UseR!\nStatisticians using multiple softwares (SAS, R, Python) will have found differences in analysis results that warrant further justification. Whilst some industries may accept results not being the same as long as they are “close”, the highly regulated pharmaceutical industry would require an identical match in results. Yet, discrepancies might still occur, and knowing the reasons (different methods, options, algorithms etc) is critical to the modern statistician and subsequent regulatory submissions.\nIn this talk I will introduce CAMIS: Comparing Analysis Method Implementations in Software. https://psiaims.github.io/CAMIS/ It is a joint-project between PHUSE, the R Validation Hub, PSI AIMS, R consortium and openstatsware. The aim of CAMIS is to investigate and document differences and similarities between different statistical softwares such as SAS and R. We use Quarto and Github to document methods, algorithms and comparisons between softwares through small case studies, and all articles are contributed by the community. In the transition from proprietary to open source technology in the industry, CAMIS can serve as a guidebook to navigate this process.\n\nkeywords: cross industry collaboration, multi-lingua, open-source, quarto"
  },
  {
    "objectID": "publications/rstats_20240613_teaching/index.html",
    "href": "publications/rstats_20240613_teaching/index.html",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "",
    "text": "Time and place: June 13 2024. Online\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "publications/rstats_20240613_teaching/index.html#about-the-topic",
    "href": "publications/rstats_20240613_teaching/index.html#about-the-topic",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "About the topic",
    "text": "About the topic\nThe Department of Biostatistics at University of Oslo offer statistics courses at different levels for medical students and PhD candidates with clinical backgrounds. The courses were traditionally taught with a focus on theory instead of data analysis, where SPSS and STATA were the tools of choice.\nSince 2023 spring semester, we have been gradually transforming some of our statistics courses into R, using Quarto course websites and Carpentries style live-coding instruction. With new Quarto tools (such as WebR) we also added interactivity in the code blocks. So far we have transformed two courses with over 100 students who have almost no programming experience. We have observed impressive progress in the skill development, and received significantly more positive feedback when it comes to statistics education.\nIn this talk, I would like to share our experience on the successes and challenges throughout the process. Looking back, is it cost-effective? Definitely. Can we do better in the future? Almost surely. If you are also planning to adopt new technology in your teaching activities, join us to learn more about what you can do to make the transition happen!\nCourse website can be accessed here"
  },
  {
    "objectID": "publications/ehr_20221013_ml_icu/index.html",
    "href": "publications/ehr_20221013_ml_icu/index.html",
    "title": "Machine Learning in Intensive Care Units",
    "section": "",
    "text": "A 45 minutes trial lecture to fulfill the requirement of my PhD degree."
  },
  {
    "objectID": "publications/ph_20220616_splverse/index.html",
    "href": "publications/ph_20220616_splverse/index.html",
    "title": "Sykdomspulsen: An automated public health surveillance platform",
    "section": "",
    "text": "About the talk\nWatch the talk on YouTube\nSykdomspulsen is a real-time analysis and disease surveillance system designed at developed at the Norwegian Institute of Public Health (FHI). Sykdomspulsen processes new data collected from 15 data sources (e.g., covid-19 cases), runs 1000.000+ statistical analysis automatically for all locations (nation, county, municipality) in Norway, produces 1000+ reports and alerts for public health authorities and shares data to the public on GitHub.\nSykdomspulsen runs on a collection of R packages, the {splverse}. {splverse} is an ecosystem for infectious disease surveillance, from analysis planning, statistical analysis to reporting via visualization, shiny website and Rmarkdown generated reports. In this talk, Chi will present how Sykdomspulsen does public health real-time surveillance during the pandemic using R. Chi will introduce some of the core packages and illustrate how they work together, with an example using real surveillance data published daily on GitHub.\n\n\nAbout the speaker\nChi is currently working at the Sykdomspulsen team as a researcher and R developer, at the Norwegian Institute of Public Health. Before she joined Sykdomspulsen in the middle of the pandemic (2020), she was a PhD student at the Department of Biostatistics at University of Oslo (OCBE), working on hospital EHR data."
  },
  {
    "objectID": "publications/dummy_talk/index.html",
    "href": "publications/dummy_talk/index.html",
    "title": "My Dummy Talk",
    "section": "",
    "text": "This is a simple demo of a Quarto talk listing with an image, title, and subtitle."
  },
  {
    "objectID": "publications/ehr_20210218_biday/index.html",
    "href": "publications/ehr_20210218_biday/index.html",
    "title": "Network Analysis of Hospital EHR data",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "publications/community_20240921_quartofriends/index.html",
    "href": "publications/community_20240921_quartofriends/index.html",
    "title": "Use Quarto, Make Friends",
    "section": "",
    "text": "It has been two years since Quarto became the most popular reproducible publication tool in data science and R community. However Quarto is so much more than just a publication tool! I started using it since late 2022, and it has helped me become more organized, productive and connected with people in the data science community.\nIn this talk I will not focus on the technical aspects on ‘how’ to use this tool. In the first part of the talk, I would like to report the latest news and trends seen in the useR conference and Posit conf, the two biggest global R events. In the second part, I will share my own experience in using Quarto for my career: from learning new skills, collaborating with co-workers, teaching university courses to networking and building a community (CAMIS collaboration). It is a powerful tool to share your work, and make new connections - both for work and for fun! I hope this talk will provide you with some new ideas on how to use this fantastic technology to fulfill your goals."
  },
  {
    "objectID": "publications/rstats_20190402_blogdown/index.html",
    "href": "publications/rstats_20190402_blogdown/index.html",
    "title": "Building Website in R: Step by Step Introduction to blogdown",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "publications/rstats_20230721_teaching/index.html",
    "href": "publications/rstats_20230721_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Time and place: July 21, 2023 10AM. Roche office, Basel, Switzerland\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "publications/rstats_20230721_teaching/index.html#about-the-topic",
    "href": "publications/rstats_20230721_teaching/index.html#about-the-topic",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "About the topic",
    "text": "About the topic\nThe 8 day introductory statistics course (MF9130) at the Faculty of Medicine, University of Oslo is designed for PhD students in medicine, biology, psychology and other health related fields. Similar to other conventional teaching methods, the course has been focusing largely on theory and hand calculation. The software has been Stata and SPSS, and data analysis was mostly left for the students to figure out on their own.\nThis year, we made an attempt to transform the course with R, and aimed to teach more practical data analysis skills. We added one session per day where the instructor guide students on R and project management, importing data , basic manipulation and statistical methods. The IT skills of the students vary greatly, and therefore we used the ‘sticky notes’ help system borrowed from the Carpentries to make sure everyone could get help in the first days. We have created a course website using Quarto, where all the material and R exercises (with rendered solution) are available for self-study. We have witnessed amazing progress - by the end of the first week, students with the least computer / data skills were able to work on dataframes, make basic plots and do a chi-squared test. This helps build students confidence in data and statistics, and as a result, they can start to work on their own datasets using the skills immediately."
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#citation-ieee",
    "href": "publications/articles/adaryukov2024.html#citation-ieee",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "",
    "text": "K. Kumar, K. Utkarsh, J. Wang, and H. V. Padullaparti, “Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems,” Proc. IEEE Power & Energy Society General Meeting (PESGM), 2025. (Accepted)"
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#citation-ieee-1",
    "href": "publications/articles/adaryukov2024.html#citation-ieee-1",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Citation (IEEE)",
    "text": "Citation (IEEE)\n\nK. Kumar, K. Utkarsh, J. Wang, and H. V. Padullaparti, “Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems,” Proc. IEEE Power & Energy Society General Meeting (PESGM), 2025. (Accepted)"
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#abstract-1",
    "href": "publications/articles/adaryukov2024.html#abstract-1",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Abstract",
    "text": "Abstract\nOur approach incorporates:\n\nSelf-training with an ensemble of multilayer perceptron classifiers.\nLabel spreading to propagate labels based on data similarity.\nBayesian Neural Networks (BNNs) for uncertainty estimation, improving confidence and reducing phase identification errors.\n\nKey Highlights: - Achieved ~98% ± 0.08 accuracy on real utility data (Duquesne Light Company) using minimal and unreliable labeled data. - Uncertainty-aware predictions reduce misclassification risk and improve smart grid reliability. - Combines pseudo-labeling, graph-based SSL, and probabilistic modeling to handle data scarcity in real-world distribution networks.\nOur SSL + Uncertainty Estimation approach provides an efficient and scalable solution for phase identification in AMI data, enabling utilities to improve modeling, simulation, and operational decision-making."
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#figures",
    "href": "publications/articles/adaryukov2024.html#figures",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Figures",
    "text": "Figures\n\n\nProposed SSL Framework Applied to AMI Data\n\n\n\nDistribution Feeder Topology\n\n\n\nTraining and Testing Data Partitions\n\n\n\nAccuracy Comparison of SSL Methods\n\n\n\n\n\n\nPresentation\n\n\n\n\n View Fullscreen Poster"
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#resources",
    "href": "publications/articles/adaryukov2024.html#resources",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Resources",
    "text": "Resources\n\nFull Paper: https://doi.org/10.31234/osf.io/j9yeq\nSupplementary Materials: https://osf.io/sfhm2/"
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#tags",
    "href": "publications/articles/adaryukov2024.html#tags",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Tags",
    "text": "Tags\nSmart Grid Phase Identification Bayesian Neural Networks Semi-Supervised Learning Uncertainty Estimation Conference Paper"
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#contribution",
    "href": "publications/articles/adaryukov2024.html#contribution",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Contribution",
    "text": "Contribution\nOur approach incorporates:\n\nSelf-training with an ensemble of multilayer perceptron classifiers.\nLabel spreading to propagate labels based on data similarity.\nBayesian Neural Networks (BNNs) for uncertainty estimation, improving confidence and reducing phase identification errors.\n\nKey Highlights:\n\nAchieved ~98% ± 0.08 accuracy on real utility data (Duquesne Light Company) using minimal and unreliable labeled data.\nUncertainty-aware predictions reduce misclassification risk and improve smart grid reliability.\nCombines pseudo-labeling, graph-based SSL, and probabilistic modeling to handle data scarcity in real-world distribution networks.\n\nOur SSL + Uncertainty Estimation approach provides an efficient and scalable solution for phase identification in AMI data, enabling utilities to improve modeling, simulation, and operational decision-making."
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#semi-supervised-learning-framework",
    "href": "publications/articles/adaryukov2024.html#semi-supervised-learning-framework",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Semi-Supervised Learning Framework",
    "text": "Semi-Supervised Learning Framework\nWe formulate SSL as a regularized optimization problem:Equation 1\n\\[\n\\min_{f \\in \\mathcal{F}} \\left[ \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\ell(f(x_i), y_i) + \\lambda R_u(f, \\mathcal{D}_U) \\right]\n\\tag{1}\\]\nWhere:\n\n( (, ) ): Supervised loss (cross-entropy)\n\n( R_u(f, _U) ): Unsupervised regularization term\n\n( ): Trade-off parameter\n\nThe challenge is designing ( R_u(f, _U) ) to effectively leverage unlabeled data.\n\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  (\\#eq:binom)\n\\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation @ref(eq:binom).\nWe formulate SSL as a regularized optimization problem:\n\\[\n\\min_{f \\in \\mathcal{F}} \\left[ \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\ell(f(x_i), y_i) + \\lambda R_u(f, \\mathcal{D}_U) \\right]\n\\] Where \\[a^2 + b^2 = d^2\\] is the Unsupervised regularization term\nWhere: \\[\nR_u(f, \\mathcal{D}_U)\n\\] is the - ( (, ) ): Supervised loss (cross-entropy)\n- ( R_u(f, _U) ): Unsupervised regularization term\n- ( ): Trade-off parameter\nThe challenge is designing ( R_u(f, _U) ) to effectively leverage unlabeled data.\n\n\\[\\ell(\\cdot, \\cdot)\\]: Supervised loss (cross-entropy)\n\\[R_u(f, \\mathcal{D}_U)\\]: Unsupervised regularization term\n\\[\\lambda\\]: Trade-off parameter\n( (, ) ): Supervised loss (cross-entropy)\n\n( R_u(f, _U) ): Unsupervised regularization term\n\n( ): Trade-off parameter\n\nWe formulate SSL as a regularized optimization problem: min_{f∈ℱ} [ (1/n_L) ∑^{n_L}_{i=1} ℓ(f(x_i), y_i) + λR_u(f, 𝒟_U) ] Where:\nℓ(·,·): Supervised loss (cross-entropy) R_u(f, 𝒟_U): Unsupervised regularization term λ: Trade-off parameter\nThe challenge: designing R_u(f, 𝒟_U) that effectively exploits unlabeled data structure ## Methodology\nWe define the binomial distribution as:(Equation 2)\n\\[\nf(k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\tag{2}\\]\nAs shown in Equation @ref(eq:binom), the binomial function defines the probability…\nBlack-Scholes (Equation 3) is a mathematical model that seeks to explain the behavior of financial derivatives, most commonly options:\n\\[\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm S^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C\n\\tag{3}\\]\n\n\nProposed SSL Framework Applied to AMI Data\n\n\n\nDistribution Feeder Topology\n\n\n\nTraining and Testing Data Partitions\n\n\n\nAccuracy Comparison of SSL Methods\n\n\n\n\n\n\nPresentation\n\n\n\n\n View Fullscreen Poster"
  },
  {
    "objectID": "publications/articles/adaryukov2024.html#methodology",
    "href": "publications/articles/adaryukov2024.html#methodology",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Methodology",
    "text": "Methodology\n\n\nProposed SSL Framework Applied to AMI Data\n\n\n\nDistribution Feeder Topology\n\n\n\nTraining and Testing Data Partitions\n\n\n\nAccuracy Comparison of SSL Methods\n\n\n\n\n\n\nPresentation\n\n\n\n\n View Fullscreen Poster"
  },
  {
    "objectID": "publications/CopyOfindex1.html",
    "href": "publications/CopyOfindex1.html",
    "title": "Math in Quarto",
    "section": "",
    "text": "_{f } \nWe define the binomial distribution as:\n\\[\\begin{equation}\nf\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n\\label{eq:binom}\n\\end{equation}\\]\nAs shown in Equation @ref(eq:binom), the binomial function defines the probability…"
  },
  {
    "objectID": "publications/CopyOfindex1.html#binomial-formula",
    "href": "publications/CopyOfindex1.html#binomial-formula",
    "title": "Math in Quarto",
    "section": "",
    "text": "_{f } \nWe define the binomial distribution as:\n\\[\\begin{equation}\nf\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n\\label{eq:binom}\n\\end{equation}\\]\nAs shown in Equation @ref(eq:binom), the binomial function defines the probability…"
  },
  {
    "objectID": "publications/CopyOfindex1.html#binomial-formula-1",
    "href": "publications/CopyOfindex1.html#binomial-formula-1",
    "title": "Math in Quarto",
    "section": "Binomial Formula",
    "text": "Binomial Formula\nWe define the binomial distribution as:(Equation 1)\n\\[\nf(k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\tag{1}\\]\nAs shown in Equation @ref(eq:binom), the binomial function defines the probability…\nBlack-Scholes (Equation 2) is a mathematical model that seeks to explain the behavior of financial derivatives, most commonly options:\n\\[\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm S^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C\n\\tag{2}\\]"
  },
  {
    "objectID": "teaching/teaching.html#coms1130",
    "href": "teaching/teaching.html#coms1130",
    "title": "Teaching",
    "section": "Spreadsheets and Databases",
    "text": "Spreadsheets and Databases"
  },
  {
    "objectID": "teaching/teaching.html#coms3090",
    "href": "teaching/teaching.html#coms3090",
    "title": "Teaching",
    "section": "Software Development Practices",
    "text": "Software Development Practices"
  },
  {
    "objectID": "teaching/teaching.html#coms3190",
    "href": "teaching/teaching.html#coms3190",
    "title": "Teaching",
    "section": "User Interface Design",
    "text": "User Interface Design"
  },
  {
    "objectID": "teaching/teaching.html#coms3620",
    "href": "teaching/teaching.html#coms3620",
    "title": "Teaching",
    "section": "Object-Oriented Analysis and Design",
    "text": "Object-Oriented Analysis and Design"
  },
  {
    "objectID": "teaching/teaching.html#coms3630",
    "href": "teaching/teaching.html#coms3630",
    "title": "Teaching",
    "section": "Software Testing",
    "text": "Software Testing"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#what-interviewers-evaluate",
    "href": "blog/technotes_20250703_research_guide/index.html#what-interviewers-evaluate",
    "title": "Research Scientist Interview Guide",
    "section": "What interviewers evaluate",
    "text": "What interviewers evaluate\n\nResearch impact: novelty, citations/adoption, reproducibility, clarity of problem–method–evidence chain.\nTechnical depth: math/ML fundamentals, experimental rigor, ablation thinking, error analysis.\nSystems sense: how ideas become products—data, infra, metrics, reliability, safety & ethics.\nExecution: scope → plan → iterate → deliver (papers, open-source, patents, internal wins).\nCommunication & collaboration: explain complex work to varied audiences; cross-discipline work.\nCulture/LP fit: ownership, bias for action, frugality, customer/impact obsession.\n\n\n\n\n\n\n\nTip\n\n\n\nBe explicit about your contribution. For each project: problem → gap → idea → method → evidence → limitations → next steps → impact."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#interview-process-at-a-glance",
    "href": "blog/technotes_20250703_research_guide/index.html#interview-process-at-a-glance",
    "title": "Research Scientist Interview Guide",
    "section": "Interview process at a glance",
    "text": "Interview process at a glance\nTypical stages\n1) Recruiter + HM intro → 2) Tech/Research screens (coding, ML/math, paper deep dive) →\n3) Onsite: research talk, systems/experimentation design, cross-functional, behavioral/bar raiser →\n4) Debrief → offer.\n(Use the SVG diagram you generated earlier or embed it with ![](research_scientist_interview_process.svg).)"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#preparation-timeline-68-weeks",
    "href": "blog/technotes_20250703_research_guide/index.html#preparation-timeline-68-weeks",
    "title": "Research Scientist Interview Guide",
    "section": "Preparation timeline (6–8 weeks)",
    "text": "Preparation timeline (6–8 weeks)\nWeeks 1–2: Foundations & portfolio - Curate 2–3 flagship projects; write 1-page project briefs (problem, novelty, 3 results, open questions). - Refresh ML math: gradients, likelihoods, bias–variance, generalization, off-policy vs on-policy RL. - DS&A 20–30 mins/day (arrays, hash maps, trees, graphs, DP—medium level). - Draft talk outline; collect figures; start a reproducible repo.\nWeeks 3–4: Research talk + deep dives - Build slides (30/45/60 min versions). Timebox: Motivation 10% → Method 35% → Evidence 40% → Limits + Roadmap 15%. - Prepare ablation stories and negative results; design a live error analysis demo if feasible. - Mock talks with labmates; iterate twice.\nWeeks 5–6: Systems & coding polish - 5 case studies: online inference, data pipelines, eval at scale, safety/guardrails, monitoring. - Practice 6–8 coding problems in 60-min sessions; review idioms (two-pointers, heap, BFS/DFS, topo sort). - Draft answers for 8–10 behavioral prompts using STAR(L).\nWeek 7+: Company-specific tuning - Read team papers/repos; align your roadmap slide to their charter. - Prepare 8–12 questions to ask (below). - Dry run full onsite (talk + 3 interviews + behavioral) in a single sitting."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#research-portfolio-deep-dive",
    "href": "blog/technotes_20250703_research_guide/index.html#research-portfolio-deep-dive",
    "title": "Research Scientist Interview Guide",
    "section": "Research portfolio deep dive",
    "text": "Research portfolio deep dive\nFor each project, be ready to answer: - Gap: What prior SOTA did not address? Why now? - Assumptions: Distributional, structural, or operational assumptions—how validated? - Method: Key design choices (loss, architecture, training regime, priors/constraints). - Evidence: Metrics that matter (with CIs); strongest ablation; hardest failure case. - Impact: External adoption, dataset/code release, internal KPI movement, patents. - Next: What would you do with 3 months & a small team?\nArtifacts checklist - 10–12 figure slide deck (vector PDFs), 1-page PDF overview, GitHub README with quickstart, repro seed + script."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#technical-machine-learning-knowledge-what-to-refresh",
    "href": "blog/technotes_20250703_research_guide/index.html#technical-machine-learning-knowledge-what-to-refresh",
    "title": "Research Scientist Interview Guide",
    "section": "Technical machine learning knowledge (what to refresh)",
    "text": "Technical machine learning knowledge (what to refresh)\n\nRL: policy gradient theorem; advantage estimation; PPO/TRPO constraints; off-policy (DQN/TD3/SAC); safe RL & constraints; exploration vs exploitation; eval instability & seeding.\nDeep learning: optimization (warmup, cosine decay, AdamW), regularization (dropout, mixup, label-smoothing), attention/transformers, LoRA/parameter-efficient finetuning.\nStatistics & probabilistic modeling: MLE/MAP; conjugacy; posterior predictive; calibration (ECE), uncertainty (epistemic vs aleatoric); A/B testing pitfalls.\nGenerative models: diffusion schedule & guidance, VAEs ELBO, GAN stability.\nLLMs: instruction tuning, RAG retrieval quality, eval (exact match, nDCG, win-rates), toxicity & safety filters, hallucination mitigation.\nVision/multimodal: contrastive learning, detection/segmentation metrics (mAP, IoU), data augmentations."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#ml-systems-experimentation-design",
    "href": "blog/technotes_20250703_research_guide/index.html#ml-systems-experimentation-design",
    "title": "Research Scientist Interview Guide",
    "section": "ML systems & experimentation design",
    "text": "ML systems & experimentation design\n5-step template 1. Clarify goal (user KPI ↔︎ technical metric; online vs offline).\n2. Data (sources, labeling strategy, noise, sampling, privacy).\n3. Model & infra (baseline → candidate → serving path; latency, cost, reliability).\n4. Evaluation (offline metrics + counterfactual replays + online guardrails; slicing).\n5. Risk & safety (bias, misuse, red-teaming, rollback plan, observability).\nExample prompt (outline answer)\n“Design a near real-time anomaly detector for a power grid substation.”\n- KPI: reduce outage MTTR by 20%; constraints: &lt;200 ms latency, 99.9% uptime.\n- Data: PMU/SCADA streams; label via weak supervision + operator tags.\n- Baseline: statistical thresholds; Model: streaming autoencoder + EWMA residuals.\n- Eval: ROC-AUC offline, time-to-detect, false alarms/day; shadow deploy → phased rollout.\n- Safety: fail-open; human-in-the-loop; drift detector; incident playbook."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#coding-algorithmic-skills",
    "href": "blog/technotes_20250703_research_guide/index.html#coding-algorithmic-skills",
    "title": "Research Scientist Interview Guide",
    "section": "Coding & algorithmic skills",
    "text": "Coding & algorithmic skills\n\nAim for clean, correct, then optimal. Speak invariants, test cases, and complexity out loud.\nPatterns to practice: two-pointers, sliding window, monotonic stack, BFS/DFS, topological sort, binary search on answer, Dijkstra/Union-Find, prefix sums, DP on sequences/trees.\nML-adjacent coding: vectorized NumPy, PyTorch modules/forward pass, dataloaders, batching, mixed precision, sanity checks."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#research-talk-structure-slides",
    "href": "blog/technotes_20250703_research_guide/index.html#research-talk-structure-slides",
    "title": "Research Scientist Interview Guide",
    "section": "Research talk: structure & slides",
    "text": "Research talk: structure & slides\nSlide budget (45 min talk + Q&A) - Title & takeaway (1), Motivation (2), Problem/Gap (2), Method (5), Results (6), Ablations (3), Error analysis (2), Limits (1), Roadmap/fit (2).\nDos - One idea per slide; consistent color for your method; readable axes; include n and CI.\n- Put the thesis of each slide in the title: “Physics constraints cut violations by 38% at same cost.”\nDon’ts - Crowded plots, cherry-picked examples, unanchored qualitative claims, tiny captions."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#behavioral-starl-research-flavors",
    "href": "blog/technotes_20250703_research_guide/index.html#behavioral-starl-research-flavors",
    "title": "Research Scientist Interview Guide",
    "section": "Behavioral: STAR(L) + research flavors",
    "text": "Behavioral: STAR(L) + research flavors\nUse STAR(L): Situation, Task, Action, Result, Learning.\nPrepare 6 stories: conflict, failure, leadership without authority, speed vs quality, mentoring, cross-team project.\nExample prompt\n“Tell me about a time your experiment invalidated a roadmap item.”\n- S/T: critical Q3 milestone hinged on SOTA surpassing baseline.\n- A: pre-registered analysis; ran holdout; flagged negative lift; proposed minimal viable alternative.\n- R: saved ~6 wks eng time; reallocated to data quality; shipped smaller win.\n- L: add “early stop” gates; improved pre-mortem checklist."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#recommended-resources",
    "href": "blog/technotes_20250703_research_guide/index.html#recommended-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended resources",
    "text": "Recommended resources\n\nPapers: recent NeurIPS/ICLR/ICML tracks relevant to the team; read 2–3 team papers.\n\nBooks: Designing Machine Learning Systems (Huyen), Deep Learning (Goodfellow), ESL (HTF), Probabilistic ML (Barber/Murphy).\n\nPractice: LeetCode medium sets; pair-program ML design prompts; mock talks."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#example-technical-research-questions",
    "href": "blog/technotes_20250703_research_guide/index.html#example-technical-research-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example technical & research questions",
    "text": "Example technical & research questions\n\nSummarize the core contribution of your latest paper in two sentences.\n\nWhich ablation most strongly supports your claim? Which one failed and why?\n\nHow would you adapt your method under 10× less data? Under severe shift?\n\nWhat is your evaluation blind spot today? How would you close it?\n\nExplain epistemic vs. aleatoric uncertainty with a concrete modeling choice.\n\nFor PPO, where does instability come from and how do you diagnose it?\n\nDesign a reliable RAG system for safety-critical queries—retrieval, scoring, guardrails, and evaluation."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#ask-the-interviewer-tailored-high-signal",
    "href": "blog/technotes_20250703_research_guide/index.html#ask-the-interviewer-tailored-high-signal",
    "title": "Research Scientist Interview Guide",
    "section": "“Ask the interviewer” (tailored, high-signal)",
    "text": "“Ask the interviewer” (tailored, high-signal)\nHiring Manager / Director - What research bets are top-of-mind for the next 6–12 months, and how is success measured? - How do scientists land their ideas in product or publications here? - What does an excellent first 90 days look like for this role?\nResearch Scientists - Which datasets/eval harnesses are canonical for the team?\n- How do you run ablations and share results (internal tooling, norms)? - Recent paper/project you’re proud of—what made it successful?\nEngineers / MLEs - Serving constraints (latency/cost/traffic), feature stores, deployment cadence, rollback.\nPM / Cross-functional - Who is the customer? What decisions change when our model improves?\nRecruiter / Comp - Leveling expectations; publication policy; visa/timeline constraints."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#offer-debrief-negotiation-brief",
    "href": "blog/technotes_20250703_research_guide/index.html#offer-debrief-negotiation-brief",
    "title": "Research Scientist Interview Guide",
    "section": "Offer, debrief & negotiation (brief)",
    "text": "Offer, debrief & negotiation (brief)\n\nKeep a brag doc of impact during process.\n\nAsk for written feedback themes; address any concerns explicitly.\n\nNegotiate with comparables: role scope, level, location; anchor on expected impact; consider sign-on, research budget, conference travel."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#checklists",
    "href": "blog/technotes_20250703_research_guide/index.html#checklists",
    "title": "Research Scientist Interview Guide",
    "section": "Checklists",
    "text": "Checklists\nDay-before technical Interview\n\nTalk readiness: 30/45/60-min versions; 1-sentence thesis; clearly mark your contributions.\nAblations & limits: 1 strongest ablation you can defend; 1 negative result and what it taught you.\nPaper deep dives: be ready to derive the key equation/algorithm; compare to 2 baselines with numbers.\nMath/ML refresh: RL (policy gradient, GAE, PPO/TRPO intuition), DL (optimizers, regularization), stats (MLE vs MAP, bias–variance, eval metrics).\nCoding drills: 2 timed mediums using core patterns (two-pointers, BFS/DFS, heap, topo sort, DP); practice test-first pseudocode.\nSystems prompts: outline 3 cases (data → model → eval → guardrails → rollout) you can walk through crisply.\nQ&A bank: 10 answers you can deliver fast (assumptions, failure modes, robustness, scalability, safety).\n\nDay-of Interview\n\nOpen strong: restate problem + constraints; define the success metric before proposing solutions.\nReasoning first: outline 2–3 approaches; justify trade-offs; state target time/space complexity.\nCoding round: implement incrementally; speak invariants and edge cases; analyze complexity after passing tests.\nResearch talk: show the central figure; defend an ablation; admit a limitation and the next experiment.\nDesign/experimentation: baseline → candidate → eval plan; define slices; propose risks & rollback.\nClose each round: 30–60s recap with decision, trade-offs, and “next step” you’d run."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#optional-306090-plan-template",
    "href": "blog/technotes_20250703_research_guide/index.html#optional-306090-plan-template",
    "title": "Research Scientist Interview Guide",
    "section": "(Optional) 30/60/90 plan template",
    "text": "(Optional) 30/60/90 plan template\n\n30 days: learn stack, replicate key results, fix one paper repo issue, propose 2 ablations.\n\n60 days: own an experiment area; ship one internal win or workshop-level result.\n\n90 days: draft conference submission or measurable product lift; roadmap next quarter."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#overview",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "Overview",
    "text": "Overview\nThis guide provides a practical framework to prepare for Research Scientist roles in academia, industry research labs (FAANG, OpenAI, DeepMind, Anthropic, NVIDIA), and national labs. It blends personal experience with hiring-manager expectations and common evaluation rubrics.\nGitHub repository: Code\nData Science Notes: Data Science Intro\nStatistical Learning Notes: Statistical Analysis"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#what-interviewers-evaluate",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#what-interviewers-evaluate",
    "title": "Research Scientist Interview Guide",
    "section": "What interviewers evaluate",
    "text": "What interviewers evaluate\n\nResearch impact: novelty, citations/adoption, reproducibility, clarity of problem–method–evidence chain.\nTechnical depth: math/ML fundamentals, experimental rigor, ablation thinking, error analysis.\nSystems sense: how ideas become products—data, infra, metrics, reliability, safety & ethics.\nExecution: scope → plan → iterate → deliver (papers, open-source, patents, internal wins).\nCommunication & collaboration: explain complex work to varied audiences; cross-discipline work.\nCulture/LP fit: ownership, bias for action, frugality, customer/impact obsession.\n\n\n\n\n\n\n\nTip\n\n\n\nBe explicit about your contribution. For each project: problem → gap → idea → method → evidence → limitations → next steps → impact."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#interview-process-at-a-glance",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#interview-process-at-a-glance",
    "title": "Research Scientist Interview Guide",
    "section": "Interview process at a glance",
    "text": "Interview process at a glance\nTypical stages\n1) Recruiter + HM intro → 2) Tech/Research screens (coding, ML/math, paper deep dive) →\n3) Onsite: research talk, systems/experimentation design, cross-functional, behavioral/bar raiser →\n4) Debrief → offer.\n(Use the SVG diagram you generated earlier or embed it with ![](research_scientist_interview_process.svg).)"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#preparation-timeline-68-weeks",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#preparation-timeline-68-weeks",
    "title": "Research Scientist Interview Guide",
    "section": "Preparation timeline (6–8 weeks)",
    "text": "Preparation timeline (6–8 weeks)\nWeeks 1–2: Foundations & portfolio - Curate 2–3 flagship projects; write 1-page project briefs (problem, novelty, 3 results, open questions). - Refresh ML math: gradients, likelihoods, bias–variance, generalization, off-policy vs on-policy RL. - DS&A 20–30 mins/day (arrays, hash maps, trees, graphs, DP—medium level). - Draft talk outline; collect figures; start a reproducible repo.\nWeeks 3–4: Research talk + deep dives - Build slides (30/45/60 min versions). Timebox: Motivation 10% → Method 35% → Evidence 40% → Limits + Roadmap 15%. - Prepare ablation stories and negative results; design a live error analysis demo if feasible. - Mock talks with labmates; iterate twice.\nWeeks 5–6: Systems & coding polish - 5 case studies: online inference, data pipelines, eval at scale, safety/guardrails, monitoring. - Practice 6–8 coding problems in 60-min sessions; review idioms (two-pointers, heap, BFS/DFS, topo sort). - Draft answers for 8–10 behavioral prompts using STAR(L).\nWeek 7+: Company-specific tuning - Read team papers/repos; align your roadmap slide to their charter. - Prepare 8–12 questions to ask (below). - Dry run full onsite (talk + 3 interviews + behavioral) in a single sitting."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#research-portfolio-deep-dive",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#research-portfolio-deep-dive",
    "title": "Research Scientist Interview Guide",
    "section": "Research portfolio deep dive",
    "text": "Research portfolio deep dive\nFor each project, be ready to answer: - Gap: What prior SOTA did not address? Why now? - Assumptions: Distributional, structural, or operational assumptions—how validated? - Method: Key design choices (loss, architecture, training regime, priors/constraints). - Evidence: Metrics that matter (with CIs); strongest ablation; hardest failure case. - Impact: External adoption, dataset/code release, internal KPI movement, patents. - Next: What would you do with 3 months & a small team?\nArtifacts checklist - 10–12 figure slide deck (vector PDFs), 1-page PDF overview, GitHub README with quickstart, repro seed + script."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#technical-machine-learning-knowledge-what-to-refresh",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#technical-machine-learning-knowledge-what-to-refresh",
    "title": "Research Scientist Interview Guide",
    "section": "Technical machine learning knowledge (what to refresh)",
    "text": "Technical machine learning knowledge (what to refresh)\n\nRL: policy gradient theorem; advantage estimation; PPO/TRPO constraints; off-policy (DQN/TD3/SAC); safe RL & constraints; exploration vs exploitation; eval instability & seeding.\nDeep learning: optimization (warmup, cosine decay, AdamW), regularization (dropout, mixup, label-smoothing), attention/transformers, LoRA/parameter-efficient finetuning.\nStatistics & probabilistic modeling: MLE/MAP; conjugacy; posterior predictive; calibration (ECE), uncertainty (epistemic vs aleatoric); A/B testing pitfalls.\nGenerative models: diffusion schedule & guidance, VAEs ELBO, GAN stability.\nLLMs: instruction tuning, RAG retrieval quality, eval (exact match, nDCG, win-rates), toxicity & safety filters, hallucination mitigation.\nVision/multimodal: contrastive learning, detection/segmentation metrics (mAP, IoU), data augmentations."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#ml-systems-experimentation-design",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#ml-systems-experimentation-design",
    "title": "Research Scientist Interview Guide",
    "section": "ML systems & experimentation design",
    "text": "ML systems & experimentation design\n5-step template 1. Clarify goal (user KPI ↔︎ technical metric; online vs offline).\n2. Data (sources, labeling strategy, noise, sampling, privacy).\n3. Model & infra (baseline → candidate → serving path; latency, cost, reliability).\n4. Evaluation (offline metrics + counterfactual replays + online guardrails; slicing).\n5. Risk & safety (bias, misuse, red-teaming, rollback plan, observability).\nExample prompt (outline answer)\n“Design a near real-time anomaly detector for a power grid substation.”\n- KPI: reduce outage MTTR by 20%; constraints: &lt;200 ms latency, 99.9% uptime.\n- Data: PMU/SCADA streams; label via weak supervision + operator tags.\n- Baseline: statistical thresholds; Model: streaming autoencoder + EWMA residuals.\n- Eval: ROC-AUC offline, time-to-detect, false alarms/day; shadow deploy → phased rollout.\n- Safety: fail-open; human-in-the-loop; drift detector; incident playbook."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#coding-algorithmic-skills",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#coding-algorithmic-skills",
    "title": "Research Scientist Interview Guide",
    "section": "Coding & algorithmic skills",
    "text": "Coding & algorithmic skills\n\nAim for clean, correct, then optimal. Speak invariants, test cases, and complexity out loud.\nPatterns to practice: two-pointers, sliding window, monotonic stack, BFS/DFS, topological sort, binary search on answer, Dijkstra/Union-Find, prefix sums, DP on sequences/trees.\nML-adjacent coding: vectorized NumPy, PyTorch modules/forward pass, dataloaders, batching, mixed precision, sanity checks."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#research-talk-structure-slides",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#research-talk-structure-slides",
    "title": "Research Scientist Interview Guide",
    "section": "Research talk: structure & slides",
    "text": "Research talk: structure & slides\nSlide budget (45 min talk + Q&A) - Title & takeaway (1), Motivation (2), Problem/Gap (2), Method (5), Results (6), Ablations (3), Error analysis (2), Limits (1), Roadmap/fit (2).\nDos - One idea per slide; consistent color for your method; readable axes; include n and CI.\n- Put the thesis of each slide in the title: “Physics constraints cut violations by 38% at same cost.”\nDon’ts - Crowded plots, cherry-picked examples, unanchored qualitative claims, tiny captions."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#behavioral-starl-research-flavors",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#behavioral-starl-research-flavors",
    "title": "Research Scientist Interview Guide",
    "section": "Behavioral: STAR(L) + research flavors",
    "text": "Behavioral: STAR(L) + research flavors\nUse STAR(L): Situation, Task, Action, Result, Learning.\nPrepare 6 stories: conflict, failure, leadership without authority, speed vs quality, mentoring, cross-team project.\nExample prompt\n“Tell me about a time your experiment invalidated a roadmap item.”\n- S/T: critical Q3 milestone hinged on SOTA surpassing baseline.\n- A: pre-registered analysis; ran holdout; flagged negative lift; proposed minimal viable alternative.\n- R: saved ~6 wks eng time; reallocated to data quality; shipped smaller win.\n- L: add “early stop” gates; improved pre-mortem checklist."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#recommended-resources",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#recommended-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended resources",
    "text": "Recommended resources\n\nPapers: recent NeurIPS/ICLR/ICML tracks relevant to the team; read 2–3 team papers.\n\nBooks: Designing Machine Learning Systems (Huyen), Deep Learning (Goodfellow), ESL (HTF), Probabilistic ML (Barber/Murphy).\n\nPractice: LeetCode medium sets; pair-program ML design prompts; mock talks."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#example-technical-research-questions",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#example-technical-research-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example technical & research questions",
    "text": "Example technical & research questions\n\nSummarize the core contribution of your latest paper in two sentences.\n\nWhich ablation most strongly supports your claim? Which one failed and why?\n\nHow would you adapt your method under 10× less data? Under severe shift?\n\nWhat is your evaluation blind spot today? How would you close it?\n\nExplain epistemic vs. aleatoric uncertainty with a concrete modeling choice.\n\nFor PPO, where does instability come from and how do you diagnose it?\n\nDesign a reliable RAG system for safety-critical queries—retrieval, scoring, guardrails, and evaluation."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#ask-the-interviewer-tailored-high-signal",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#ask-the-interviewer-tailored-high-signal",
    "title": "Research Scientist Interview Guide",
    "section": "“Ask the interviewer” (tailored, high-signal)",
    "text": "“Ask the interviewer” (tailored, high-signal)\n\nUse the set that best fits the team. If time is short, ask the Top 3 in each block.\n\n\nHiring Manager (core, any research team)\n\nTop 3\n\nWhat research bets matter most in the next 6–12 months, and how will you measure success?\nWhat makes a “hell-yes” hire here after 90 days? What work would signal that?\nHow do ideas transition from a paper/prototype to production or a public result?\n\nHow is impact recognized—publications, product metrics, patents, internal adoption?\nWhat are examples of projects that didn’t land? Why, and what changed afterward?\nWhere are the biggest data/infra bottlenecks that a new scientist can unlock?\n\n\n\n\nResearch Scientists (peer scientists)\n\nTop 3\n\nWhich canonical datasets/eval harnesses are used for your area? How are baselines enforced?\nWhat’s a recent ablation or negative result that changed your roadmap?\nHow do you share/replicate experiments (internal tooling, seeds, result store)?\n\nHow are collaborations formed across teams? Any “platform” teams I should align with?\nWhat’s the cadence for paper reviews, reading groups, and internal talks?\n\n\n\n\nEngineers / MLEs (production & infra)\n\nTop 3\n\nWhat are the serving constraints (latency, throughput, cost) and reliability SLOs?\nWhat’s the path from notebook → feature store → online eval → rollout/rollback?\nHow do you monitor drift and failures in the wild? What’s the on-call/ownership model?\n\nWhat’s the CI/CD story for models (gating tests, shadow, canary, A/B infra)?\nWhat would you change in our current stack if you could?\n\n\n\n\nPM / Cross-functional (applied impact)\n\nTop 3\n\nWhich decision or workflow actually changes if this model improves by X%?\nWhat is the single metric you’d show leadership to justify continued investment?\nWhat risks (safety, bias, misuse) keep you up at night for this application?\n\nWhere does data come from and how does quality/coverage limit the roadmap?\n\n\n\n\nRecruiter / Compensation\n\nLevel targeting and calibration—what evidence best demonstrates readiness for this level?\nPublication & open-source policy (authors, timing, preprints), conference travel norms.\nVisa/relocation timeline; expected hire start window and interview re-try policy."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#offer-debrief-negotiation-brief",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#offer-debrief-negotiation-brief",
    "title": "Research Scientist Interview Guide",
    "section": "Offer, debrief & negotiation (brief)",
    "text": "Offer, debrief & negotiation (brief)\n\nKeep a brag doc of impact during process.\n\nAsk for written feedback themes; address any concerns explicitly.\n\nNegotiate with comparables: role scope, level, location; anchor on expected impact; consider sign-on, research budget, conference travel."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#checklists",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#checklists",
    "title": "Research Scientist Interview Guide",
    "section": "Checklists",
    "text": "Checklists\nDay-before technical Interview\n\nTalk readiness: 30/45/60-min versions; 1-sentence thesis; clearly mark your contributions.\nAblations & limits: 1 strongest ablation you can defend; 1 negative result and what it taught you.\nPaper deep dives: be ready to derive the key equation/algorithm; compare to 2 baselines with numbers.\nMath/ML refresh: RL (policy gradient, GAE, PPO/TRPO intuition), DL (optimizers, regularization), stats (MLE vs MAP, bias–variance, eval metrics).\nCoding drills: 2 timed mediums using core patterns (two-pointers, BFS/DFS, heap, topo sort, DP); practice test-first pseudocode.\nSystems prompts: outline 3 cases (data → model → eval → guardrails → rollout) you can walk through crisply.\nQ&A bank: 10 answers you can deliver fast (assumptions, failure modes, robustness, scalability, safety).\n\nDay-of Interview\n\nOpen strong: restate problem + constraints; define the success metric before proposing solutions.\nReasoning first: outline 2–3 approaches; justify trade-offs; state target time/space complexity.\nCoding round: implement incrementally; speak invariants and edge cases; analyze complexity after passing tests.\nResearch talk: show the central figure; defend an ablation; admit a limitation and the next experiment.\nDesign/experimentation: baseline → candidate → eval plan; define slices; propose risks & rollback.\nClose each round: 30–60s recap with decision, trade-offs, and “next step” you’d run."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#mentorship",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’d like feedback on your talk, paper deep dive, or a full mock onsite, reach me at cs.kundann@gmail.com."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#optional-306090-plan-template",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#optional-306090-plan-template",
    "title": "Research Scientist Interview Guide",
    "section": "(Optional) 30/60/90 plan template",
    "text": "(Optional) 30/60/90 plan template\n\n30 days: learn stack, replicate key results, fix one paper repo issue, propose 2 ablations.\n\n60 days: own an experiment area; ship one internal win or workshop-level result.\n\n90 days: draft conference submission or measurable product lift; roadmap next quarter."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#behavioral-star-research-flavors",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#behavioral-star-research-flavors",
    "title": "Research Scientist Interview Guide",
    "section": "Behavioral: STAR + research flavors",
    "text": "Behavioral: STAR + research flavors\nUse STAR: Situation, Task, Action, Result, Learning.\nPrepare 6 stories: conflict, failure, leadership without authority, speed vs quality, mentoring, cross-team project.\nExample prompt\n“Tell me about a time your experiment invalidated a roadmap item.”\n- S/T: critical Q3 milestone hinged on SOTA surpassing baseline.\n- A: pre-registered analysis; ran holdout; flagged negative lift; proposed minimal viable alternative.\n- R: saved ~6 wks eng time; reallocated to data quality; shipped smaller win.\n- L: add “early stop” gates; improved pre-mortem checklist."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#checklists-1",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#checklists-1",
    "title": "Research Scientist Interview Guide",
    "section": "Checklists",
    "text": "Checklists\nDay-before (technical)\n\nRepro lock-in: fix random seeds; rerun end-to-end once; save artifacts; export env (pip freeze &gt; requirements.txt or conda env export -n &lt;env&gt; &gt; environment.yml).\nFast demo path: keep a CPU-only/lightweight notebook or script that finishes in &lt;2–3 min; cache sample data & model weights locally.\nResults sanity: regenerate key tables/plots; include confidence intervals; verify top ablations; prepare 3 failure cases + why.\nSlides polish: vector figures; consistent metric names; add a “Setup” slide (data → model → metric); backup “extra” slides for deep-dive Q&A.\nPaper/project briefs: 1-page per flagship project (problem → gap → idea → evidence → limits → next).\nCoding warm-up: solve 2 medium DS&A; practice templates (BFS/DFS, two-pointers, heap, topo sort, DP).\nSystems/design prep: 3 case studies ready (data → model → eval → guardrails → rollout); write a 5-step evaluation plan you can reuse.\nQA bank: prepare 10 technical Q&As (assumptions, ablations, baselines, scalability, safety).\n\nDay-of (technical execution)\n\nEnvironment quick test: open IDE/notebook; import core libs; run a 10-second sanity cell (e.g., small tensor op / toy fit).\nResearch talk: state one-sentence thesis up front; mark your contributions; show 1 ablation + 1 error analysis; keep strict time.\nCoding round: restate problem & constraints; outline approach first; write small tests; code clean; analyze time/space; cover edge cases.\nDesign/experimentation: start with KPI & constraints; present baseline → candidate; define metrics & slices; propose risk/rollback plan.\nQuantified wrap-ups: end each session with a 30–60s summary (decision, trade-offs, next step).\nCapture learnings: jot technical gaps/questions right after each round to address in later interviews."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#offer-debrief-negotiation",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#offer-debrief-negotiation",
    "title": "Research Scientist Interview Guide",
    "section": "Offer, Debrief & Negotiation",
    "text": "Offer, Debrief & Negotiation\n1) Right after the loop\n\nKeep a short brag doc: 5–8 bullets tying your work to measurable impact (citations, benchmarks, improvements).\nSend thank-you notes; ask the recruiter for decision timeline and whether the team needs any follow-ups (extra slides, code pointers).\n\n2) Debrief (if you don’t get detailed feedback)\n\nAsk for 3 bullets: (i) strengths that stood out, (ii) top concern, (iii) what would change the decision next time.\nIf there’s a miscalibration (e.g., level/scope), propose a targeted follow-up (short tech screen or focused deep dive).\n\n3) Offer review (when it comes)\n\nBreak it down: base + bonus + equity/RSUs (grant, vesting, refreshers) + sign-on + extras (compute budget, conference travel, publication policy, patent bonus).\nClarify: level, title, team mandate, location policy, start date, performance review cycles, and conference/OSS policies.\n\n4) Negotiation (impact-first)\n\nAnchor with scope & impact (what you can deliver in 6–12 months), plus comparables (peer offers or market data for the same level/geo).\nPrioritize asks (pick 2–3): sign-on / equity / level / research budget / conf travel.\n\nSample script:\n“Given the scope (X) and the impact I’m positioned to deliver (Y), I’m targeting total comp of Z at level L. If level is fixed, increasing equity by A or adding a B sign-on would bridge the gap. I’d also value C (e.g., conference travel commitment).”\n\n5) Timelines\n\nIf you need time: “I’m very excited. To make a well-considered decision, could we set a reply date of ? I want to complete one pending loop and compare scopes fairly.”\nIf there’s an exploding deadline, ask for a short extension in exchange for a firm decision date and clear intent.\n\n6) If rejected\n\nRequest specific growth areas and ask whether a re-interview window (e.g., 6 months) with a targeted bar (e.g., systems/experiments) is possible."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#ask-the-interviewer",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#ask-the-interviewer",
    "title": "Research Scientist Interview Guide",
    "section": "“Ask the interviewer”",
    "text": "“Ask the interviewer”\n\nUse the set that best fits the team. If time is short, ask the Top 3 in each block.\n\n\nHiring Manager (core, any research team)\n\nTop 3\n\nWhat research bets matter most in the next 6–12 months, and how will you measure success?\nWhat makes a “hell-yes” hire here after 90 days? What work would signal that?\nHow do ideas transition from a paper/prototype to production or a public result?\n\nHow is impact recognized—publications, product metrics, patents, internal adoption?\nWhat are examples of projects that didn’t land? Why, and what changed afterward?\nWhere are the biggest data/infra bottlenecks that a new scientist can unlock?\n\n\n\n\nResearch Scientists (peer scientists)\n\nTop 3\n\nWhich canonical datasets/eval harnesses are used for your area? How are baselines enforced?\nWhat’s a recent ablation or negative result that changed your roadmap?\nHow do you share/replicate experiments (internal tooling, seeds, result store)?\n\nHow are collaborations formed across teams? Any “platform” teams I should align with?\nWhat’s the cadence for paper reviews, reading groups, and internal talks?\n\n\n\n\nEngineers / MLEs (production & infra)\n\nTop 3\n\nWhat are the serving constraints (latency, throughput, cost) and reliability SLOs?\nWhat’s the path from notebook → feature store → online eval → rollout/rollback?\nHow do you monitor drift and failures in the wild? What’s the on-call/ownership model?\n\nWhat’s the CI/CD story for models (gating tests, shadow, canary, A/B infra)?\nWhat would you change in our current stack if you could?\n\n\n\n\nPM / Cross-functional (applied impact)\n\nTop 3\n\nWhich decision or workflow actually changes if this model improves by X%?\nWhat is the single metric you’d show leadership to justify continued investment?\nWhat risks (safety, bias, misuse) keep you up at night for this application?\n\nWhere does data come from and how does quality/coverage limit the roadmap?\n\n\n\n\nRecruiter / Compensation\n\nLevel targeting and calibration—what evidence best demonstrates readiness for this level?\nPublication & open-source policy (authors, timing, preprints), conference travel norms.\nVisa/relocation timeline; expected hire start window and interview re-try policy."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#domain-specific-add-ons",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#domain-specific-add-ons",
    "title": "Research Scientist Interview Guide",
    "section": "Domain-specific add-ons",
    "text": "Domain-specific add-ons\n\nIf the team is RL / Control (energy, robotics, autonomy)\n\nWhat control horizon & loop latency are assumed (e.g., 200 ms, 1 s)? Any hard real-time constraints?\nHow are safety constraints enforced (e.g., physics-informed losses, shielded RL, reachability)?\nWhat are the reference environments (e.g., CityLearn/PowerGym, IEEE 13/34/123 bus, HIL)?\nHow do sim-to-real gaps show up, and how do you mitigate them (domain randomization, calibration)?\nWhich offline evaluation and counterfactual replay methods are trusted before online trials?\nWhat’s the bar for replacing a heuristic/OPF with an RL policy (guardrails, rollback, audits)?\n\n\n\nIf the team is LLM / RAG / GenAI\n\nRetrieval stack: index type, chunking, rerankers, eval (nDCG, recall@k, answer faithfulness).\nHallucination budget & safety: filters/guardrails, red-teaming, and incident handling.\nFinetuning strategy: SFT/LoRA vs. prompt-only; distillation plans; model/versioning policy.\nWhat constitutes “win” in offline eval vs. human eval? How do you resolve conflicts?\nData governance: PII/PHI handling, dedup, license compliance, auto-eval for drift.\n\n\n\nIf the team is Vision / Multimodal\n\nCanonical datasets & metrics (COCO mAP, IoU, retrieval R@k); how are domain shifts handled?\nLabeling strategy & quality control; synthetic data or augmentation policies.\nDeployment constraints: throughput on edge vs. server; quantization/compilation toolchain.\nFailure modes that matter most (false positives/negatives, OOD, adversarial artifacts)."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#if-you-only-have-time-for-3-questions-universal",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#if-you-only-have-time-for-3-questions-universal",
    "title": "Research Scientist Interview Guide",
    "section": "If you only have time for 3 questions (universal)",
    "text": "If you only have time for 3 questions (universal)\n\nStrategy: What is the one result you’d want me to deliver in 6 months that proves this hire was a “yes”?\n\nExecution: What is the critical bottleneck (data, infra, evaluation) preventing faster progress today?\n\nFit: Which strengths would make me the complement to the current team’s skills?"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/CopyOfindex.html#example-domain-specific",
    "href": "blog/technotes_20250703_research_guide/CopyOfindex.html#example-domain-specific",
    "title": "Research Scientist Interview Guide",
    "section": "Example Domain-specific",
    "text": "Example Domain-specific\n\nIf the team is RL / Control (energy, robotics, autonomy)\n\nWhat control horizon & loop latency are assumed (e.g., 200 ms, 1 s)? Any hard real-time constraints?\nHow are safety constraints enforced (e.g., physics-informed losses, shielded RL, reachability)?\nWhat are the reference environments (e.g., CityLearn/PowerGym, IEEE 13/34/123 bus, HIL)?\nHow do sim-to-real gaps show up, and how do you mitigate them (domain randomization, calibration)?\nWhich offline evaluation and counterfactual replay methods are trusted before online trials?\nWhat’s the bar for replacing a heuristic/OPF with an RL policy (guardrails, rollback, audits)?\n\n\n\nIf the team is LLM / RAG / GenAI\n\nRetrieval stack: index type, chunking, rerankers, eval (nDCG, recall@k, answer faithfulness).\nHallucination budget & safety: filters/guardrails, red-teaming, and incident handling.\nFinetuning strategy: SFT/LoRA vs. prompt-only; distillation plans; model/versioning policy.\nWhat constitutes “win” in offline eval vs. human eval? How do you resolve conflicts?\nData governance: PII/PHI handling, dedup, license compliance, auto-eval for drift.\n\n\n\nIf the team is Vision / Multimodal\n\nCanonical datasets & metrics (COCO mAP, IoU, retrieval R@k); how are domain shifts handled?\nLabeling strategy & quality control; synthetic data or augmentation policies.\nDeployment constraints: throughput on edge vs. server; quantization/compilation toolchain.\nFailure modes that matter most (false positives/negatives, OOD, adversarial artifacts).\n\n\n\n\nIf you only have time for 3 questions (universal)\n\nStrategy: What is the one result you’d want me to deliver in 6 months that proves this hire was a “yes”?\n\nExecution: What is the critical bottleneck (data, infra, evaluation) preventing faster progress today?\n\nFit: Which strengths would make me the complement to the current team’s skills?"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#behavioral-star-research-flavors",
    "href": "blog/technotes_20250703_research_guide/index.html#behavioral-star-research-flavors",
    "title": "Research Scientist Interview Guide",
    "section": "Behavioral: STAR + research flavors",
    "text": "Behavioral: STAR + research flavors\nUse STAR: Situation, Task, Action, Result, Learning.\nPrepare 6 stories: conflict, failure, leadership without authority, speed vs quality, mentoring, cross-team project.\nExample prompt\n“Tell me about a time your experiment invalidated a roadmap item.”\n- S/T: critical Q3 milestone hinged on SOTA surpassing baseline.\n- A: pre-registered analysis; ran holdout; flagged negative lift; proposed minimal viable alternative.\n- R: saved ~6 wks eng time; reallocated to data quality; shipped smaller win.\n- L: add “early stop” gates; improved pre-mortem checklist."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#ask-the-interviewer",
    "href": "blog/technotes_20250703_research_guide/index.html#ask-the-interviewer",
    "title": "Research Scientist Interview Guide",
    "section": "“Ask the interviewer”",
    "text": "“Ask the interviewer”\n\nUse the set that best fits the team. If time is short, ask the Top 3 in each block.\n\n\nHiring Manager (core, any research team)\n\nTop 3\n\nWhat research bets matter most in the next 6–12 months, and how will you measure success?\nWhat makes a “hell-yes” hire here after 90 days? What work would signal that?\nHow do ideas transition from a paper/prototype to production or a public result?\n\nHow is impact recognized—publications, product metrics, patents, internal adoption?\nWhat are examples of projects that didn’t land? Why, and what changed afterward?\nWhere are the biggest data/infra bottlenecks that a new scientist can unlock?\n\n\n\n\nResearch Scientists (peer scientists)\n\nTop 3\n\nWhich canonical datasets/eval harnesses are used for your area? How are baselines enforced?\nWhat’s a recent ablation or negative result that changed your roadmap?\nHow do you share/replicate experiments (internal tooling, seeds, result store)?\n\nHow are collaborations formed across teams? Any “platform” teams I should align with?\nWhat’s the cadence for paper reviews, reading groups, and internal talks?\n\n\n\n\nEngineers / MLEs (production & infra)\n\nTop 3\n\nWhat are the serving constraints (latency, throughput, cost) and reliability SLOs?\nWhat’s the path from notebook → feature store → online eval → rollout/rollback?\nHow do you monitor drift and failures in the wild? What’s the on-call/ownership model?\n\nWhat’s the CI/CD story for models (gating tests, shadow, canary, A/B infra)?\nWhat would you change in our current stack if you could?\n\n\n\n\nPM / Cross-functional (applied impact)\n\nTop 3\n\nWhich decision or workflow actually changes if this model improves by X%?\nWhat is the single metric you’d show leadership to justify continued investment?\nWhat risks (safety, bias, misuse) keep you up at night for this application?\n\nWhere does data come from and how does quality/coverage limit the roadmap?\n\n\n\n\nRecruiter / Compensation\n\nLevel targeting and calibration—what evidence best demonstrates readiness for this level?\nPublication & open-source policy (authors, timing, preprints), conference travel norms.\nVisa/relocation timeline; expected hire start window and interview re-try policy."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#example-domain-specific",
    "href": "blog/technotes_20250703_research_guide/index.html#example-domain-specific",
    "title": "Research Scientist Interview Guide",
    "section": "Example Domain-specific",
    "text": "Example Domain-specific\n\nIf the team is RL / Control (energy, robotics, autonomy)\n\nWhat control horizon & loop latency are assumed (e.g., 200 ms, 1 s)? Any hard real-time constraints?\nHow are safety constraints enforced (e.g., physics-informed losses, shielded RL, reachability)?\nWhat are the reference environments (e.g., CityLearn/PowerGym, IEEE 13/34/123 bus, HIL)?\nHow do sim-to-real gaps show up, and how do you mitigate them (domain randomization, calibration)?\nWhich offline evaluation and counterfactual replay methods are trusted before online trials?\nWhat’s the bar for replacing a heuristic/OPF with an RL policy (guardrails, rollback, audits)?\n\n\n\nIf the team is LLM / RAG / GenAI\n\nRetrieval stack: index type, chunking, rerankers, eval (nDCG, recall@k, answer faithfulness).\nHallucination budget & safety: filters/guardrails, red-teaming, and incident handling.\nFinetuning strategy: SFT/LoRA vs. prompt-only; distillation plans; model/versioning policy.\nWhat constitutes “win” in offline eval vs. human eval? How do you resolve conflicts?\nData governance: PII/PHI handling, dedup, license compliance, auto-eval for drift.\n\n\n\nIf the team is Vision / Multimodal\n\nCanonical datasets & metrics (COCO mAP, IoU, retrieval R@k); how are domain shifts handled?\nLabeling strategy & quality control; synthetic data or augmentation policies.\nDeployment constraints: throughput on edge vs. server; quantization/compilation toolchain.\nFailure modes that matter most (false positives/negatives, OOD, adversarial artifacts).\n\n\n\n\nIf you only have time for 3 questions (universal)\n\nStrategy: What is the one result you’d want me to deliver in 6 months that proves this hire was a “yes”?\n\nExecution: What is the critical bottleneck (data, infra, evaluation) preventing faster progress today?\n\nFit: Which strengths would make me the complement to the current team’s skills?"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#offer-debrief-negotiation",
    "href": "blog/technotes_20250703_research_guide/index.html#offer-debrief-negotiation",
    "title": "Research Scientist Interview Guide",
    "section": "Offer, Debrief & Negotiation",
    "text": "Offer, Debrief & Negotiation\n1) Right after the loop\n\nKeep a short brag doc: 5–8 bullets tying your work to measurable impact (citations, benchmarks, improvements).\nSend thank-you notes; ask the recruiter for decision timeline and whether the team needs any follow-ups (extra slides, code pointers).\n\n2) Debrief (if you don’t get detailed feedback)\n\nAsk for 3 bullets: (i) strengths that stood out, (ii) top concern, (iii) what would change the decision next time.\nIf there’s a miscalibration (e.g., level/scope), propose a targeted follow-up (short tech screen or focused deep dive).\n\n3) Offer review (when it comes)\n\nBreak it down: base + bonus + equity/RSUs (grant, vesting, refreshers) + sign-on + extras (compute budget, conference travel, publication policy, patent bonus).\nClarify: level, title, team mandate, location policy, start date, performance review cycles, and conference/OSS policies.\n\n4) Negotiation (impact-first)\n\nAnchor with scope & impact (what you can deliver in 6–12 months), plus comparables (peer offers or market data for the same level/geo).\nPrioritize asks (pick 2–3): sign-on / equity / level / research budget / conf travel.\n\nSample script:\n“Given the scope (X) and the impact I’m positioned to deliver (Y), I’m targeting total comp of Z at level L. If level is fixed, increasing equity by A or adding a B sign-on would bridge the gap. I’d also value C (e.g., conference travel commitment).”\n\n5) Timelines\n\nIf you need time: “I’m very excited. To make a well-considered decision, could we set a reply date of ? I want to complete one pending loop and compare scopes fairly.”\nIf there’s an exploding deadline, ask for a short extension in exchange for a firm decision date and clear intent.\n\n6) If rejected\n\nRequest specific growth areas and ask whether a re-interview window (e.g., 6 months) with a targeted bar (e.g., systems/experiments) is possible."
  },
  {
    "objectID": "others/cv.html",
    "href": "others/cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "📄 Resume"
  },
  {
    "objectID": "statistics/stat.html",
    "href": "statistics/stat.html",
    "title": "Statistics",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/cv.html",
    "href": "statistics/cv.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/sykdomspulsen/index.html",
    "href": "statistics/sykdomspulsen/index.html",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to Code. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nPaper"
  },
  {
    "objectID": "statistics/sykdomspulsen/index.html#overview",
    "href": "statistics/sykdomspulsen/index.html#overview",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to Code. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nPaper"
  },
  {
    "objectID": "statistics/index.html",
    "href": "statistics/index.html",
    "title": "Statistical Analysis",
    "section": "",
    "text": "Deep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nThis porfolio includes openly available educational material I have created, along with works, software, and tools I have contributed to or designed.\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nThis porfolio includes openly available educational material I have created, along with works, software, and tools I have contributed to or designed.\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/ehr-title/index.html",
    "href": "statistics/ehr-title/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr"
  },
  {
    "objectID": "statistics/robo.html",
    "href": "statistics/robo.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/os_teaching/index.html",
    "href": "statistics/os_teaching/index.html",
    "title": "Teach in R and Quarto",
    "section": "",
    "text": "MF9130E - Introductory course in statistics\n8-day intensive course on introductory statistics. April 2023 we made it with R rather than propriety software, coupled with live-coding sessions to enhance understanding of basic concepts such as distribution and hypothesis tests.\nCode\nCourse website\nRead more about the experience in\n\nblogpost\npresentation"
  },
  {
    "objectID": "statistics/prohect2.html",
    "href": "statistics/prohect2.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/drl.html",
    "href": "statistics/drl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/nor_mortality/index.html",
    "href": "statistics/nor_mortality/index.html",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "statistics/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "statistics/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "statistics/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "statistics/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Mortality Surveillance in Norway",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "statistics/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "statistics/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Mortality Surveillance in Norway",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "statistics/dl.html",
    "href": "statistics/dl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/dan/index.html",
    "href": "statistics/dan/index.html",
    "title": "Data Apothecary’s Notes",
    "section": "",
    "text": "(This is my own note-taking system using quarto)\n\nAbout the notes\nData Apothecary’s Notes is a note-taking repository for modern data science skills with a focus on drug development and clinical trials. Content will be gradually added while I learn the topics. Therefore, it is by no means a complete guide by the time you read it!\nI try to organize the content in a modular way. I think these should cover the important aspects in which a data scientist / modern statistician should know.\n\nstudy design\ninference\nmodels\nreporting\nprogramming\n\n\n\nWhy quarto\nIn short, quarto has the advantage of making a very well structured website with code chunks easy. No more worry about scattered notes in different folders - put them together, render it so you can find your notes quickly!"
  },
  {
    "objectID": "statistics/projects.html",
    "href": "statistics/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package Code to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\nNo matching items\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistics/noreden/index.html",
    "href": "statistics/noreden/index.html",
    "title": "Noreden",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "statistics/phuse/index.html",
    "href": "statistics/phuse/index.html",
    "title": "PHUSE - CAMIS",
    "section": "",
    "text": "I am contributing to two working groups at PHUSE: CAMIS, and RWD - Real World Data Guideline (early stage).\n\nCAMIS: Comparing Analysis Method Implementations in Software\nCAMIS is a cross-industry group formed of members from PHUSE, PSI and ASA.\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comparison and comprehensive explanations.\n\n\nRWD Working Group\nThis is a newly formed working group, working on statistical programming guidelines while working on RWD (read world data)."
  },
  {
    "objectID": "statistics/projects1.html",
    "href": "statistics/projects1.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stat_ml/rstats_20230721_teaching/index.html",
    "href": "stat_ml/rstats_20230721_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Time and place: July 21, 2023 10AM. Roche office, Basel, Switzerland\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "stat_ml/rstats_20230721_teaching/index.html#about-the-topic",
    "href": "stat_ml/rstats_20230721_teaching/index.html#about-the-topic",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "About the topic",
    "text": "About the topic\nThe 8 day introductory statistics course (MF9130) at the Faculty of Medicine, University of Oslo is designed for PhD students in medicine, biology, psychology and other health related fields. Similar to other conventional teaching methods, the course has been focusing largely on theory and hand calculation. The software has been Stata and SPSS, and data analysis was mostly left for the students to figure out on their own.\nThis year, we made an attempt to transform the course with R, and aimed to teach more practical data analysis skills. We added one session per day where the instructor guide students on R and project management, importing data , basic manipulation and statistical methods. The IT skills of the students vary greatly, and therefore we used the ‘sticky notes’ help system borrowed from the Carpentries to make sure everyone could get help in the first days. We have created a course website using Quarto, where all the material and R exercises (with rendered solution) are available for self-study. We have witnessed amazing progress - by the end of the first week, students with the least computer / data skills were able to work on dataframes, make basic plots and do a chi-squared test. This helps build students confidence in data and statistics, and as a result, they can start to work on their own datasets using the skills immediately."
  },
  {
    "objectID": "stat_ml/rstats_20190402_blogdown/index.html",
    "href": "stat_ml/rstats_20190402_blogdown/index.html",
    "title": "Building Website in R: Step by Step Introduction to blogdown",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "stat_ml/community_20240921_quartofriends/index.html",
    "href": "stat_ml/community_20240921_quartofriends/index.html",
    "title": "Use Quarto, Make Friends",
    "section": "",
    "text": "It has been two years since Quarto became the most popular reproducible publication tool in data science and R community. However Quarto is so much more than just a publication tool! I started using it since late 2022, and it has helped me become more organized, productive and connected with people in the data science community.\nIn this talk I will not focus on the technical aspects on ‘how’ to use this tool. In the first part of the talk, I would like to report the latest news and trends seen in the useR conference and Posit conf, the two biggest global R events. In the second part, I will share my own experience in using Quarto for my career: from learning new skills, collaborating with co-workers, teaching university courses to networking and building a community (CAMIS collaboration). It is a powerful tool to share your work, and make new connections - both for work and for fun! I hope this talk will provide you with some new ideas on how to use this fantastic technology to fulfill your goals."
  },
  {
    "objectID": "stat_ml/ehr_20210218_biday/index.html",
    "href": "stat_ml/ehr_20210218_biday/index.html",
    "title": "Network Analysis of Hospital EHR data",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "stat_ml/index.html",
    "href": "stat_ml/index.html",
    "title": "Statistical Machine Learning",
    "section": "",
    "text": "Statistical Analysis\n\n\nStatistical ML\nExplore notes and resources on Statistics and Statistical Machine Learning\n\n\nStatistical Analysis\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nDate\n\n\n\n\n\n\n\n\nUse Quarto, Make Friends\n\n\nKolkata UseR meetup\n\n\n2024-09-21\n\n\n\n\n\n\nOne step closer to better Electronic Health Records data\n\n\nPHUSE Single Day Event Basel\n\n\n2024-09-18\n\n\n\n\n\n\nCAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n\n\n\n\n\n2024-07-10\n\n\n\n\n\n\nA one year recap on teaching statistcis to medical students: how can R and Quarto help?\n\n\nR/Medicine 2024 - Online\n\n\n2024-06-13\n\n\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\nBasel R meeting\n\n\n2023-07-21\n\n\n\n\n\n\nSykdomspulsen: An automated public health surveillance platform\n\n\nOslo UseR meetup \n\n\n2022-06-16\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stat_ml/index.html#upcoming",
    "href": "stat_ml/index.html#upcoming",
    "title": "Statistical Machine Learning",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "stat_ml/index.html#selected-previous-talks",
    "href": "stat_ml/index.html#selected-previous-talks",
    "title": "Statistical Machine Learning",
    "section": "Selected previous talks",
    "text": "Selected previous talks\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nDate\n\n\n\n\n\n\n\n\nUse Quarto, Make Friends\n\n\nKolkata UseR meetup\n\n\n2024-09-21\n\n\n\n\n\n\nOne step closer to better Electronic Health Records data\n\n\nPHUSE Single Day Event Basel\n\n\n2024-09-18\n\n\n\n\n\n\nCAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n\n\n\n\n\n2024-07-10\n\n\n\n\n\n\nA one year recap on teaching statistcis to medical students: how can R and Quarto help?\n\n\nR/Medicine 2024 - Online\n\n\n2024-06-13\n\n\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\nBasel R meeting\n\n\n2023-07-21\n\n\n\n\n\n\nSykdomspulsen: An automated public health surveillance platform\n\n\nOslo UseR meetup \n\n\n2022-06-16\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stat_ml/rstats_20240613_teaching/index.html",
    "href": "stat_ml/rstats_20240613_teaching/index.html",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "",
    "text": "Time and place: June 13 2024. Online\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "stat_ml/rstats_20240613_teaching/index.html#about-the-topic",
    "href": "stat_ml/rstats_20240613_teaching/index.html#about-the-topic",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "About the topic",
    "text": "About the topic\nThe Department of Biostatistics at University of Oslo offer statistics courses at different levels for medical students and PhD candidates with clinical backgrounds. The courses were traditionally taught with a focus on theory instead of data analysis, where SPSS and STATA were the tools of choice.\nSince 2023 spring semester, we have been gradually transforming some of our statistics courses into R, using Quarto course websites and Carpentries style live-coding instruction. With new Quarto tools (such as WebR) we also added interactivity in the code blocks. So far we have transformed two courses with over 100 students who have almost no programming experience. We have observed impressive progress in the skill development, and received significantly more positive feedback when it comes to statistics education.\nIn this talk, I would like to share our experience on the successes and challenges throughout the process. Looking back, is it cost-effective? Definitely. Can we do better in the future? Almost surely. If you are also planning to adopt new technology in your teaching activities, join us to learn more about what you can do to make the transition happen!\nCourse website can be accessed here"
  },
  {
    "objectID": "stat_ml/ehr_20221013_ml_icu/index.html",
    "href": "stat_ml/ehr_20221013_ml_icu/index.html",
    "title": "Machine Learning in Intensive Care Units",
    "section": "",
    "text": "A 45 minutes trial lecture to fulfill the requirement of my PhD degree."
  },
  {
    "objectID": "computer_vision/ph_20230330_sp/index.html",
    "href": "computer_vision/ph_20230330_sp/index.html",
    "title": "Public health surveillance and reporting",
    "section": "",
    "text": "Time and place: Mar. 30, 2023 12:00 PM–1:00 PM\nHybrid: Georg Sverdrups hus and Zoom\nEvent page"
  },
  {
    "objectID": "computer_vision/ph_20230330_sp/index.html#about-the-topic",
    "href": "computer_vision/ph_20230330_sp/index.html#about-the-topic",
    "title": "Public health surveillance and reporting",
    "section": "About the topic",
    "text": "About the topic\nSituational awareness is key to fast response during a public health emergency, such as COVID-19 pandemic. However, making disease surveillance reports that cover different geographical units for various metrics and data registries is both resource intensive and time consuming. Open source tools such as R packages, GitHub and Airflow can make this process automatic, reproducible and scalable.\nEvery day during the pandemic, Sykdomspulsen team at the Norwegian Institute of Public Health (FHI/NIPH) fetched data from more than 15 data sources, cleaned, censored datasets and carried out a wide range of statistical analyses. Over 1000 situational reports containing automated graphs and tables were produced before breakfast time.\nGrab you matpakke and join us for a presentation from Chi Zhang about how Sykdomspulsen team used and developed open source software to make public health surveillance and reporting more efficient, followed up by a discussion on the benefits and concerns of making these data public. We will end with an open Q&A session as usual!"
  },
  {
    "objectID": "computer_vision/ehr_20240918_betterehr/index.html",
    "href": "computer_vision/ehr_20240918_betterehr/index.html",
    "title": "One step closer to better Electronic Health Records data",
    "section": "",
    "text": "Real-World Data (RWD) like Electronic Health Records (EHR) is crucial for understanding drug usage and various treatments and generating Real-World Evidence (RWE). Risk prediction has been a major application where EHR is used, and there is now a shift towards causal inference, which requires data of even higher quality. Patients undergo treatments (drugs, procedures) at various times during their hospital stays, yet the data being recorded are messy and error-prone for various reasons. Analysts spend significant amount of time to sit together with clinicians to identify and understand abnormal records, and unfortunately this process is challenging to automate.\nThis talk will use an example on antibiotics prescription and use at a Nordic hospital to illustrate how some EHR systems can improve for better clinical decision-making and better data for research. I will also introduce a pilot R package (ggehr) that facilitates visual exploration of EHR data, and how it can help reconstruct patient journeys and enable analysts to perform effective quality control."
  },
  {
    "objectID": "computer_vision/community_20240710_camis/index.html",
    "href": "computer_vision/community_20240710_camis/index.html",
    "title": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations",
    "section": "",
    "text": "2024.7.8-11, Salzburg, Austria. Conference link: UseR!\nStatisticians using multiple softwares (SAS, R, Python) will have found differences in analysis results that warrant further justification. Whilst some industries may accept results not being the same as long as they are “close”, the highly regulated pharmaceutical industry would require an identical match in results. Yet, discrepancies might still occur, and knowing the reasons (different methods, options, algorithms etc) is critical to the modern statistician and subsequent regulatory submissions.\nIn this talk I will introduce CAMIS: Comparing Analysis Method Implementations in Software. https://psiaims.github.io/CAMIS/ It is a joint-project between PHUSE, the R Validation Hub, PSI AIMS, R consortium and openstatsware. The aim of CAMIS is to investigate and document differences and similarities between different statistical softwares such as SAS and R. We use Quarto and Github to document methods, algorithms and comparisons between softwares through small case studies, and all articles are contributed by the community. In the transition from proprietary to open source technology in the industry, CAMIS can serve as a guidebook to navigate this process.\n\nkeywords: cross industry collaboration, multi-lingua, open-source, quarto"
  },
  {
    "objectID": "computer_vision/dummy_talk/index.html",
    "href": "computer_vision/dummy_talk/index.html",
    "title": "My Dummy Talk",
    "section": "",
    "text": "This is a simple demo of a Quarto talk listing with an image, title, and subtitle."
  },
  {
    "objectID": "computer_vision/ph_20220616_splverse/index.html",
    "href": "computer_vision/ph_20220616_splverse/index.html",
    "title": "Sykdomspulsen: An automated public health surveillance platform",
    "section": "",
    "text": "About the talk\nWatch the talk on YouTube\nSykdomspulsen is a real-time analysis and disease surveillance system designed at developed at the Norwegian Institute of Public Health (FHI). Sykdomspulsen processes new data collected from 15 data sources (e.g., covid-19 cases), runs 1000.000+ statistical analysis automatically for all locations (nation, county, municipality) in Norway, produces 1000+ reports and alerts for public health authorities and shares data to the public on GitHub.\nSykdomspulsen runs on a collection of R packages, the {splverse}. {splverse} is an ecosystem for infectious disease surveillance, from analysis planning, statistical analysis to reporting via visualization, shiny website and Rmarkdown generated reports. In this talk, Chi will present how Sykdomspulsen does public health real-time surveillance during the pandemic using R. Chi will introduce some of the core packages and illustrate how they work together, with an example using real surveillance data published daily on GitHub.\n\n\nAbout the speaker\nChi is currently working at the Sykdomspulsen team as a researcher and R developer, at the Norwegian Institute of Public Health. Before she joined Sykdomspulsen in the middle of the pandemic (2020), she was a PhD student at the Department of Biostatistics at University of Oslo (OCBE), working on hospital EHR data."
  },
  {
    "objectID": "self_driving/rstats_20230721_teaching/index.html",
    "href": "self_driving/rstats_20230721_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Time and place: July 21, 2023 10AM. Roche office, Basel, Switzerland\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "self_driving/rstats_20230721_teaching/index.html#about-the-topic",
    "href": "self_driving/rstats_20230721_teaching/index.html#about-the-topic",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "About the topic",
    "text": "About the topic\nThe 8 day introductory statistics course (MF9130) at the Faculty of Medicine, University of Oslo is designed for PhD students in medicine, biology, psychology and other health related fields. Similar to other conventional teaching methods, the course has been focusing largely on theory and hand calculation. The software has been Stata and SPSS, and data analysis was mostly left for the students to figure out on their own.\nThis year, we made an attempt to transform the course with R, and aimed to teach more practical data analysis skills. We added one session per day where the instructor guide students on R and project management, importing data , basic manipulation and statistical methods. The IT skills of the students vary greatly, and therefore we used the ‘sticky notes’ help system borrowed from the Carpentries to make sure everyone could get help in the first days. We have created a course website using Quarto, where all the material and R exercises (with rendered solution) are available for self-study. We have witnessed amazing progress - by the end of the first week, students with the least computer / data skills were able to work on dataframes, make basic plots and do a chi-squared test. This helps build students confidence in data and statistics, and as a result, they can start to work on their own datasets using the skills immediately."
  },
  {
    "objectID": "self_driving/rstats_20190402_blogdown/index.html",
    "href": "self_driving/rstats_20190402_blogdown/index.html",
    "title": "Building Website in R: Step by Step Introduction to blogdown",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "self_driving/community_20240921_quartofriends/index.html",
    "href": "self_driving/community_20240921_quartofriends/index.html",
    "title": "Use Quarto, Make Friends",
    "section": "",
    "text": "It has been two years since Quarto became the most popular reproducible publication tool in data science and R community. However Quarto is so much more than just a publication tool! I started using it since late 2022, and it has helped me become more organized, productive and connected with people in the data science community.\nIn this talk I will not focus on the technical aspects on ‘how’ to use this tool. In the first part of the talk, I would like to report the latest news and trends seen in the useR conference and Posit conf, the two biggest global R events. In the second part, I will share my own experience in using Quarto for my career: from learning new skills, collaborating with co-workers, teaching university courses to networking and building a community (CAMIS collaboration). It is a powerful tool to share your work, and make new connections - both for work and for fun! I hope this talk will provide you with some new ideas on how to use this fantastic technology to fulfill your goals."
  },
  {
    "objectID": "self_driving/ehr_20210218_biday/index.html",
    "href": "self_driving/ehr_20210218_biday/index.html",
    "title": "Network Analysis of Hospital EHR data",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "self_driving/index.html",
    "href": "self_driving/index.html",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "self_driving/index.html#upcoming",
    "href": "self_driving/index.html#upcoming",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "self_driving/index.html#selected-previous-talks",
    "href": "self_driving/index.html#selected-previous-talks",
    "title": "Talks",
    "section": "Selected previous talks",
    "text": "Selected previous talks\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nDate\n\n\n\n\n\n\n\n\nUse Quarto, Make Friends\n\n\nKolkata UseR meetup\n\n\n2024-09-21\n\n\n\n\n\n\nOne step closer to better Electronic Health Records data\n\n\nPHUSE Single Day Event Basel\n\n\n2024-09-18\n\n\n\n\n\n\nCAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n\n\n\n\n\n2024-07-10\n\n\n\n\n\n\nA one year recap on teaching statistcis to medical students: how can R and Quarto help?\n\n\nR/Medicine 2024 - Online\n\n\n2024-06-13\n\n\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\nBasel R meeting\n\n\n2023-07-21\n\n\n\n\n\n\nSykdomspulsen: An automated public health surveillance platform\n\n\nOslo UseR meetup \n\n\n2022-06-16\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "self_driving/rstats_20240613_teaching/index.html",
    "href": "self_driving/rstats_20240613_teaching/index.html",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "",
    "text": "Time and place: June 13 2024. Online\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "self_driving/rstats_20240613_teaching/index.html#about-the-topic",
    "href": "self_driving/rstats_20240613_teaching/index.html#about-the-topic",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "About the topic",
    "text": "About the topic\nThe Department of Biostatistics at University of Oslo offer statistics courses at different levels for medical students and PhD candidates with clinical backgrounds. The courses were traditionally taught with a focus on theory instead of data analysis, where SPSS and STATA were the tools of choice.\nSince 2023 spring semester, we have been gradually transforming some of our statistics courses into R, using Quarto course websites and Carpentries style live-coding instruction. With new Quarto tools (such as WebR) we also added interactivity in the code blocks. So far we have transformed two courses with over 100 students who have almost no programming experience. We have observed impressive progress in the skill development, and received significantly more positive feedback when it comes to statistics education.\nIn this talk, I would like to share our experience on the successes and challenges throughout the process. Looking back, is it cost-effective? Definitely. Can we do better in the future? Almost surely. If you are also planning to adopt new technology in your teaching activities, join us to learn more about what you can do to make the transition happen!\nCourse website can be accessed here"
  },
  {
    "objectID": "self_driving/ehr_20221013_ml_icu/index.html",
    "href": "self_driving/ehr_20221013_ml_icu/index.html",
    "title": "Machine Learning in Intensive Care Units",
    "section": "",
    "text": "A 45 minutes trial lecture to fulfill the requirement of my PhD degree."
  },
  {
    "objectID": "self_driving/ph_20220616_splverse/index.html",
    "href": "self_driving/ph_20220616_splverse/index.html",
    "title": "Sykdomspulsen: An automated public health surveillance platform",
    "section": "",
    "text": "About the talk\nWatch the talk on YouTube\nSykdomspulsen is a real-time analysis and disease surveillance system designed at developed at the Norwegian Institute of Public Health (FHI). Sykdomspulsen processes new data collected from 15 data sources (e.g., covid-19 cases), runs 1000.000+ statistical analysis automatically for all locations (nation, county, municipality) in Norway, produces 1000+ reports and alerts for public health authorities and shares data to the public on GitHub.\nSykdomspulsen runs on a collection of R packages, the {splverse}. {splverse} is an ecosystem for infectious disease surveillance, from analysis planning, statistical analysis to reporting via visualization, shiny website and Rmarkdown generated reports. In this talk, Chi will present how Sykdomspulsen does public health real-time surveillance during the pandemic using R. Chi will introduce some of the core packages and illustrate how they work together, with an example using real surveillance data published daily on GitHub.\n\n\nAbout the speaker\nChi is currently working at the Sykdomspulsen team as a researcher and R developer, at the Norwegian Institute of Public Health. Before she joined Sykdomspulsen in the middle of the pandemic (2020), she was a PhD student at the Department of Biostatistics at University of Oslo (OCBE), working on hospital EHR data."
  },
  {
    "objectID": "self_driving/dummy_talk/index.html",
    "href": "self_driving/dummy_talk/index.html",
    "title": "My Dummy Talk",
    "section": "",
    "text": "This is a simple demo of a Quarto talk listing with an image, title, and subtitle."
  },
  {
    "objectID": "self_driving/community_20240710_camis/index.html",
    "href": "self_driving/community_20240710_camis/index.html",
    "title": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations",
    "section": "",
    "text": "2024.7.8-11, Salzburg, Austria. Conference link: UseR!\nStatisticians using multiple softwares (SAS, R, Python) will have found differences in analysis results that warrant further justification. Whilst some industries may accept results not being the same as long as they are “close”, the highly regulated pharmaceutical industry would require an identical match in results. Yet, discrepancies might still occur, and knowing the reasons (different methods, options, algorithms etc) is critical to the modern statistician and subsequent regulatory submissions.\nIn this talk I will introduce CAMIS: Comparing Analysis Method Implementations in Software. https://psiaims.github.io/CAMIS/ It is a joint-project between PHUSE, the R Validation Hub, PSI AIMS, R consortium and openstatsware. The aim of CAMIS is to investigate and document differences and similarities between different statistical softwares such as SAS and R. We use Quarto and Github to document methods, algorithms and comparisons between softwares through small case studies, and all articles are contributed by the community. In the transition from proprietary to open source technology in the industry, CAMIS can serve as a guidebook to navigate this process.\n\nkeywords: cross industry collaboration, multi-lingua, open-source, quarto"
  },
  {
    "objectID": "self_driving/ehr_20240918_betterehr/index.html",
    "href": "self_driving/ehr_20240918_betterehr/index.html",
    "title": "One step closer to better Electronic Health Records data",
    "section": "",
    "text": "Real-World Data (RWD) like Electronic Health Records (EHR) is crucial for understanding drug usage and various treatments and generating Real-World Evidence (RWE). Risk prediction has been a major application where EHR is used, and there is now a shift towards causal inference, which requires data of even higher quality. Patients undergo treatments (drugs, procedures) at various times during their hospital stays, yet the data being recorded are messy and error-prone for various reasons. Analysts spend significant amount of time to sit together with clinicians to identify and understand abnormal records, and unfortunately this process is challenging to automate.\nThis talk will use an example on antibiotics prescription and use at a Nordic hospital to illustrate how some EHR systems can improve for better clinical decision-making and better data for research. I will also introduce a pilot R package (ggehr) that facilitates visual exploration of EHR data, and how it can help reconstruct patient journeys and enable analysts to perform effective quality control."
  },
  {
    "objectID": "self_driving/ph_20230330_sp/index.html",
    "href": "self_driving/ph_20230330_sp/index.html",
    "title": "Public health surveillance and reporting",
    "section": "",
    "text": "Time and place: Mar. 30, 2023 12:00 PM–1:00 PM\nHybrid: Georg Sverdrups hus and Zoom\nEvent page"
  },
  {
    "objectID": "self_driving/ph_20230330_sp/index.html#about-the-topic",
    "href": "self_driving/ph_20230330_sp/index.html#about-the-topic",
    "title": "Public health surveillance and reporting",
    "section": "About the topic",
    "text": "About the topic\nSituational awareness is key to fast response during a public health emergency, such as COVID-19 pandemic. However, making disease surveillance reports that cover different geographical units for various metrics and data registries is both resource intensive and time consuming. Open source tools such as R packages, GitHub and Airflow can make this process automatic, reproducible and scalable.\nEvery day during the pandemic, Sykdomspulsen team at the Norwegian Institute of Public Health (FHI/NIPH) fetched data from more than 15 data sources, cleaned, censored datasets and carried out a wide range of statistical analyses. Over 1000 situational reports containing automated graphs and tables were produced before breakfast time.\nGrab you matpakke and join us for a presentation from Chi Zhang about how Sykdomspulsen team used and developed open source software to make public health surveillance and reporting more efficient, followed up by a discussion on the benefits and concerns of making these data public. We will end with an open Q&A session as usual!"
  },
  {
    "objectID": "computer_vision/ehr_20221013_ml_icu/index.html",
    "href": "computer_vision/ehr_20221013_ml_icu/index.html",
    "title": "Machine Learning in Intensive Care Units",
    "section": "",
    "text": "A 45 minutes trial lecture to fulfill the requirement of my PhD degree."
  },
  {
    "objectID": "computer_vision/rstats_20240613_teaching/index.html",
    "href": "computer_vision/rstats_20240613_teaching/index.html",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "",
    "text": "Time and place: June 13 2024. Online\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "computer_vision/rstats_20240613_teaching/index.html#about-the-topic",
    "href": "computer_vision/rstats_20240613_teaching/index.html#about-the-topic",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "About the topic",
    "text": "About the topic\nThe Department of Biostatistics at University of Oslo offer statistics courses at different levels for medical students and PhD candidates with clinical backgrounds. The courses were traditionally taught with a focus on theory instead of data analysis, where SPSS and STATA were the tools of choice.\nSince 2023 spring semester, we have been gradually transforming some of our statistics courses into R, using Quarto course websites and Carpentries style live-coding instruction. With new Quarto tools (such as WebR) we also added interactivity in the code blocks. So far we have transformed two courses with over 100 students who have almost no programming experience. We have observed impressive progress in the skill development, and received significantly more positive feedback when it comes to statistics education.\nIn this talk, I would like to share our experience on the successes and challenges throughout the process. Looking back, is it cost-effective? Definitely. Can we do better in the future? Almost surely. If you are also planning to adopt new technology in your teaching activities, join us to learn more about what you can do to make the transition happen!\nCourse website can be accessed here"
  },
  {
    "objectID": "computer_vision/index.html",
    "href": "computer_vision/index.html",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "computer_vision/index.html#upcoming",
    "href": "computer_vision/index.html#upcoming",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "computer_vision/index.html#selected-previous-talks",
    "href": "computer_vision/index.html#selected-previous-talks",
    "title": "Talks",
    "section": "Selected previous talks",
    "text": "Selected previous talks\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nDate\n\n\n\n\n\n\n\n\nUse Quarto, Make Friends\n\n\nKolkata UseR meetup\n\n\n2024-09-21\n\n\n\n\n\n\nOne step closer to better Electronic Health Records data\n\n\nPHUSE Single Day Event Basel\n\n\n2024-09-18\n\n\n\n\n\n\nCAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n\n\n\n\n\n2024-07-10\n\n\n\n\n\n\nA one year recap on teaching statistcis to medical students: how can R and Quarto help?\n\n\nR/Medicine 2024 - Online\n\n\n2024-06-13\n\n\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\nBasel R meeting\n\n\n2023-07-21\n\n\n\n\n\n\nSykdomspulsen: An automated public health surveillance platform\n\n\nOslo UseR meetup \n\n\n2022-06-16\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "computer_vision/ehr_20210218_biday/index.html",
    "href": "computer_vision/ehr_20210218_biday/index.html",
    "title": "Network Analysis of Hospital EHR data",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "computer_vision/community_20240921_quartofriends/index.html",
    "href": "computer_vision/community_20240921_quartofriends/index.html",
    "title": "Use Quarto, Make Friends",
    "section": "",
    "text": "It has been two years since Quarto became the most popular reproducible publication tool in data science and R community. However Quarto is so much more than just a publication tool! I started using it since late 2022, and it has helped me become more organized, productive and connected with people in the data science community.\nIn this talk I will not focus on the technical aspects on ‘how’ to use this tool. In the first part of the talk, I would like to report the latest news and trends seen in the useR conference and Posit conf, the two biggest global R events. In the second part, I will share my own experience in using Quarto for my career: from learning new skills, collaborating with co-workers, teaching university courses to networking and building a community (CAMIS collaboration). It is a powerful tool to share your work, and make new connections - both for work and for fun! I hope this talk will provide you with some new ideas on how to use this fantastic technology to fulfill your goals."
  },
  {
    "objectID": "computer_vision/rstats_20190402_blogdown/index.html",
    "href": "computer_vision/rstats_20190402_blogdown/index.html",
    "title": "Building Website in R: Step by Step Introduction to blogdown",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "computer_vision/rstats_20230721_teaching/index.html",
    "href": "computer_vision/rstats_20230721_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Time and place: July 21, 2023 10AM. Roche office, Basel, Switzerland\nSlides for this talk can be accessed Code."
  },
  {
    "objectID": "computer_vision/rstats_20230721_teaching/index.html#about-the-topic",
    "href": "computer_vision/rstats_20230721_teaching/index.html#about-the-topic",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "About the topic",
    "text": "About the topic\nThe 8 day introductory statistics course (MF9130) at the Faculty of Medicine, University of Oslo is designed for PhD students in medicine, biology, psychology and other health related fields. Similar to other conventional teaching methods, the course has been focusing largely on theory and hand calculation. The software has been Stata and SPSS, and data analysis was mostly left for the students to figure out on their own.\nThis year, we made an attempt to transform the course with R, and aimed to teach more practical data analysis skills. We added one session per day where the instructor guide students on R and project management, importing data , basic manipulation and statistical methods. The IT skills of the students vary greatly, and therefore we used the ‘sticky notes’ help system borrowed from the Carpentries to make sure everyone could get help in the first days. We have created a course website using Quarto, where all the material and R exercises (with rendered solution) are available for self-study. We have witnessed amazing progress - by the end of the first week, students with the least computer / data skills were able to work on dataframes, make basic plots and do a chi-squared test. This helps build students confidence in data and statistics, and as a result, they can start to work on their own datasets using the skills immediately."
  },
  {
    "objectID": "stat_ml/ph_20220616_splverse/index.html",
    "href": "stat_ml/ph_20220616_splverse/index.html",
    "title": "Sykdomspulsen: An automated public health surveillance platform",
    "section": "",
    "text": "About the talk\nWatch the talk on YouTube\nSykdomspulsen is a real-time analysis and disease surveillance system designed at developed at the Norwegian Institute of Public Health (FHI). Sykdomspulsen processes new data collected from 15 data sources (e.g., covid-19 cases), runs 1000.000+ statistical analysis automatically for all locations (nation, county, municipality) in Norway, produces 1000+ reports and alerts for public health authorities and shares data to the public on GitHub.\nSykdomspulsen runs on a collection of R packages, the {splverse}. {splverse} is an ecosystem for infectious disease surveillance, from analysis planning, statistical analysis to reporting via visualization, shiny website and Rmarkdown generated reports. In this talk, Chi will present how Sykdomspulsen does public health real-time surveillance during the pandemic using R. Chi will introduce some of the core packages and illustrate how they work together, with an example using real surveillance data published daily on GitHub.\n\n\nAbout the speaker\nChi is currently working at the Sykdomspulsen team as a researcher and R developer, at the Norwegian Institute of Public Health. Before she joined Sykdomspulsen in the middle of the pandemic (2020), she was a PhD student at the Department of Biostatistics at University of Oslo (OCBE), working on hospital EHR data."
  },
  {
    "objectID": "stat_ml/dummy_talk/index.html",
    "href": "stat_ml/dummy_talk/index.html",
    "title": "My Dummy Talk",
    "section": "",
    "text": "This is a simple demo of a Quarto talk listing with an image, title, and subtitle."
  },
  {
    "objectID": "stat_ml/community_20240710_camis/index.html",
    "href": "stat_ml/community_20240710_camis/index.html",
    "title": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations",
    "section": "",
    "text": "2024.7.8-11, Salzburg, Austria. Conference link: UseR!\nStatisticians using multiple softwares (SAS, R, Python) will have found differences in analysis results that warrant further justification. Whilst some industries may accept results not being the same as long as they are “close”, the highly regulated pharmaceutical industry would require an identical match in results. Yet, discrepancies might still occur, and knowing the reasons (different methods, options, algorithms etc) is critical to the modern statistician and subsequent regulatory submissions.\nIn this talk I will introduce CAMIS: Comparing Analysis Method Implementations in Software. https://psiaims.github.io/CAMIS/ It is a joint-project between PHUSE, the R Validation Hub, PSI AIMS, R consortium and openstatsware. The aim of CAMIS is to investigate and document differences and similarities between different statistical softwares such as SAS and R. We use Quarto and Github to document methods, algorithms and comparisons between softwares through small case studies, and all articles are contributed by the community. In the transition from proprietary to open source technology in the industry, CAMIS can serve as a guidebook to navigate this process.\n\nkeywords: cross industry collaboration, multi-lingua, open-source, quarto"
  },
  {
    "objectID": "stat_ml/ehr_20240918_betterehr/index.html",
    "href": "stat_ml/ehr_20240918_betterehr/index.html",
    "title": "One step closer to better Electronic Health Records data",
    "section": "",
    "text": "Real-World Data (RWD) like Electronic Health Records (EHR) is crucial for understanding drug usage and various treatments and generating Real-World Evidence (RWE). Risk prediction has been a major application where EHR is used, and there is now a shift towards causal inference, which requires data of even higher quality. Patients undergo treatments (drugs, procedures) at various times during their hospital stays, yet the data being recorded are messy and error-prone for various reasons. Analysts spend significant amount of time to sit together with clinicians to identify and understand abnormal records, and unfortunately this process is challenging to automate.\nThis talk will use an example on antibiotics prescription and use at a Nordic hospital to illustrate how some EHR systems can improve for better clinical decision-making and better data for research. I will also introduce a pilot R package (ggehr) that facilitates visual exploration of EHR data, and how it can help reconstruct patient journeys and enable analysts to perform effective quality control."
  },
  {
    "objectID": "stat_ml/ph_20230330_sp/index.html",
    "href": "stat_ml/ph_20230330_sp/index.html",
    "title": "Public health surveillance and reporting",
    "section": "",
    "text": "Time and place: Mar. 30, 2023 12:00 PM–1:00 PM\nHybrid: Georg Sverdrups hus and Zoom\nEvent page"
  },
  {
    "objectID": "stat_ml/ph_20230330_sp/index.html#about-the-topic",
    "href": "stat_ml/ph_20230330_sp/index.html#about-the-topic",
    "title": "Public health surveillance and reporting",
    "section": "About the topic",
    "text": "About the topic\nSituational awareness is key to fast response during a public health emergency, such as COVID-19 pandemic. However, making disease surveillance reports that cover different geographical units for various metrics and data registries is both resource intensive and time consuming. Open source tools such as R packages, GitHub and Airflow can make this process automatic, reproducible and scalable.\nEvery day during the pandemic, Sykdomspulsen team at the Norwegian Institute of Public Health (FHI/NIPH) fetched data from more than 15 data sources, cleaned, censored datasets and carried out a wide range of statistical analyses. Over 1000 situational reports containing automated graphs and tables were produced before breakfast time.\nGrab you matpakke and join us for a presentation from Chi Zhang about how Sykdomspulsen team used and developed open source software to make public health surveillance and reporting more efficient, followed up by a discussion on the benefits and concerns of making these data public. We will end with an open Q&A session as usual!"
  },
  {
    "objectID": "publications/articles/pesgm2025.html",
    "href": "publications/articles/pesgm2025.html",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "",
    "text": "K. Kumar, K. Utkarsh, J. Wang, and H. V. Padullaparti, “Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems,” Proc. IEEE Power & Energy Society General Meeting (PESGM), 2025."
  },
  {
    "objectID": "publications/articles/pesgm2025.html#abstract",
    "href": "publications/articles/pesgm2025.html#abstract",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Abstract",
    "text": "Abstract\nThe integration of advanced metering infrastructure (AMI) into power distribution networks generates valuable data for tasks such as phase identification; however, the limited and unreliable availability of labeled data in the form of customer phase connectivity presents challenges. To address this issue, we propose a semi-supervised learning (SSL) bayesian framework that effectively leverages both limited labeled and unlimited unlabeled data.\n\nWhy Phase Identification Needs a New Approach ?\n\nProblem: Utilities don’t know which phase customers are connected to this affects voltage regulation, DER integration, and fault localization.\n\n\n\n\n\nFig. 1: Illustration of Semi-Supervised Learning Techniques\n\n\n\n\nChallenges & Motivation\n\n\n\nChallenge: Ground truth phase data is scarce, unreliable, and costly to collect.\n\n\nProblem: Supervised ML methods require large amounts of labeled data and often unavailable or unreliable.\n\n\nMotivation: How do we scale phase identification without needing tons of labeled data?"
  },
  {
    "objectID": "publications/articles/pesgm2025.html#contribution",
    "href": "publications/articles/pesgm2025.html#contribution",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Contribution",
    "text": "Contribution\nOur approach incorporates:\n\nSelf-training with an ensemble of multilayer perceptron classifiers.\nLabel spreading to propagate labels based on data similarity.\nBayesian Neural Networks (BNNs) for uncertainty estimation, improving confidence and reducing phase identification errors.\n\nKey Highlights:*\n\nAchieved ~98% ± 0.08 accuracy on real utility data (Duquesne Light Company) using minimal and unreliable labeled data.\nUncertainty-aware predictions reduce misclassification risk and improve smart grid reliability.\nCombines pseudo-labeling, graph-based SSL, and probabilistic modeling to handle data scarcity in real-world distribution networks.\n\nOur “SSL + Uncertainty Estimation” approach provides an efficient and scalable solution for phase identification in AMI data, enabling utilities to improve modeling, simulation, and operational decision-making."
  },
  {
    "objectID": "publications/articles/pesgm2025.html#semi-supervised-learning-framework",
    "href": "publications/articles/pesgm2025.html#semi-supervised-learning-framework",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Semi-Supervised Learning Framework",
    "text": "Semi-Supervised Learning Framework\nWe formulate SSL as a regularized optimization problem:Equation 1\n\\[\n\\min_{f \\in \\mathcal{F}} \\left[ \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\ell(f(x_i), y_i) + \\lambda R_u(f, \\mathcal{D}_U) \\right]\n\\tag{1}\\]\nWhere:\n\n( (, ) ): Supervised loss (cross-entropy)\n\n( R_u(f, _U) ): Unsupervised regularization term\n\n( ): Trade-off parameter\n\nThe challenge is designing ( R_u(f, _U) ) to effectively leverage unlabeled data.\n\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  (\\#eq:binom)\n\\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation @ref(eq:binom).\nWe formulate SSL as a regularized optimization problem:\n\\[\n\\min_{f \\in \\mathcal{F}} \\left[ \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\ell(f(x_i), y_i) + \\lambda R_u(f, \\mathcal{D}_U) \\right]\n\\] Where \\[a^2 + b^2 = d^2\\] is the Unsupervised regularization term\nWhere: \\[\nR_u(f, \\mathcal{D}_U)\n\\] is the - ( (, ) ): Supervised loss (cross-entropy)\n- ( R_u(f, _U) ): Unsupervised regularization term\n- ( ): Trade-off parameter\nThe challenge is designing ( R_u(f, _U) ) to effectively leverage unlabeled data.\n\n\\[\\ell(\\cdot, \\cdot)\\]: Supervised loss (cross-entropy)\n\\[R_u(f, \\mathcal{D}_U)\\]: Unsupervised regularization term\n\\[\\lambda\\]: Trade-off parameter\n( (, ) ): Supervised loss (cross-entropy)\n\n( R_u(f, _U) ): Unsupervised regularization term\n\n( ): Trade-off parameter\n\nWe formulate SSL as a regularized optimization problem: min_{f∈ℱ} [ (1/n_L) ∑^{n_L}_{i=1} ℓ(f(x_i), y_i) + λR_u(f, 𝒟_U) ] Where:\nℓ(·,·): Supervised loss (cross-entropy) R_u(f, 𝒟_U): Unsupervised regularization term λ: Trade-off parameter\nThe challenge: designing R_u(f, 𝒟_U) that effectively exploits unlabeled data structure ## Methodology\nWe define the binomial distribution as:(Equation 2)\n\\[\nf(k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\tag{2}\\]\nAs shown in Equation @ref(eq:binom), the binomial function defines the probability…\nBlack-Scholes (Equation 3) is a mathematical model that seeks to explain the behavior of financial derivatives, most commonly options:\n\\[\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm S^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C\n\\tag{3}\\]\n\n\nProposed SSL Framework Applied to AMI Data\n\n\n\nDistribution Feeder Topology\n\n\n\nTraining and Testing Data Partitions\n\n\n\nAccuracy Comparison of SSL Methods\n\n\n\n\n\n\nPresentation\n\n\n\n\n View Fullscreen Poster"
  },
  {
    "objectID": "docs/publications/articles/naps2025.html",
    "href": "docs/publications/articles/naps2025.html",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "",
    "text": "Caumiant, E. P., Kang, D., Girard, J. M., & Fairbairn, C. E. (in press). Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication. Psychology of Addictive Behaviors."
  },
  {
    "objectID": "docs/publications/articles/naps2025.html#citation-apa-7",
    "href": "docs/publications/articles/naps2025.html#citation-apa-7",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "",
    "text": "Caumiant, E. P., Kang, D., Girard, J. M., & Fairbairn, C. E. (in press). Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication. Psychology of Addictive Behaviors."
  },
  {
    "objectID": "docs/publications/articles/naps2025.html#abstract",
    "href": "docs/publications/articles/naps2025.html#abstract",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "Abstract",
    "text": "Abstract\nObjective: Emotion measurement is central to capturing acute alcohol reinforcement and so to informing models of alcohol use disorder etiology. Yet our understanding of how alcohol impacts emotion as assessed across diverse response modalities remains incomplete. The present study leverages a social alcohol-administration paradigm to assess drinking-related emotions, aiming to elucidate impacts of intoxication on self-reported versus behaviorally expressed emotion. Method: Participants (N = 60; Mage = 22.5; 50% male; 55% White) attended two counterbalanced laboratory sessions, on one of which they were administered an alcoholic beverage (target blood alcohol content .08%) and on the other a nonalcoholic control beverage. Participants in both conditions were accurately informed of beverage contents and consumed study beverages in assigned groups of three while their behavior was videotaped. Emotion was assessed via self-report as well as continuous coding of facial muscle movements. Results: The relationship between self-reported and behaviorally expressed emotion diverged significantly across beverage conditions: positive affect: b = −0.174, t = −2.36, p = .022; negative affect, b = 0.4319, t = 2.37, p = .021. Specifically, self-reports and behavioral displays converged among sober but not intoxicated participants. Further, alcohol’s effects on positive facial displays remained significant in models controlling for self-reported positive and negative emotion, with alcohol enhancing Duchenne smiles 20% beyond effects captured via self-reports, pointing to unique effects of alcohol on behavioral indicators of positive emotion. Conclusions: Findings highlight effects of acute intoxication on the convergence and divergence of emotion measures, thus informing our understanding of measures for capturing emotions that are most proximal to drinking and thus most immediately reinforcing of alcohol consumption."
  },
  {
    "objectID": "docs/publications/articles/naps2025.html#impact-statement",
    "href": "docs/publications/articles/naps2025.html#impact-statement",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "Impact Statement",
    "text": "Impact Statement\nThis study indicates that, while participants’ self-reported and behaviorally expressed emotion held consistent when not drinking, these measures diverged following alcohol consumption. Results further indicated alcohol’s effects on positive emotion were not fully captured by self-report, a tool that has been relied on throughout addiction research. As a result, this study highlights the importance of using multiple methods of capturing emotion, including behavioral and self-report methods, when studying alcohol’s effects on emotion and drinking behaviors."
  },
  {
    "objectID": "docs/publications/articles/isgt2024.html",
    "href": "docs/publications/articles/isgt2024.html",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "",
    "text": "Bowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259."
  },
  {
    "objectID": "docs/publications/articles/isgt2024.html#citation-apa-7",
    "href": "docs/publications/articles/isgt2024.html#citation-apa-7",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "",
    "text": "Bowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259."
  },
  {
    "objectID": "docs/publications/articles/isgt2024.html#abstract",
    "href": "docs/publications/articles/isgt2024.html#abstract",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "Abstract",
    "text": "Abstract\nPhysical attractiveness plays a central role in psychosocial experiences. One of the top research priorities has been to identify factors affecting perceptions of physical attractiveness (PPA). Recent work suggests PPA derives from different sources (e.g., target, perceiver, stimulus type). Although smiles in particular are believed to enhance PPA, support has been surprisingly limited. This study comprehensively examines the effect of smiles on PPA and, more broadly, evaluates the roles of target, perceiver, and stimulus type in PPA variation. Perceivers (n=181) rated both static images and 5-s videos of targets displaying smiling and neutral-expressions. Smiling images were rated as more attractive than neutral-expression images (regardless of stimulus motion format). Interestingly, perceptions of physical attractiveness were based more on the perceiver than on either the target or format in which the target was presented. Results clarify the effect of smiles, and highlight the significant role of the perceiver, in PPA."
  },
  {
    "objectID": "docs/publications/articles/ICCPS2016.html",
    "href": "docs/publications/articles/ICCPS2016.html",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "",
    "text": "Fairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental & Clinical Psychopharmacology, 23(4), 255–264."
  },
  {
    "objectID": "docs/publications/articles/ICCPS2016.html#citation-apa-7",
    "href": "docs/publications/articles/ICCPS2016.html#citation-apa-7",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "",
    "text": "Fairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental & Clinical Psychopharmacology, 23(4), 255–264."
  },
  {
    "objectID": "docs/publications/articles/ICCPS2016.html#abstract",
    "href": "docs/publications/articles/ICCPS2016.html#abstract",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "Abstract",
    "text": "Abstract\nMen and women differ dramatically in their rates of alcohol use disorder (AUD), and researchers have long been interested in identifying mechanisms underlying male vulnerability to problem drinking. Surveys suggest that social processes underlie sex differences in drinking patterns, with men reporting greater social enhancement from alcohol than women, and all-male social drinking contexts being associated with particularly high rates of hazardous drinking. But experimental evidence for sex differences in social-emotional response to alcohol has heretofore been lacking. Research using larger sample sizes, a social context, and more sensitive measures of alcohol’s rewarding effects may be necessary to better understand sex differences in the etiology of AUD. This study explored the acute effects of alcohol during social exchange on speech volume –an objective measure of social-emotional experience that was reliably captured at the group level. Social drinkers (360 male; 360 female) consumed alcohol (.82g/kg males; .74g/kg females), placebo, or a no-alcohol control beverage in groups of three over 36-minutes. Within each of the three beverage conditions, equal numbers of groups consisted of all males, all females, 2 females and 1 male, and 1 female and 2 males. Speech volume was monitored continuously throughout the drink period, and group volume emerged as a robust correlate of self-report and facial indexes of social reward. Notably, alcohol-related increases in group volume were observed selectively in all-male groups but not in groups containing any females. Results point to social enhancement as a promising direction for research exploring factors underlying sex differences in problem drinking."
  },
  {
    "objectID": "docs/publications/articles/gridedge2025.html",
    "href": "docs/publications/articles/gridedge2025.html",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "",
    "text": "Butler, R. M., Christian, C., Girard, J. M., Vanzhula, I. A., & Levinson, C. A. (2024). Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of imaginal exposure for eating disorders. Behavior Research and Therapy, 180, 104577."
  },
  {
    "objectID": "docs/publications/articles/gridedge2025.html#citation-apa-7",
    "href": "docs/publications/articles/gridedge2025.html#citation-apa-7",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "",
    "text": "Butler, R. M., Christian, C., Girard, J. M., Vanzhula, I. A., & Levinson, C. A. (2024). Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of imaginal exposure for eating disorders. Behavior Research and Therapy, 180, 104577."
  },
  {
    "objectID": "docs/publications/articles/gridedge2025.html#abstract",
    "href": "docs/publications/articles/gridedge2025.html#abstract",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "Abstract",
    "text": "Abstract\nObjective: Imaginal exposure is a novel intervention for eating disorders (EDs) that has been investigated as a method for targeting ED symptoms and fears. Research is needed to understand mechanisms of change during imaginal exposure for EDs, including whether within- and between-session distress reduction is related to treatment outcomes.\nMethod: Study 1 tested four sessions of online imaginal exposure (N = 143). Study 2 examined combined imaginal and in vivo exposure, comprising six imaginal exposure sessions (N = 26). ED symptoms and fears were assessed pre- and posttreatment, and subjective distress and state anxiety were collected during sessions.\nResults: Subjective distress tended to increase within-session in both studies, and within-session reduction was not associated with change in ED symptoms or fears. In Study 1, between-session reduction of distress and state anxiety was associated with greater decreases in ED symptoms and fears pre-to posttreatment. In Study 2, between-session distress reduction occurred but was not related to outcomes.\nConclusions: Within-session distress reduction may not promote change during exposure for EDs, whereas between-session distress reduction may be associated with better treatment outcomes. These findings corroborate research on distress reduction during exposure for anxiety disorders. Clinicians might consider approaches to exposure-based treatment that focus on distress tolerance and promote between-session distress reduction."
  },
  {
    "objectID": "docs/publications/articles/IMCLA2022.html",
    "href": "docs/publications/articles/IMCLA2022.html",
    "title": "Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders",
    "section": "",
    "text": "Chung, Y., Girard, J. M., Ravichandran, C., Ongur, D., Cohen, B. M., Baker, J. T. (in press). Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders. Journal of Psychopathology and Clinical Science."
  },
  {
    "objectID": "docs/publications/articles/IMCLA2022.html#citation-apa-7",
    "href": "docs/publications/articles/IMCLA2022.html#citation-apa-7",
    "title": "Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders",
    "section": "",
    "text": "Chung, Y., Girard, J. M., Ravichandran, C., Ongur, D., Cohen, B. M., Baker, J. T. (in press). Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders. Journal of Psychopathology and Clinical Science."
  },
  {
    "objectID": "docs/publications/articles/IMCLA2022.html#abstract",
    "href": "docs/publications/articles/IMCLA2022.html#abstract",
    "title": "Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders",
    "section": "Abstract",
    "text": "Abstract\nPrevailing factor models of psychosis are centered on schizophrenia-related disorders defined by the DSM and ICD, restricting generalizability to other clinical presentations featuring psychosis, even though affective psychoses are more common. This study aims to bridge this gap by conducting exploratory and confirmatory factor analyses, utilizing clinical ratings collected from patients with either affective or non-affective psychoses (n = 1042). Drawing from established clinical instruments, such as the Positive and Negative Syndrome Scale, Young Mania Rating Scale, and Montgomery-Åsberg Depression Rating Scale, a broad spectrum of core psychotic symptoms was considered for the model development. Among the candidate models considered, including correlated factors and multifactor models, a model with seven correlated factors encompassing positive symptoms, negative symptoms, depression, mania, disorganization, hostility, and anxiety was most interpretable with acceptable fit. The seven factors exhibited expected associations with external validators, were replicable through cross- validation, and were generalizable across affective and non-affective psychoses."
  },
  {
    "objectID": "docs/publications/articles/Journal1.html",
    "href": "docs/publications/articles/Journal1.html",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "",
    "text": "Gerber, A. H., Girard, J. M., Scott, S. B., & Lerner, M. D. (2019). Alexithymia – Not autism – is associated with frequency of social interactions in adults. Behaviour Research and Therapy, 123, 103477."
  },
  {
    "objectID": "docs/publications/articles/Journal1.html#citation-apa-7",
    "href": "docs/publications/articles/Journal1.html#citation-apa-7",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "",
    "text": "Gerber, A. H., Girard, J. M., Scott, S. B., & Lerner, M. D. (2019). Alexithymia – Not autism – is associated with frequency of social interactions in adults. Behaviour Research and Therapy, 123, 103477."
  },
  {
    "objectID": "docs/publications/articles/Journal1.html#abstract",
    "href": "docs/publications/articles/Journal1.html#abstract",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "Abstract",
    "text": "Abstract\n\nObjective\nWhile much is known about the quality of social behavior among neurotypical individuals and those with autism spectrum disorder (ASD), little work has evaluated quantity of social interactions. This study used ecological momentary assessment (EMA) to quantify in vivo daily patterns of social interaction in adults as a function of demographic and clinical factors.\n\n\nMethod\nAdults with and without ASD (\\(N_{ASD}=23\\), \\(N_{NT}=52\\)) were trained in an EMA protocol to report their social interactions via smartphone over one week. Participants completed measures of IQ, ASD symptom severity and alexithymia symptom severity.\n\n\nResults\nCyclical multilevel models were used to account for nesting of observations. Results suggest a daily cyclical pattern of social interaction that was robust to ASD and alexithymia symptoms. Adults with ASD did not have fewer social interactions than neurotypical peers; however, severity of alexithymia symptoms predicted fewer social interactions regardless of ASD status.\n\n\nConclusions\nThese findings suggest that alexithymia, not ASD severity, may drive social isolation and highlight the need to reevaluate previously accepted notions regarding differences in social behavior utilizing modern methods."
  },
  {
    "objectID": "docs/publications/articles/pesgm2024.html",
    "href": "docs/publications/articles/pesgm2024.html",
    "title": "It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports",
    "section": "",
    "text": "Baber, G. R., Hamilton, N. A., Girard, J. M., Cohen, J. M., Gratton, M. K. P., Ellis, S., & Hemmer, E. (in press). It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports. Sleep."
  },
  {
    "objectID": "docs/publications/articles/pesgm2024.html#citation-apa-7",
    "href": "docs/publications/articles/pesgm2024.html#citation-apa-7",
    "title": "It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports",
    "section": "",
    "text": "Baber, G. R., Hamilton, N. A., Girard, J. M., Cohen, J. M., Gratton, M. K. P., Ellis, S., & Hemmer, E. (in press). It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports. Sleep."
  },
  {
    "objectID": "docs/publications/articles/pesgm2024.html#abstract",
    "href": "docs/publications/articles/pesgm2024.html#abstract",
    "title": "It’s the sentiment that counts: Comparing sentiment analysis tools for estimating affective valence in dream reports",
    "section": "Abstract",
    "text": "Abstract\nTBA"
  },
  {
    "objectID": "publications/articles/pesgm2024.html",
    "href": "publications/articles/pesgm2024.html",
    "title": "Bayesian Optimization for Deep Reinforcement Learning for Robust Volt-Var Control",
    "section": "",
    "text": "K. Kumar, A. A. Mantha and G. Ravikumar, “Bayesian Optimization for Deep Reinforcement Learning for Robust Volt-Var Control,” 2024 IEEE Power & Energy Society General Meeting (PESGM), Seattle, WA, USA, 2024, pp. 1-5, doi: 10.1109/PESGM51994.2024.10688889. IEEE"
  },
  {
    "objectID": "publications/articles/pesgm2024.html#citation-apa-7",
    "href": "publications/articles/pesgm2024.html#citation-apa-7",
    "title": "Deep Rl-based volt-var control and attack resiliency for der-integrated distribution grids",
    "section": "",
    "text": "K. Kumar and G. Ravikumar, “Deep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids,” 2024 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT), Washington, DC, USA, 2024, pp. 1-5, doi: 10.1109/ISGT59692.2024.10454163. IEEE"
  },
  {
    "objectID": "publications/articles/pesgm2024.html#abstract",
    "href": "publications/articles/pesgm2024.html#abstract",
    "title": "Bayesian Optimization for Deep Reinforcement Learning for Robust Volt-Var Control",
    "section": "Abstract",
    "text": "Abstract\nThe high penetration of Renewable Energy Sources (RES) into the grid introduces complexity to the operation and optimization of energy. One potential solution to the challenge is to use deep reinforcement learning (DRL) based techniques to regulate voltage and reactive power under dynamic conditions. However, there is a need to optimize the DRL for better performance and robustness. This paper proposes a Bayesian optimization (BO) technique within the DRL framework to improve the performance and robustness of volt-var control (VVC) in power distribution systems. We combine the actor-critic DRL algorithm with the BO framework to yield fast optimal volt-var control policies. We use BO techniques to estimate DRL-based VVC decisions and accelerate model-training convergence. In the case study, we demonstrated that the BO in DRL on IEEE-13 has improved decision-making by 21.11% and 81.81% for 123 bus test systems. Our research shows that Bayesian-enabled DRL adapts to different grid configurations and maintains voltage profiles within desired limits, thereby improving DRL control policies."
  },
  {
    "objectID": "publications/articles/Journal1.html",
    "href": "publications/articles/Journal1.html",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "",
    "text": "Gerber, A. H., Girard, J. M., Scott, S. B., & Lerner, M. D. (2019). Alexithymia – Not autism – is associated with frequency of social interactions in adults. Behaviour Research and Therapy, 123, 103477."
  },
  {
    "objectID": "publications/articles/Journal1.html#citation-apa-7",
    "href": "publications/articles/Journal1.html#citation-apa-7",
    "title": "Alexithymia – not autism – is associated with frequency of social interactions in adults",
    "section": "",
    "text": "Gerber, A. H., Girard, J. M., Scott, S. B., & Lerner, M. D. (2019). Alexithymia – Not autism – is associated with frequency of social interactions in adults. Behaviour Research and Therapy, 123, 103477."
  },
  {
    "objectID": "publications/articles/Journal1.html#abstract",
    "href": "publications/articles/Journal1.html#abstract",
    "title": "Bayesian-Optimized Bidirectional Long-Short-Term Memory network for Wind Power Forecasting with Uncertainty Quantification",
    "section": "Abstract",
    "text": "Abstract\n\nObjective\nWind energy technologies, including advanced management and scheduling, rely on accurate wind power forecasting (WPF) for optimal operation. Enhancing forecast precision is crucial for reducing volatility in wind power and improving forecasting reliability. While forecasting methods estimate future values from historical data, traditional approaches often struggle with computational efficiency and model complexity. To address these challenges, we propose a hybrid forecasting model that integrates multi- variate estimation (MVE) and pure prediction (TSP) using a bidirectional long-short-term memory network (Bi- LSTM) optimized with Tree-structured Parzen Estimator (TPE)- based Bayesian optimization. The model incorporates numerical weather prediction (NWP) data for real-time forecasting, a key limitation in existing methods. MVE utilizes features such as wind speed, direction, temperature, and pressure, while TSP captures historical power generation patterns. The TPE-optimized Bi-LSTM architecture effectively captures bidirectional temporal dependencies, improving in both short-term and long-term forecasting. The model is evaluated using a six-year historical wind energy dataset from NREL, with performance assessed through RMSE, MAE, and R2 score. It outperforms traditional LSTM variants (Vanilla LSTM, Stacked LSTM, Bi-LSTM) and state-of-the-art models such as Transformers and GRUs, achieving R² scores of 0.976 for MVE and 0.932, 0.928, and 0.864 for TSP across short-term, day- ahead, and long-term forecasting, respectively. Additionally, TPE based Bayesian optimization reduces computational time around 8-10%, enhancing hyperparameter tuning efficiency. The study further analyzes the model’s computational burden, scalability, and practical implementation, offering a robust and efficient approach for improving wind power forecasting accuracy. \n\n\nMethod\n(\\(N_{ASD}=23\\), \\(N_{NT}=52\\))\n\n\nResults\n\n\nConclusions"
  },
  {
    "objectID": "publications/articles/IMCLA2022.html",
    "href": "publications/articles/IMCLA2022.html",
    "title": "Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders",
    "section": "",
    "text": "Chung, Y., Girard, J. M., Ravichandran, C., Ongur, D., Cohen, B. M., Baker, J. T. (in press). Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders. Journal of Psychopathology and Clinical Science."
  },
  {
    "objectID": "publications/articles/IMCLA2022.html#citation-apa-7",
    "href": "publications/articles/IMCLA2022.html#citation-apa-7",
    "title": "Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders",
    "section": "",
    "text": "Chung, Y., Girard, J. M., Ravichandran, C., Ongur, D., Cohen, B. M., Baker, J. T. (in press). Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders. Journal of Psychopathology and Clinical Science."
  },
  {
    "objectID": "publications/articles/IMCLA2022.html#abstract",
    "href": "publications/articles/IMCLA2022.html#abstract",
    "title": "Transdiagnostic modeling of clinician-rated symptoms in affective and non-affective psychotic disorders",
    "section": "Abstract",
    "text": "Abstract\nPrevailing factor models of psychosis are centered on schizophrenia-related disorders defined by the DSM and ICD, restricting generalizability to other clinical presentations featuring psychosis, even though affective psychoses are more common. This study aims to bridge this gap by conducting exploratory and confirmatory factor analyses, utilizing clinical ratings collected from patients with either affective or non-affective psychoses (n = 1042). Drawing from established clinical instruments, such as the Positive and Negative Syndrome Scale, Young Mania Rating Scale, and Montgomery-Åsberg Depression Rating Scale, a broad spectrum of core psychotic symptoms was considered for the model development. Among the candidate models considered, including correlated factors and multifactor models, a model with seven correlated factors encompassing positive symptoms, negative symptoms, depression, mania, disorganization, hostility, and anxiety was most interpretable with acceptable fit. The seven factors exhibited expected associations with external validators, were replicable through cross- validation, and were generalizable across affective and non-affective psychoses."
  },
  {
    "objectID": "publications/articles/gridedge2025.html",
    "href": "publications/articles/gridedge2025.html",
    "title": "Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids",
    "section": "",
    "text": "K. Kumar and G. Ravikumar, “Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids,” 2025 IEEE PES Grid Edge Technologies Conference & Exposition (Grid Edge), San Diego, CA, USA, 2025, pp. 1-5, doi: 10.1109/GridEdge61154.2025.10887439. IEEE"
  },
  {
    "objectID": "publications/articles/gridedge2025.html#citation-apa-7",
    "href": "publications/articles/gridedge2025.html#citation-apa-7",
    "title": "Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of exposure for eating disorders",
    "section": "",
    "text": "Butler, R. M., Christian, C., Girard, J. M., Vanzhula, I. A., & Levinson, C. A. (2024). Are within- and between-session changes in distress associated with treatment outcomes? Findings from two clinical trials of imaginal exposure for eating disorders. Behavior Research and Therapy, 180, 104577."
  },
  {
    "objectID": "publications/articles/gridedge2025.html#abstract",
    "href": "publications/articles/gridedge2025.html#abstract",
    "title": "Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids",
    "section": "Abstract",
    "text": "Abstract\nThe integration of renewable energy resources has made power system management increasingly complex. DRL is a potential solution to optimize power system operations, but it requires significant time and resources during training. The control policies developed using DRL are specific to a single grid and require retraining from scratch for other grids. Training the DRL model from scratch is computationally expensive. This paper proposes a novel TL with a DRL framework to optimize VV C across different grids. This framework significantly reduces training time and improves VVC control performance by fine-tuning pre-trained DRL models for various grids. We developed a policy reuse classifier that transfers the knowledge from the IEEE-123 Bus system to the IEEE-13 Bus system. We performed an impact analysis to determine the effectiveness of TL. Our results show that TL improves the VVC control policy by 69.51 %, achieves faster convergence, and reduces the training time by 98.14%."
  },
  {
    "objectID": "publications/articles/ICCPS2016.html",
    "href": "publications/articles/ICCPS2016.html",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "",
    "text": "Fairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental & Clinical Psychopharmacology, 23(4), 255–264."
  },
  {
    "objectID": "publications/articles/ICCPS2016.html#citation-apa-7",
    "href": "publications/articles/ICCPS2016.html#citation-apa-7",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "",
    "text": "Fairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental & Clinical Psychopharmacology, 23(4), 255–264."
  },
  {
    "objectID": "publications/articles/ICCPS2016.html#abstract",
    "href": "publications/articles/ICCPS2016.html#abstract",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "Abstract",
    "text": "Abstract\nMen and women differ dramatically in their rates of alcohol use disorder (AUD), and researchers have long been interested in identifying mechanisms underlying male vulnerability to problem drinking. Surveys suggest that social processes underlie sex differences in drinking patterns, with men reporting greater social enhancement from alcohol than women, and all-male social drinking contexts being associated with particularly high rates of hazardous drinking. But experimental evidence for sex differences in social-emotional response to alcohol has heretofore been lacking. Research using larger sample sizes, a social context, and more sensitive measures of alcohol’s rewarding effects may be necessary to better understand sex differences in the etiology of AUD. This study explored the acute effects of alcohol during social exchange on speech volume –an objective measure of social-emotional experience that was reliably captured at the group level. Social drinkers (360 male; 360 female) consumed alcohol (.82g/kg males; .74g/kg females), placebo, or a no-alcohol control beverage in groups of three over 36-minutes. Within each of the three beverage conditions, equal numbers of groups consisted of all males, all females, 2 females and 1 male, and 1 female and 2 males. Speech volume was monitored continuously throughout the drink period, and group volume emerged as a robust correlate of self-report and facial indexes of social reward. Notably, alcohol-related increases in group volume were observed selectively in all-male groups but not in groups containing any females. Results point to social enhancement as a promising direction for research exploring factors underlying sex differences in problem drinking."
  },
  {
    "objectID": "publications/articles/isgt2024.html",
    "href": "publications/articles/isgt2024.html",
    "title": "Deep Rl-based volt-var control and attack resiliency for der-integrated distribution grids",
    "section": "",
    "text": "K. Kumar and G. Ravikumar, “Deep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids,” 2024 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT), Washington, DC, USA, 2024, pp. 1-5, doi: 10.1109/ISGT59692.2024.10454163. IEEE"
  },
  {
    "objectID": "publications/articles/isgt2024.html#citation-apa-7",
    "href": "publications/articles/isgt2024.html#citation-apa-7",
    "title": "In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions",
    "section": "",
    "text": "Bowdring, M. A., Sayette, M. A., Girard, J. M., & Woods, W. C. (2021). In the eye of the beholder: A comprehensive analysis of stimulus type, perceiver, and target in physical attractiveness perceptions. Journal of Nonverbal Behavior, 45(2), 241–259."
  },
  {
    "objectID": "publications/articles/isgt2024.html#abstract",
    "href": "publications/articles/isgt2024.html#abstract",
    "title": "Deep Rl-based volt-var control and attack resiliency for der-integrated distribution grids",
    "section": "Abstract",
    "text": "Abstract\nIntegrating distributed energy resources (DERs) into a power system requires more advanced control mechanisms. One of the control strategies used for Volt-VAR control (VVC) is to manage voltage and reactive power. With the increase in the complexity of the power system, there is a need to develop an autonomous and robust control mechanism using deep reinforcement learning (DRL) to enhance grid performance and adjust voltage and reactive power settings. These adjustments minimize losses and enhance voltage stability in the grid. In this paper, we proposed a novel approach to develop a DRL-based VVC framework and mitigation techniques to protect against stealthy white-box attacks targeting the trained control policies of the DRL model. The mitigation technique on the trained DRL is proposed to control the voltage violations on the smart grid to enhance the stability of the grid and minimize voltage irregularities. Our proposed mitigation technique provided better control policies for DRL-based VVC, successfully mitigating 100 percent of voltage violations in the smart grid environment. The results show that the mitigation technique enhances the security and robustness of trained DRL VVC agents."
  },
  {
    "objectID": "publications/articles/naps2025.html",
    "href": "publications/articles/naps2025.html",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "",
    "text": "Caumiant, E. P., Kang, D., Girard, J. M., & Fairbairn, C. E. (in press). Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication. Psychology of Addictive Behaviors."
  },
  {
    "objectID": "publications/articles/naps2025.html#citation-apa-7",
    "href": "publications/articles/naps2025.html#citation-apa-7",
    "title": "Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication",
    "section": "",
    "text": "Caumiant, E. P., Kang, D., Girard, J. M., & Fairbairn, C. E. (in press). Alcohol and emotion: Analyzing convergence between facially expressed and self-reported indices of emotion under alcohol intoxication. Psychology of Addictive Behaviors."
  },
  {
    "objectID": "publications/articles/naps2025.html#abstract",
    "href": "publications/articles/naps2025.html#abstract",
    "title": "A Multi-Objective Optimization Framework for Carbon-Aware Smart Energy Management",
    "section": "Abstract",
    "text": "Abstract\nAs renewable energy sources become more integrated into the power grid, efficient scheduling of household energy consumption is essential to reduce costs and carbon footprint. In this paper, we introduce an energy optimization framework that optimizes the timing of appliance use based on dynamic carbon intensity and electricity prices. We determine optimal operation schedules using a multi-objective optimization model with a Gurobi solver and machine learning. We combine Random Forest and XGBoost for demand prediction, incorporating their uncertainty estimates into the optimization constraints. The model optimizes ON/OFF appliance schedules while considering constraints like minimum operating times and power balance. It shifts usage to lower-cost and carbon-intensity periods, which helps to reduce energy consumption.\nKey contributions include ML-based demand predictions and mixed-integer programming (MIP) optimization that improves robustness to prediction errors while adjusting schedules based on time-of-use pricing and carbon intensity. Our smart scheduling and load shifting achieve a 35.8% reduction in costs, a 38.6% decrease in carbon emissions, and a 25.8% reduction in peak demand using a carbon-aware scheduling algorithm. This framework effectively shifts loads to low-cost, low-carbon times, offering significant economic and environmental benefits for residential energy management without compromising user comfort."
  },
  {
    "objectID": "publications/articles/naps2025.html#impact-statement",
    "href": "publications/articles/naps2025.html#impact-statement",
    "title": "A Multi-Objective Optimization Framework for Carbon-Aware Smart Energy Management",
    "section": "Impact Statement",
    "text": "Impact Statement"
  },
  {
    "objectID": "publications/articles/gridedge2025.html#citation",
    "href": "publications/articles/gridedge2025.html#citation",
    "title": "Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids",
    "section": "",
    "text": "K. Kumar and G. Ravikumar, “Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids,” 2025 IEEE PES Grid Edge Technologies Conference & Exposition (Grid Edge), San Diego, CA, USA, 2025, pp. 1-5, doi: 10.1109/GridEdge61154.2025.10887439. IEEE"
  },
  {
    "objectID": "publications/articles/pesgm2024.html#citation",
    "href": "publications/articles/pesgm2024.html#citation",
    "title": "Bayesian Optimization for Deep Reinforcement Learning for Robust Volt-Var Control",
    "section": "",
    "text": "K. Kumar, A. A. Mantha and G. Ravikumar, “Bayesian Optimization for Deep Reinforcement Learning for Robust Volt-Var Control,” 2024 IEEE Power & Energy Society General Meeting (PESGM), Seattle, WA, USA, 2024, pp. 1-5, doi: 10.1109/PESGM51994.2024.10688889. IEEE"
  },
  {
    "objectID": "publications/articles/isgt2024.html#citation",
    "href": "publications/articles/isgt2024.html#citation",
    "title": "Deep Rl-based volt-var control and attack resiliency for der-integrated distribution grids",
    "section": "",
    "text": "K. Kumar and G. Ravikumar, “Deep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids,” 2024 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT), Washington, DC, USA, 2024, pp. 1-5, doi: 10.1109/ISGT59692.2024.10454163. IEEE"
  },
  {
    "objectID": "publications/articles/icmla2022.html",
    "href": "publications/articles/icmla2022.html",
    "title": "Deep learning and pattern-based methodology for multivariable sensor data regression",
    "section": "",
    "text": "J. K. Francis, C. Kumar, J. Herrera-Gerena, K. Kumar and M. J. Darr, “Deep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression,” 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA), Nassau, Bahamas, 2022, pp. 748-753, doi: 10.1109/ICMLA55696.2022.00125. IEEE."
  },
  {
    "objectID": "publications/articles/icmla2022.html#citation",
    "href": "publications/articles/icmla2022.html#citation",
    "title": "Deep learning and pattern-based methodology for multivariable sensor data regression",
    "section": "",
    "text": "J. K. Francis, C. Kumar, J. Herrera-Gerena, K. Kumar and M. J. Darr, “Deep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression,” 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA), Nassau, Bahamas, 2022, pp. 748-753, doi: 10.1109/ICMLA55696.2022.00125. IEEE."
  },
  {
    "objectID": "publications/articles/icmla2022.html#abstract",
    "href": "publications/articles/icmla2022.html#abstract",
    "title": "Deep learning and pattern-based methodology for multivariable sensor data regression",
    "section": "Abstract",
    "text": "Abstract\nWe propose a deep learning methodology for multivariable regression based on pattern recognition that triggers fast learning over sensor data. We used a conversion of sensors-to-image, which enables us to take advantage of Computer Vision architectures and training processes. In addition to this data preparation methodology, we explore using state-of-the-art architectures to generate regression outputs to predict agricultural crop continuous yield information. Finally, we compare with some top models reported in MLCAS2021. We found that using a straightforward training process, we were able to accomplish an MAE of 4.394, RMSE of 5.945, and R2 of 0.861."
  },
  {
    "objectID": "publications/articles/iccpsS2016.html",
    "href": "publications/articles/iccpsS2016.html",
    "title": "Deep value of information estimators for collaborative human-machine information gathering",
    "section": "",
    "text": "K. G. Lore, N. Sweet, K. Kumar, N. Ahmed and S. Sarkar, “Deep Value of Information Estimators for Collaborative Human-Machine Information Gathering,” 2016 ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS), Vienna, Austria, 2016, pp. 1-10, doi: 10.1109/ICCPS.2016.7479095. IEEE."
  },
  {
    "objectID": "publications/articles/iccpsS2016.html#citation-apa-7",
    "href": "publications/articles/iccpsS2016.html#citation-apa-7",
    "title": "Speech volume indexes gender differences in the social-emotional effects of alcohol",
    "section": "",
    "text": "Fairbairn, C. E., Sayette, M. A., Amole, M. C., Dimoff, J. D., Cohn, J. F., & Girard, J. M. (2015). Speech volume indexes sex differences in the social-emotional effects of alcohol. Experimental & Clinical Psychopharmacology, 23(4), 255–264."
  },
  {
    "objectID": "publications/articles/iccpsS2016.html#abstract",
    "href": "publications/articles/iccpsS2016.html#abstract",
    "title": "Deep value of information estimators for collaborative human-machine information gathering",
    "section": "Abstract",
    "text": "Abstract\nEffective human-machine collaboration can significantly improve many learning and planning strategies for information gathering via fusion of ‘hard’ and ‘soft’ data originating from machine and human sensors, respectively. However, gathering the most informative data from human sensors without task overloading remains a critical technical challenge. In this context, Value of Information (VOI) is a crucial decision- theoretic metric for scheduling interaction with human sensors. We present a new Deep Learning based VOI estimation framework that can be used to schedule collaborative human-machine sensing with efficient online inference and minimal policy hand-tuning. Supervised learning is used to train deep convolutional neural networks (CNNs) to extract hierarchical features from ‘images’ of belief spaces obtained via data fusion. These features can be associated with soft data query choices to reliably compute VOI for human interaction. The CNN framework is described in detail, and a performance comparison to a feature- based POMDP scheduling policy is provided. The practical feasibility of our method is also demonstrated on a mobile robotic search problem with language-based semantic human sensor inputs."
  },
  {
    "objectID": "publications/articles/iccpsS2016.html#citation",
    "href": "publications/articles/iccpsS2016.html#citation",
    "title": "Deep value of information estimators for collaborative human-machine information gathering",
    "section": "",
    "text": "K. G. Lore, N. Sweet, K. Kumar, N. Ahmed and S. Sarkar, “Deep Value of Information Estimators for Collaborative Human-Machine Information Gathering,” 2016 ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS), Vienna, Austria, 2016, pp. 1-10, doi: 10.1109/ICCPS.2016.7479095. IEEE."
  },
  {
    "objectID": "publications/articles/journal1.html#abstract",
    "href": "publications/articles/journal1.html#abstract",
    "title": "Bayesian-Optimized Bidirectional Long-Short-Term Memory network for Wind Power Forecasting with Uncertainty Quantification",
    "section": "Abstract",
    "text": "Abstract\n\nObjective\nWind energy technologies, including advanced management and scheduling, rely on accurate wind power forecasting (WPF) for optimal operation. Enhancing forecast precision is crucial for reducing volatility in wind power and improving forecasting reliability. While forecasting methods estimate future values from historical data, traditional approaches often struggle with computational efficiency and model complexity. To address these challenges, we propose a hybrid forecasting model that integrates multi- variate estimation (MVE) and pure prediction (TSP) using a bidirectional long-short-term memory network (Bi- LSTM) optimized with Tree-structured Parzen Estimator (TPE)- based Bayesian optimization. The model incorporates numerical weather prediction (NWP) data for real-time forecasting, a key limitation in existing methods. MVE utilizes features such as wind speed, direction, temperature, and pressure, while TSP captures historical power generation patterns. The TPE-optimized Bi-LSTM architecture effectively captures bidirectional temporal dependencies, improving in both short-term and long-term forecasting. The model is evaluated using a six-year historical wind energy dataset from NREL, with performance assessed through RMSE, MAE, and R2 score. It outperforms traditional LSTM variants (Vanilla LSTM, Stacked LSTM, Bi-LSTM) and state-of-the-art models such as Transformers and GRUs, achieving R² scores of 0.976 for MVE and 0.932, 0.928, and 0.864 for TSP across short-term, day- ahead, and long-term forecasting, respectively. Additionally, TPE based Bayesian optimization reduces computational time around 8-10%, enhancing hyperparameter tuning efficiency. The study further analyzes the model’s computational burden, scalability, and practical implementation, offering a robust and efficient approach for improving wind power forecasting accuracy. \n\n\nMethod\n(\\(N_{ASD}=23\\), \\(N_{NT}=52\\))\n\n\nResults\n\n\nConclusions"
  },
  {
    "objectID": "publications/proceedings/swartz2023.html",
    "href": "publications/proceedings/swartz2023.html",
    "title": "Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth",
    "section": "",
    "text": "Swartz, H. A., Bylsma, L. M., Fournier, J. C., Girard, J. M., Spotts, C., Cohn, J. F., & Morency, L.-P. (2023). Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth. Journal of Affective Disorders, 333, 543–552."
  },
  {
    "objectID": "publications/proceedings/swartz2023.html#citation-apa-7",
    "href": "publications/proceedings/swartz2023.html#citation-apa-7",
    "title": "Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth",
    "section": "",
    "text": "Swartz, H. A., Bylsma, L. M., Fournier, J. C., Girard, J. M., Spotts, C., Cohn, J. F., & Morency, L.-P. (2023). Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth. Journal of Affective Disorders, 333, 543–552."
  },
  {
    "objectID": "publications/proceedings/swartz2023.html#abstract",
    "href": "publications/proceedings/swartz2023.html#abstract",
    "title": "Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth",
    "section": "Abstract",
    "text": "Abstract\nBackground: Expert consensus guidelines recommend Cognitive Behavioral Therapy (CBT) and Interpersonal Psychotherapy (IPT), interventions that were historically delivered face-to-face, as first-line treatments for Major Depressive Disorder (MDD). Despite the ubiquity of telehealth following the COVID-19 pandemic, little is known about differential outcomes with CBT versus IPT delivered in-person (IP) or via telehealth (TH) or whether working alliance is affected.\nMethods: Adults meeting DSM-5 criteria for MDD were randomly assigned to either 8 sessions of IPT or CBT (group). Mid-trial, COVID-19 forced a change of therapy delivery from IP to TH (study phase). We compared changes in Hamilton Rating Scale for Depression (HRSD-17) and Working Alliance Inventory (WAI) scores for individuals by group and phase: CBT-IP (n = 24), CBT-TH (n = 11), IPT-IP (n = 25) and IPT-TH (n = 17).\nResults: HRSD-17 scores declined significantly from pre to post treatment (pre: M = 17.7, SD = 4.4 vs. post: M = 11.7, SD = 5.9; p &lt; .001; d = 1.45) without significant group or phase effects. WAI scores did not differ by group or phase. Number of completed therapy sessions was greater for TH (M = 7.8, SD = 1.2) relative to IP (M = 7.2, SD = 1.6) (Mann-Whitney U = 387.50, z = 2.24, p = .025).\nLimitations: Participants were not randomly assigned to IP versus TH. Sample size is small.\nConclusions: This study provides preliminary evidence supporting the efficacy of both brief IPT and CBT, delivered by either TH or IP, for depression. It showed that working alliance is preserved in TH, and delivery via TH may improve therapy adherence. Prospective, randomized controlled trials are needed to definitively test efficacy of brief IPT and CBT delivered via TH versus IP."
  },
  {
    "objectID": "publications/proceedings/shepherd2017.html",
    "href": "publications/proceedings/shepherd2017.html",
    "title": "Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents",
    "section": "",
    "text": "Shepherd, L. M., Sly, K. F., & Girard, J. M. (2017). Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents. Journal of Adolescence, 61, 50–63."
  },
  {
    "objectID": "publications/proceedings/shepherd2017.html#citation-apa-7",
    "href": "publications/proceedings/shepherd2017.html#citation-apa-7",
    "title": "Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents",
    "section": "",
    "text": "Shepherd, L. M., Sly, K. F., & Girard, J. M. (2017). Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents. Journal of Adolescence, 61, 50–63."
  },
  {
    "objectID": "publications/proceedings/shepherd2017.html#abstract",
    "href": "publications/proceedings/shepherd2017.html#abstract",
    "title": "Comparison of comprehensive and abstinence-only sexuality education in young African American adolescents",
    "section": "Abstract",
    "text": "Abstract\nThe purpose of this study was to identify predictors of sexual behavior and condom use in African American adolescents, as well as to evaluate the effectiveness of comprehensive sexuality and abstinence-only education to reduce adolescent sexual behavior and increase condom use. Participants included 450 adolescents aged 12-14 years in the southern United States. Regression analyses showed favorable attitudes toward sexual behavior and social norms significantly predicted recent sexual behavior, and favorable attitudes toward condoms significantly predicted condom usage. Self-efficacy was not found to be predictive of adolescents’ sexual behavior or condom use. There were no significant differences in recent sexual behavior based on type of sexuality education. Adolescents who received abstinence-only education had reduced favorable attitudes toward condom use, and were more likely to have unprotected sex than the comparison group. Findings suggest that adolescents who receive abstinence-only education are at greater risk of engaging in unprotected sex."
  },
  {
    "objectID": "publications/proceedings/ross2017.html",
    "href": "publications/proceedings/ross2017.html",
    "title": "Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment",
    "section": "",
    "text": "Ross, J. M., Girard, J. M., Wright, A. G. C., Beeney, J. E., Scott, L. N., Hallquist, M. N., Lazarus, S. A., Stepp, S. D., & Pilkonis, P. A. (2017). Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment. Psychological Assessment, 29(2), 123–134."
  },
  {
    "objectID": "publications/proceedings/ross2017.html#citation-apa-7",
    "href": "publications/proceedings/ross2017.html#citation-apa-7",
    "title": "Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment",
    "section": "",
    "text": "Ross, J. M., Girard, J. M., Wright, A. G. C., Beeney, J. E., Scott, L. N., Hallquist, M. N., Lazarus, S. A., Stepp, S. D., & Pilkonis, P. A. (2017). Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment. Psychological Assessment, 29(2), 123–134."
  },
  {
    "objectID": "publications/proceedings/ross2017.html#abstract",
    "href": "publications/proceedings/ross2017.html#abstract",
    "title": "Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment",
    "section": "Abstract",
    "text": "Abstract\nRelationships are among the most salient factors affecting happiness and wellbeing for individuals and families. Relationship science has identified the study of dyadic behavioral patterns between couple members during conflict as an important window in to relational functioning with both short-term and long-term consequences. Several methods have been developed for the momentary assessment of behavior during interpersonal transactions. Among these, the most popular is the Specific Affect Coding System (SPAFF), which organizes social behavior into a set of discrete behavioral constructs. This study examines the interpersonal meaning of the SPAFF codes through the lens of interpersonal theory, which uses the fundamental dimensions of Dominance and Affiliation to organize interpersonal behavior. A sample of 67 couples completed a conflict task, which was video recorded and coded using SPAFF and a method for rating momentary interpersonal behavior, the Continuous Assessment of Interpersonal Dynamics (CAID). Actor partner interdependence models in a multilevel structural equation modeling framework were used to study the covariation of SPAFF codes and CAID ratings. Results showed that a number of SPAFF codes had clear interpersonal signatures, but many did not. Additionally, actor and partner effects for the same codes were strongly consistent with interpersonal theory’s principle of complementarity. Thus, findings reveal points of convergence and divergence in the 2 systems and provide support for central tenets of interpersonal theory. Future directions based on these initial findings are discussed."
  },
  {
    "objectID": "publications/proceedings/pacella2018.html",
    "href": "publications/proceedings/pacella2018.html",
    "title": "The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study",
    "section": "",
    "text": "Pacella, M. L., Girard, J. M., Wright, A. G. C., Suffoletto, B., & Callaway, C. W. (2018). The association between daily posttraumatic stress symptoms and pain over the first 14 days after injury: An experience sampling study. Academic Emergency Medicine, 25(8), 844–855."
  },
  {
    "objectID": "publications/proceedings/pacella2018.html#citation-apa-7",
    "href": "publications/proceedings/pacella2018.html#citation-apa-7",
    "title": "The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study",
    "section": "",
    "text": "Pacella, M. L., Girard, J. M., Wright, A. G. C., Suffoletto, B., & Callaway, C. W. (2018). The association between daily posttraumatic stress symptoms and pain over the first 14 days after injury: An experience sampling study. Academic Emergency Medicine, 25(8), 844–855."
  },
  {
    "objectID": "publications/proceedings/pacella2018.html#abstract",
    "href": "publications/proceedings/pacella2018.html#abstract",
    "title": "The association between daily posttraumatic stress symptoms and pain over the first 14-days after injury: An experience sampling study",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nPsychosocial factors and responses to injury modify the transition from acute to chronic pain. Specifically, posttraumatic stress disorder (PTSD) symptoms (reexperiencing, avoidance, and hyperarousal symptoms) exacerbate and cooccur with chronic pain. Yet no study has prospectively considered the associations among these psychological processes and pain reports using experience sampling methods (ESMs) during the acute aftermath of injury. This study applied ESM via daily text messaging to monitor and detect relationships among psychosocial factors and postinjury pain across the first 14 days after emergency department (ED) discharge.\n\n\nMethods\nWe recruited 75 adults (59% male; mean ± SD age = 34 ± 11.73 years) who experienced a potentially traumatic injury (i.e., involving life threat or serious injury) in the past 24 hours from the EDs of two Level I trauma centers. Participants received five questions per day via text messaging from Day 1 to Day 14 post–ED discharge; three questions measured PTSD symptoms, one question measured perceived social support, and one question measured physical pain.\n\n\nResults\nSixty-seven participants provided sufficient data for inclusion in the final analyses, and the average response rate per subject was 86%. Pain severity score decreased from a mean ± SD of 7.2 ± 2.0 to 4.4 ± 2.69 over 14 days and 50% of the variance in daily pain scores was within person. In multilevel structural equation models, pain scores decreased over time, and daily fluctuations of hyperarousal (B = 0.22, 95% confidetnce interval = 0.08–0.36) were uniquely associated with daily fluctuations in reported pain level within each person.\n\n\nConclusions\nDaily hyperarousal symptoms predict same-day pain severity over the acute postinjury recovery period. We also demonstrated feasibility to screen and identify patients at risk for pain chronicity in the acute aftermath of injury. Early interventions aimed at addressing hyperarousal (e.g., anxiolytics) could potentially aid in reducing experience of pain."
  },
  {
    "objectID": "publications/proceedings/hopwood2020.html",
    "href": "publications/proceedings/hopwood2020.html",
    "title": "Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict",
    "section": "",
    "text": "Hopwood, C. J., Harrison, A. L., Amole, M. C., Girard, J. M., Wright, A. G. C., Thomas, K. M., Sadler, P., Ansell, E. B., Chaplin, T. M., Morey, L. C., Crowley, M. J., Durbin, C. E., & Kashy, D. A. (2020). Properties of the continuous assessment of interpersonal dynamics across sex, level of familiarity, and interpersonal conflict. Assessment, 27(1), 40–56."
  },
  {
    "objectID": "publications/proceedings/hopwood2020.html#citation-apa-7",
    "href": "publications/proceedings/hopwood2020.html#citation-apa-7",
    "title": "Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict",
    "section": "",
    "text": "Hopwood, C. J., Harrison, A. L., Amole, M. C., Girard, J. M., Wright, A. G. C., Thomas, K. M., Sadler, P., Ansell, E. B., Chaplin, T. M., Morey, L. C., Crowley, M. J., Durbin, C. E., & Kashy, D. A. (2020). Properties of the continuous assessment of interpersonal dynamics across sex, level of familiarity, and interpersonal conflict. Assessment, 27(1), 40–56."
  },
  {
    "objectID": "publications/proceedings/hopwood2020.html#abstract",
    "href": "publications/proceedings/hopwood2020.html#abstract",
    "title": "Properties of the Continuous Assessment of Interpersonal Dynamics across sex, level of familiarity, and interpersonal conflict",
    "section": "Abstract",
    "text": "Abstract\nThe Continuous Assessment of Interpersonal Dynamics (CAID) is a method in which trained observers code individuals’ dominance and warmth continuously while they interact in dyads. This method has significant promise for assessing dynamic interpersonal processes. The purpose of this study was to examine the impact of individual sex, dyadic familiarity, and situational conflict on patterns of interpersonal warmth, dominance, and complementarity as assessed via CAID. We used six samples with 603 dyads, including 2 samples of unacquainted mixed-sex undergraduates interacting in a collaborative task, 2 samples of couples interacting in both collaborative and conflict tasks, and 2 samples of mothers and children interacting in both collaborative and conflict tasks. Complementarity effects were robust across all samples, and individuals tended to be relatively warm and dominant. Results from multilevel models indicated that women were slightly warmer than men whereas there were no sex differences in dominance. Unfamiliar dyads and dyads interacting in more collaborative tasks were relatively warmer, more submissive, and more complementary on warmth but less complementary on dominance. These findings speak to the utility of the CAID method for assessing interpersonal dynamics and provide norms for researchers who use the method for different types of samples and applications."
  },
  {
    "objectID": "publications/proceedings/girard2022a.html",
    "href": "publications/proceedings/girard2022a.html",
    "title": "Computational analysis of spoken language in acute psychosis and mania",
    "section": "",
    "text": "Girard, J. M., Vail, A. K., Liebenthal, E., Brown, K., Kilciksiz, C. M., Pennant, L., Liebson, E., Öngür, D., Morency, L.-P., & Baker, J. T. (2022). Computational analysis of spoken language in acute psychosis and mania. Schizophrenia Research, 245, 97–115."
  },
  {
    "objectID": "publications/proceedings/girard2022a.html#citation-apa-7",
    "href": "publications/proceedings/girard2022a.html#citation-apa-7",
    "title": "Computational analysis of spoken language in acute psychosis and mania",
    "section": "",
    "text": "Girard, J. M., Vail, A. K., Liebenthal, E., Brown, K., Kilciksiz, C. M., Pennant, L., Liebson, E., Öngür, D., Morency, L.-P., & Baker, J. T. (2022). Computational analysis of spoken language in acute psychosis and mania. Schizophrenia Research, 245, 97–115."
  },
  {
    "objectID": "publications/proceedings/girard2022a.html#abstract",
    "href": "publications/proceedings/girard2022a.html#abstract",
    "title": "Computational analysis of spoken language in acute psychosis and mania",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nThis study aimed to (1) determine the feasibility of collecting behavioral data from participants hospitalized with acute psychosis and (2) begin to evaluate the clinical information that can be computationally derived from such data.\n\n\nMethods\nBehavioral data was collected across 99 sessions from 38 participants recruited from an inpatient psychiatric unit. Each session started with a semi-structured interview modeled on a typical “clinical rounds” encounter and included administration of the Positive and Negative Syndrome Scale (PANSS).\n\n\nAnalysis\nWe quantified aspects of participants’ verbal behavior during the interview using lexical, coherence, and disfluency features. We then used two complementary approaches to explore our second objective. The first approach used predictive models to estimate participants’ PANSS scores from their language features. Our second approach used inferential models to quantify the relationships between individual language features and symptom measures.\n\n\nResults\nOur predictive models showed promise but lacked sufficient data to achieve clinically useful accuracy. Our inferential models identified statistically significant relationships between numerous language features and symptom domains.\n\n\nConclusion\nOur interview recording procedures were well-tolerated and produced adequate data for transcription and analysis. The results of our inferential modeling suggest that automatic measurements of expressive language contain signals highly relevant to the assessment of psychosis. These findings establish the potential of measuring language during a clinical interview in a naturalistic setting and generate specific hypotheses that can be tested in future studies. This, in turn, will lead to more accurate modeling and better understanding of the relationships between expressive language and psychosis."
  },
  {
    "objectID": "publications/proceedings/girard2018a.html",
    "href": "publications/proceedings/girard2018a.html",
    "title": "DARMA: Software for dual axis rating and media annotation",
    "section": "",
    "text": "Girard, J. M., & Wright, A. G. C. (2018). DARMA: Software for Dual Axis Rating and Media Annotation. Behavior Research Methods, 50(3), 902–909."
  },
  {
    "objectID": "publications/proceedings/girard2018a.html#citation-apa-7",
    "href": "publications/proceedings/girard2018a.html#citation-apa-7",
    "title": "DARMA: Software for dual axis rating and media annotation",
    "section": "",
    "text": "Girard, J. M., & Wright, A. G. C. (2018). DARMA: Software for Dual Axis Rating and Media Annotation. Behavior Research Methods, 50(3), 902–909."
  },
  {
    "objectID": "publications/proceedings/girard2018a.html#abstract",
    "href": "publications/proceedings/girard2018a.html#abstract",
    "title": "DARMA: Software for dual axis rating and media annotation",
    "section": "Abstract",
    "text": "Abstract\nContinuous measurement systems provide a means of measuring dynamic behavioral and experiential processes as they play out over time. DARMA is a modernized continuous measurement system that synchronizes media playback and the continuous recording of two-dimensional measurements. These measurements can be observational or self-reported and are provided in real-time through the manipulation of a computer joystick. DARMA also provides tools for reviewing and comparing collected measurements and for customizing various settings. DARMA is a domain-independent software tool that was designed to aid researchers who are interested in gaining a deeper understanding of behavior and experience. It is especially well-suited to the study of affective and interpersonal processes, such as the perception and expression of emotional states and the communication of social signals. DARMA is open-source using the GNU General Public License (GPL) and is available for free download from http://darma.jmgirard.com."
  },
  {
    "objectID": "publications/proceedings/girard2016a.html",
    "href": "publications/proceedings/girard2016a.html",
    "title": "A primer on observational measurement",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2016). A primer on observational measurement. Assessment, 23(4), 404–413."
  },
  {
    "objectID": "publications/proceedings/girard2016a.html#citation-apa-7",
    "href": "publications/proceedings/girard2016a.html#citation-apa-7",
    "title": "A primer on observational measurement",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2016). A primer on observational measurement. Assessment, 23(4), 404–413."
  },
  {
    "objectID": "publications/proceedings/girard2016a.html#abstract",
    "href": "publications/proceedings/girard2016a.html#abstract",
    "title": "A primer on observational measurement",
    "section": "Abstract",
    "text": "Abstract\nObservational measurement plays an integral role in a variety of scientific endeavors within biology, psychology, sociology, education, medicine, and marketing. The current article provides an interdisciplinary primer on observational measurement; in particular, it highlights recent advances in observational methodology and the challenges that accompany such growth. First, we detail the various types of instrument that can be used to standardize measurements across observers. Second, we argue for the importance of validity in observational measurement and provide several approaches to validation based on contemporary validity theory. Third, we outline the challenges currently faced by observational researchers pertaining to measurement drift, observer reactivity, reliability analysis, and time/expense. Fourth, we describe recent advances in computer-assisted measurement, fully automated measurement, and statistical data analysis. Finally, we identify several key directions for future observational research to explore."
  },
  {
    "objectID": "publications/proceedings/girard2015b.html",
    "href": "publications/proceedings/girard2015b.html",
    "title": "Estimating smile intensity: A better way",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., & De la Torre, F. (2015). Estimating smile intensity: A better way. Pattern Recognition Letters, 66, 13–21."
  },
  {
    "objectID": "publications/proceedings/girard2015b.html#citation-apa-7",
    "href": "publications/proceedings/girard2015b.html#citation-apa-7",
    "title": "Estimating smile intensity: A better way",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., & De la Torre, F. (2015). Estimating smile intensity: A better way. Pattern Recognition Letters, 66, 13–21."
  },
  {
    "objectID": "publications/proceedings/girard2015b.html#abstract",
    "href": "publications/proceedings/girard2015b.html#abstract",
    "title": "Estimating smile intensity: A better way",
    "section": "Abstract",
    "text": "Abstract\nBoth the occurrence and intensity of facial expressions are critical to what the face reveals. While much progress has been made towards the automatic detection of facial expression occurrence, controversy exists about how to estimate expression intensity. The most straight-forward approach is to train multiclass or regression models using intensity ground truth. However, collecting intensity ground truth is even more time consuming and expensive than collecting binary ground truth. As a shortcut, some researchers have proposed using the decision values of binary-trained maximum margin classifiers as a proxy for expression intensity. We provide empirical evidence that this heuristic is flawed in practice as well as in theory. Unfortunately, there are no shortcuts when it comes to estimating smile intensity: researchers must take the time to collect and train on intensity ground truth. However, if they do so, high reliability with expert human coders can be achieved. Intensity-trained multiclass and regression models outperformed binary-trained classifier decision values on smile intensity estimation across multiple databases and methods for feature extraction and dimensionality reduction. Multiclass models even outperformed binary–trained classifiers on smile occurrence detection."
  },
  {
    "objectID": "publications/proceedings/girard2014c.html",
    "href": "publications/proceedings/girard2014c.html",
    "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
    "section": "",
    "text": "Zhang, X., Yin, L., Cohn, J. F., Canavan, S., Reale, M., Horowitz, A., Liu, P., & Girard, J. M. (2014). BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database. Image and Vision Computing, 32(10), 692–706."
  },
  {
    "objectID": "publications/proceedings/girard2014c.html#citation-apa-7",
    "href": "publications/proceedings/girard2014c.html#citation-apa-7",
    "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
    "section": "",
    "text": "Zhang, X., Yin, L., Cohn, J. F., Canavan, S., Reale, M., Horowitz, A., Liu, P., & Girard, J. M. (2014). BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database. Image and Vision Computing, 32(10), 692–706."
  },
  {
    "objectID": "publications/proceedings/girard2014c.html#abstract",
    "href": "publications/proceedings/girard2014c.html#abstract",
    "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
    "section": "Abstract",
    "text": "Abstract\nFacial expression is central to human experience. Its efficiency and valid measurement are challenges that automated facial image analysis seeks to address. Most publically available databases are limited to 2D static images or video of posed facial behavior. Because posed and un-posed (aka “spontaneous”) facial expressions differ along several dimensions including complexity and timing, well-annotated video of un-posed facial behavior is needed. Moreover, because the face is a three-dimensional deformable object, 2D video may be insufficient, and therefore 3D video archives are required. We present a newly developed 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains. To the best of our knowledge, this new database is the first of its kind for the public. The work promotes the exploration of 3D spatiotemporal features in subtle facial expression, better understanding of the relation between pose and motion dynamics in facial action units, and deeper understanding of naturally occurring facial action."
  },
  {
    "objectID": "publications/proceedings/girard2014a.html",
    "href": "publications/proceedings/girard2014a.html",
    "title": "Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., Hammal, Z., & Rosenwald, D. P. (2014). Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses. Image and Vision Computing, 32(10), 641–647."
  },
  {
    "objectID": "publications/proceedings/girard2014a.html#citation-apa-7",
    "href": "publications/proceedings/girard2014a.html#citation-apa-7",
    "title": "Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., Hammal, Z., & Rosenwald, D. P. (2014). Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses. Image and Vision Computing, 32(10), 641–647."
  },
  {
    "objectID": "publications/proceedings/girard2014a.html#abstract",
    "href": "publications/proceedings/girard2014a.html#abstract",
    "title": "Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses",
    "section": "Abstract",
    "text": "Abstract\nThe relationship between nonverbal behavior and severity of depression was investigated by following depressed participants over the course of treatment and video recording a series of clinical interviews. Facial expressions and head pose were analyzed from video using manual and automatic systems. Both systems were highly consistent for FACS action units (AUs) and showed similar effects for change over time in depression severity. When symptom severity was high, participants made fewer affiliative facial expressions (AUs 12 and 15) and more non-affiliative facial expressions (AU 14). Participants also exhibited diminished head motion (i.e., amplitude and velocity) when symptom severity was high. These results are consistent with the Social Withdrawal hypothesis: that depressed individuals use nonverbal behavior to maintain or increase interpersonal distance. As individuals recover, they send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and revealed the same pattern of findings suggests that automatic facial expression analysis may be ready to relieve the burden of manual coding in behavioral and clinical science."
  },
  {
    "objectID": "publications/proceedings/girard2014b.html",
    "href": "publications/proceedings/girard2014b.html",
    "title": "CARMA: Software for continuous affect rating and media annotation",
    "section": "",
    "text": "Girard, J. M. (2014). CARMA: Software for continuous affect rating and media annotation. Journal of Open Research Software, 2(1), e5–e5."
  },
  {
    "objectID": "publications/proceedings/girard2014b.html#citation-apa-7",
    "href": "publications/proceedings/girard2014b.html#citation-apa-7",
    "title": "CARMA: Software for continuous affect rating and media annotation",
    "section": "",
    "text": "Girard, J. M. (2014). CARMA: Software for continuous affect rating and media annotation. Journal of Open Research Software, 2(1), e5–e5."
  },
  {
    "objectID": "publications/proceedings/girard2014b.html#abstract",
    "href": "publications/proceedings/girard2014b.html#abstract",
    "title": "CARMA: Software for continuous affect rating and media annotation",
    "section": "Abstract",
    "text": "Abstract\nCARMA is a media annotation program that collects continuous ratings while displaying audio and video files. It is designed to be highly user-friendly and easily customizable. Based on Gottman and Levenson’s affect rating dial, CARMA enables researchers and study participants to provide moment-by-moment ratings of multimedia files using a computer mouse or keyboard. The rating scale can be configured on a number of parameters including the labels for its upper and lower bounds, its numerical range, and its visual representation. Annotations can be displayed alongside the multimedia file and saved for easy import into statistical analysis software. CARMA provides a tool for researchers in affective computing, human-computer interaction, and the social sciences who need to capture the unfolding of subjective experience and observable behavior over time."
  },
  {
    "objectID": "publications/proceedings/girard2015a.html",
    "href": "publications/proceedings/girard2015a.html",
    "title": "Spontaneous facial expression in unscripted social interactions can be measured automatically",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Sayette, M. A., & De la Torre, F. (2015). Spontaneous facial expression in unscripted social interactions can be measured automatically. Behavior Research Methods, 47(4), 1136–1147."
  },
  {
    "objectID": "publications/proceedings/girard2015a.html#citation-apa-7",
    "href": "publications/proceedings/girard2015a.html#citation-apa-7",
    "title": "Spontaneous facial expression in unscripted social interactions can be measured automatically",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Jeni, L. A., Sayette, M. A., & De la Torre, F. (2015). Spontaneous facial expression in unscripted social interactions can be measured automatically. Behavior Research Methods, 47(4), 1136–1147."
  },
  {
    "objectID": "publications/proceedings/girard2015a.html#abstract",
    "href": "publications/proceedings/girard2015a.html#abstract",
    "title": "Spontaneous facial expression in unscripted social interactions can be measured automatically",
    "section": "Abstract",
    "text": "Abstract\nMethods to assess individual facial actions have potential to shed light on important behavioral phenomena ranging from emotion and social interaction to psychological disorders and health. However, manual coding of such actions is labor intensive and requires extensive training. To date, establishing reliable automated coding of unscripted facial actions has been a daunting challenge impeding development of psychological theories and applications requiring facial expression assessment. It is therefore essential that automated coding systems be developed with enough precision and robustness to ease the burden of manual coding in challenging data involving variation in participant gender, ethnicity, head pose, speech, and occlusion. We report a major advance in automated coding of spontaneous facial actions during an unscripted social interaction involving three strangers. For each participant (n = 80, 47 % women, 15 % Nonwhite), 25 facial action units (AUs) were manually coded from video using the Facial Action Coding System. Twelve AUs occurred more than 3 % of the time and were processed using automated FACS coding. Automated coding showed very strong reliability for the proportion of time that each AU occurred (mean intraclass correlation = 0.89), and the more stringent criterion of frame-by-frame reliability was moderate to strong (mean Matthew’s correlation = 0.61). With few exceptions, differences in AU detection related to gender, ethnicity, pose, and average pixel intensity were small. Fewer than 6 % of frames could be coded manually but not automatically. These findings suggest automated FACS coding has progressed sufficiently to be applied to observational research in emotion and related areas of study."
  },
  {
    "objectID": "publications/proceedings/girard2015c.html",
    "href": "publications/proceedings/girard2015c.html",
    "title": "Automated audiovisual depression analysis",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2015). Automated audiovisual depression analysis. Current Opinion in Psychology, 4, 75–79."
  },
  {
    "objectID": "publications/proceedings/girard2015c.html#citation-apa-7",
    "href": "publications/proceedings/girard2015c.html#citation-apa-7",
    "title": "Automated audiovisual depression analysis",
    "section": "",
    "text": "Girard, J. M., & Cohn, J. F. (2015). Automated audiovisual depression analysis. Current Opinion in Psychology, 4, 75–79."
  },
  {
    "objectID": "publications/proceedings/girard2015c.html#abstract",
    "href": "publications/proceedings/girard2015c.html#abstract",
    "title": "Automated audiovisual depression analysis",
    "section": "Abstract",
    "text": "Abstract\nAnalysis of observable behavior in depression primarily relies on subjective measures. New computational approaches make possible automated audiovisual measurement of behaviors that humans struggle to quantify (e.g., movement velocity and voice inflection). These tools have the potential to improve screening and diagnosis, identify new behavioral indicators of depression, measure response to clinical intervention, and test clinical theories about underlying mechanisms. Highlights include a study that measured the temporal coordination of vocal tract and facial movements, a study that predicted which adolescents would go on to develop depression based on their voice qualities, and a study that tested the behavioral predictions of clinical theories using automated measures of facial actions and head motion."
  },
  {
    "objectID": "publications/proceedings/girard2017a.html",
    "href": "publications/proceedings/girard2017a.html",
    "title": "Interpersonal problems across levels of the psychopathology hierarchy",
    "section": "",
    "text": "Girard, J. M., Wright, A. G. C., Beeney, J. E., Lazarus, S. A., Scott, L. N., Stepp, S. D., & Pilkonis, P. A. (2017). Interpersonal problems across levels of the psychopathology hierarchy. Comprehensive Psychiatry, 79, 53–69."
  },
  {
    "objectID": "publications/proceedings/girard2017a.html#citation-apa-7",
    "href": "publications/proceedings/girard2017a.html#citation-apa-7",
    "title": "Interpersonal problems across levels of the psychopathology hierarchy",
    "section": "",
    "text": "Girard, J. M., Wright, A. G. C., Beeney, J. E., Lazarus, S. A., Scott, L. N., Stepp, S. D., & Pilkonis, P. A. (2017). Interpersonal problems across levels of the psychopathology hierarchy. Comprehensive Psychiatry, 79, 53–69."
  },
  {
    "objectID": "publications/proceedings/girard2017a.html#abstract",
    "href": "publications/proceedings/girard2017a.html#abstract",
    "title": "Interpersonal problems across levels of the psychopathology hierarchy",
    "section": "Abstract",
    "text": "Abstract\nWe examined the relationship between psychopathology and interpersonal problems in a sample of 825 clinical and community participants. Sixteen psychiatric diagnoses and five transdiagnostic dimensions were examined in relation to self-reported interpersonal problems. The structural summary method was used with the Inventory of Interpersonal Problems Circumplex Scales to examine interpersonal problem profiles for each diagnosis and dimension. We built a structural model of mental disorders including factors corresponding to detachment (avoidant personality, social phobia, major depression), internalizing (dependent personality, borderline personality, panic disorder, posttraumatic stress, major depression), disinhibition (antisocial personality, drug dependence, alcohol dependence, borderline personality), dominance (histrionic personality, narcissistic personality, paranoid personality), and compulsivity (obsessive-compulsive personality). All dimensions showed good interpersonal prototypicality (e.g., detachment was defined by a socially avoidant/nonassertive interpersonal profile) except for internalizing, which was diffusely associated with elevated interpersonal distress. The findings for individual disorders were largely consistent with the dimension that each disorder loaded on, with the exception of the internalizing and dominance disorders, which were interpersonally heterogeneous. These results replicate previous findings and provide novel insights into social dysfunction in psychopathology by wedding the power of hierarchical (i.e., dimensional) modeling and interpersonal circumplex assessment."
  },
  {
    "objectID": "publications/proceedings/girard2021a.html",
    "href": "publications/proceedings/girard2021a.html",
    "title": "Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Yin, L., & Morency, L.-P. (2021). Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion. Affective Science, 2(1), 32–47."
  },
  {
    "objectID": "publications/proceedings/girard2021a.html#citation-apa-7",
    "href": "publications/proceedings/girard2021a.html#citation-apa-7",
    "title": "Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion",
    "section": "",
    "text": "Girard, J. M., Cohn, J. F., Yin, L., & Morency, L.-P. (2021). Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion. Affective Science, 2(1), 32–47."
  },
  {
    "objectID": "publications/proceedings/girard2021a.html#abstract",
    "href": "publications/proceedings/girard2021a.html#abstract",
    "title": "Reconsidering the Duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion",
    "section": "Abstract",
    "text": "Abstract\nThe common view of emotional expressions is that certain configurations of facial-muscle movements reliably reveal certain categories of emotion. The principal exemplar of this view is the Duchenne smile, a configuration of facial-muscle movements (i.e., smiling with eye constriction) that has been argued to reliably reveal genuine positive emotion. In this paper, we formalized a list of hypotheses that have been proposed regarding the Duchenne smile, briefly reviewed the literature weighing on these hypotheses, identified limitations and unanswered questions, and conducted two empirical studies to begin addressing these limitations and answering these questions. Both studies analyzed a database of 751 smiles observed while 136 participants completed experimental tasks designed to elicit amusement, embarrassment, fear, and physical pain. Study 1 focused on participants’ self-reported positive emotion and Study 2 focused on how third-party observers would perceive videos of these smiles. Most of the hypotheses that have been proposed about the Duchenne smile were either contradicted by or only weakly supported by our data. Eye constriction did provide some information about experienced positive emotion, but this information was lacking in specificity, already provided by other smile characteristics, and highly dependent on context. Eye constriction provided more information about perceived positive emotion, including some unique information over other smile characteristics, but context was also important here as well. Overall, our results suggest that accurately inferring positive emotion from a smile requires more sophisticated methods than simply looking for the presence/absence (or even the intensity) of eye constriction."
  },
  {
    "objectID": "publications/proceedings/grove2019.html",
    "href": "publications/proceedings/grove2019.html",
    "title": "Narcissistic admiration and rivalry: An interpersonal approach to construct validation",
    "section": "",
    "text": "Grove, J. L., Smith, T. W., Girard, J. M., & Wright, A. G. (2019). Narcissistic admiration and rivalry: An interpersonal approach to construct validation. Journal of Personality Disorders, 33(6), 751–775."
  },
  {
    "objectID": "publications/proceedings/grove2019.html#citation-apa-7",
    "href": "publications/proceedings/grove2019.html#citation-apa-7",
    "title": "Narcissistic admiration and rivalry: An interpersonal approach to construct validation",
    "section": "",
    "text": "Grove, J. L., Smith, T. W., Girard, J. M., & Wright, A. G. (2019). Narcissistic admiration and rivalry: An interpersonal approach to construct validation. Journal of Personality Disorders, 33(6), 751–775."
  },
  {
    "objectID": "publications/proceedings/grove2019.html#abstract",
    "href": "publications/proceedings/grove2019.html#abstract",
    "title": "Narcissistic admiration and rivalry: An interpersonal approach to construct validation",
    "section": "Abstract",
    "text": "Abstract\nThe present study applied the interpersonal perspective in testing the narcissistic admiration and rivalry concept (NARC) and examining the construct validity of the corresponding Narcissistic Admiration and Rivalry Questionnaire (NARQ). Two undergraduate samples (Sample 1: N = 290; Sample 2: N = 188) completed self-report measures of interpersonal processes based in the interpersonal circumplex (IPC), as well as measures of related constructs. In examining IPC correlates, we used a novel bootstrapping approach to determine if admiration and rivalry related to differing interpersonal profiles. Consistent with our hypotheses, admiration was distinctly related to generally agentic (i.e., dominant) interpersonal processes, whereas rivalry generally reflected (low) communal (i.e., hostile) interpersonal processes. Furthermore, NARQ-admiration and NARQ-rivalry related to generally adaptive and maladaptive aspects of status-related constructs, emotional, personality, and social adjustment, respectively. This research provides further support for the NARC, as well as construct validation for the NARQ."
  },
  {
    "objectID": "publications/proceedings/mcduff2017.html",
    "href": "publications/proceedings/mcduff2017.html",
    "title": "Large-scale observational evidence of cross-cultural differences in facial behavior",
    "section": "",
    "text": "McDuff, D., Girard, J. M., & El Kaliouby, R. (2017). Large-scale observational evidence of cross-cultural differences in facial behavior. Journal of Nonverbal Behavior, 41(1), 1–19."
  },
  {
    "objectID": "publications/proceedings/mcduff2017.html#citation-apa-7",
    "href": "publications/proceedings/mcduff2017.html#citation-apa-7",
    "title": "Large-scale observational evidence of cross-cultural differences in facial behavior",
    "section": "",
    "text": "McDuff, D., Girard, J. M., & El Kaliouby, R. (2017). Large-scale observational evidence of cross-cultural differences in facial behavior. Journal of Nonverbal Behavior, 41(1), 1–19."
  },
  {
    "objectID": "publications/proceedings/mcduff2017.html#abstract",
    "href": "publications/proceedings/mcduff2017.html#abstract",
    "title": "Large-scale observational evidence of cross-cultural differences in facial behavior",
    "section": "Abstract",
    "text": "Abstract\nSelf-report studies have found evidence that cultures differ in the display rules they have for facial expressions (i.e., for what is appropriate for different people at different times). However, observational studies of actual patterns of facial behavior have been rare and typically limited to the analysis of dozens of participants from two or three regions. We present the first large-scale evidence of cultural differences in observed facial behavior, including 740,984 participants from 12 countries around the world. We used an Internet-based framework to collect video data of participants in two different settings: in their homes and in market research facilities. Using computer vision algorithms designed for this dataset, we measured smiling and brow furrowing expressions as participants watched television ads. Our results reveal novel findings and provide empirical evidence to support theories about cultural and gender differences in display rules. Participants from more individualist cultures displayed more brow furrowing overall, whereas smiling depended on both culture and setting. Specifically, participants from more individualist countries were more expressive in the facility setting, while participants from more collectivist countries were more expressive in the home setting. Female participants displayed more smiling and less brow furrowing than male participants overall, with the latter difference being more pronounced in more individualist countries. This is the first study to leverage advances in computer science to enable large-scale observational research that would not have been possible using traditional methods."
  },
  {
    "objectID": "publications/proceedings/rincon2024.html",
    "href": "publications/proceedings/rincon2024.html",
    "title": "Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics",
    "section": "",
    "text": "Rincon Caicedo, M., Girard, J. M., Punt, S. E., Giovanetti, A. K., & Ilardi, S. S. (in press). Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics. Journal of Latinx Psychology."
  },
  {
    "objectID": "publications/proceedings/rincon2024.html#citation-apa-7",
    "href": "publications/proceedings/rincon2024.html#citation-apa-7",
    "title": "Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics",
    "section": "",
    "text": "Rincon Caicedo, M., Girard, J. M., Punt, S. E., Giovanetti, A. K., & Ilardi, S. S. (in press). Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics. Journal of Latinx Psychology."
  },
  {
    "objectID": "publications/proceedings/rincon2024.html#abstract",
    "href": "publications/proceedings/rincon2024.html#abstract",
    "title": "Depressive symptoms among Hispanic Americans: Investigating the interplay of acculturation and demographics",
    "section": "Abstract",
    "text": "Abstract\nAs with many other racial and ethnic minorities, Hispanic Americans face substantial disparities in health care access and disease prevalence. The published literature on mental health disorders among Hispanic individuals, however, is not robust, and their experience of depressive disorders remains poorly understood. The construct of acculturation may help elucidate the risk of depression among Hispanic Americans and may inform the development of appropriate policy and treatment resources. We examined the degree to which acculturation may interact with key demographic variables (sex, age, socioeconomic status [SES], and Mexican ancestry) in accounting for depressive symptomatology among Hispanic Americans. We conducted a series of Bayesian generalized linear mixed models using data from the National Health and Nutrition Examination Survey to investigate the self-reported depressive symptomatology (measured by the Patient Health Questionnaire−9) of Mexican Americans and other Hispanic individuals and to examine possible effects of acculturation and demographics on depressive symptomatology in this population. Mexican Americans had substantially lower levels of depression than other Hispanic individuals. Acculturation was positively associated with depression severity, but this effect was moderated by sex and SES. High acculturation was more strongly linked to depression among men and those of high SES. Acculturation and several demographic factors were associated with depressive symptomatology among Hispanic individuals. Acculturation can be useful in understanding risk, developing culturally informed interventions, and implementing community-level changes to address the burden of Hispanic depression."
  },
  {
    "objectID": "publications/proceedings/sewall2021.html",
    "href": "publications/proceedings/sewall2021.html",
    "title": "A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder",
    "section": "",
    "text": "Sewall, C. J. R., Girard, J. M., Merranko, J., Hafeman, D., Goldstein, B. I., Strober, M., Hower, H., Weinstock, L. M., Yen, S., Ryan, N. D., Keller, M. B., Liao, F., Diler, R. S., Gill, M. K., Axelson, D., Birmaher, B., & Goldstein, T. R. (2021). A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder. The Journal of Child Psychology and Psychiatry, 62(7), 905–9115."
  },
  {
    "objectID": "publications/proceedings/sewall2021.html#citation-apa-7",
    "href": "publications/proceedings/sewall2021.html#citation-apa-7",
    "title": "A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder",
    "section": "",
    "text": "Sewall, C. J. R., Girard, J. M., Merranko, J., Hafeman, D., Goldstein, B. I., Strober, M., Hower, H., Weinstock, L. M., Yen, S., Ryan, N. D., Keller, M. B., Liao, F., Diler, R. S., Gill, M. K., Axelson, D., Birmaher, B., & Goldstein, T. R. (2021). A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder. The Journal of Child Psychology and Psychiatry, 62(7), 905–9115."
  },
  {
    "objectID": "publications/proceedings/sewall2021.html#abstract",
    "href": "publications/proceedings/sewall2021.html#abstract",
    "title": "A Bayesian multilevel analysis of the longitudinal associations between relationship quality and suicidal ideation and attempts among youth with bipolar disorder",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nYouth with bipolar disorder (BD) are at high risk for suicidal thoughts and behaviors and frequently experience interpersonal impairment, which is a risk factor for suicide. Yet, no study to date has examined the longitudinal associations between relationship quality in family/peer domains and suicidal thoughts and behaviors among youth with BD. Thus, we investigated how between-person differences – reflecting the average relationship quality across time – and within-person changes, reflecting recent fluctuations in relationship quality, act as distal and/or proximal risk factors for suicidal ideation (SI) and suicide attempts.\n\n\nMethods\nWe used longitudinal data from the Course and Outcome of Bipolar Youth Study (N = 413). Relationship quality variables were decomposed into stable (i.e., average) and varying (i.e., recent) components and entered, along with major clinical covariates, into separate Bayesian multilevel models predicting SI and suicide attempt. We also examined how the relationship quality effects interacted with age and sex.\n\n\nResults\nPoorer average relationship quality with parents (β = −.33, 95% Bayesian highest density interval (HDI) [−0.54, −0.11]) or friends (β = −.33, 95% HDI [−0.55, −0.11]) was longitudinally associated with increased risk of SI but not suicide attempt. Worsening recent relationship quality with parents (β = −.10, 95% HDI [−0.19, −0.03]) and, to a lesser extent, friends (β = −.06, 95% HDI [−0.15, 0.03]) was longitudinally associated with increased risk of SI, but only worsening recent relationship quality with parents was also associated with increased risk of suicide attempt (β = −.15, 95% HDI [−0.31, 0.01]). The effects of certain relationship quality variables were moderated by gender but not age.\n\n\nConclusions\nAmong youth with BD, having poorer average relationship quality with peers and/or parents represents a distal risk factor for SI but not suicide attempts. Additionally, worsening recent relationship quality with parents may be a time-sensitive indicator of increased risk for SI or suicide attempt."
  },
  {
    "objectID": "publications/proceedings/sprunger2024.html",
    "href": "publications/proceedings/sprunger2024.html",
    "title": "Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample",
    "section": "",
    "text": "Sprunger, J. G., Girard, J. M., & Chard, K. M. (2024). Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample. Journal of Traumatic Stress."
  },
  {
    "objectID": "publications/proceedings/sprunger2024.html#citation-apa-7",
    "href": "publications/proceedings/sprunger2024.html#citation-apa-7",
    "title": "Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample",
    "section": "",
    "text": "Sprunger, J. G., Girard, J. M., & Chard, K. M. (2024). Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample. Journal of Traumatic Stress."
  },
  {
    "objectID": "publications/proceedings/sprunger2024.html#abstract",
    "href": "publications/proceedings/sprunger2024.html#abstract",
    "title": "Associations between transdiagnostic traits of psychopathology and hybrid posttraumatic stress disorder factors in a trauma-exposed community sample",
    "section": "Abstract",
    "text": "Abstract\nDimensional conceptualizations of psychopathology hold promise for understanding the high rates of comorbidity with posttraumatic stress disorder (PTSD). Linking PTSD symptoms to transdiagnostic dimensions of psychopathology may enable researchers and clinicians to understand the patterns and breadth of behavioral sequelae following traumatic experiences that may be shared with other psychiatric disorders. To explore this premise, we recruited a trauma-exposed online community sample \\((N = 462)\\) and measured dimensional transdiagnostic traits of psychopathology using parceled facets derived from the Personality Inventory for DSM-5 Faceted–Short Form. PTSD symptom factors were measured using the PTSD Checklist for DSM-5 and derived using confirmatory factor analysis according to the seven-factor hybrid model (i.e., Intrusions, Avoidance, Negative Affect, Anhedonia, Externalizing Behaviors, Anxious Arousal, And Dysphoric Arousal). We observed hypothesized associations between PTSD factors and transdiagnostic traits indicating that some transdiagnostic dimensions were associated with nearly all PTSD symptom factors (e.g., emotional lability: rmean = .35), whereas others showed more unique relationships (e.g., hostility–Externalizing Behavior: \\(r = .60\\); hostility with other PTSD factors: \\(r = .12–.31\\)). All PTSD factors were correlated with traits beyond those that would appear to be construct-relevant, suggesting the possibility of indirect associations that should be explicated in future research. The results indicate the breadth of trait-like consequences associated with PTSD symptom exacerbation, with implications for case conceptualization and treatment planning. Although PTSD is not a personality disorder, the findings indicate that increased PTSD factor severity is moderately associated with different patterns of trait-like disruptions in many areas of functioning."
  },
  {
    "objectID": "publications/proceedings/vanoest2022.html",
    "href": "publications/proceedings/vanoest2022.html",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "",
    "text": "van Oest, R., & Girard, J. M. (2022). Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement. Psychological Methods, 27(6), 1069–1088."
  },
  {
    "objectID": "publications/proceedings/vanoest2022.html#citation-apa-7",
    "href": "publications/proceedings/vanoest2022.html#citation-apa-7",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "",
    "text": "van Oest, R., & Girard, J. M. (2022). Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement. Psychological Methods, 27(6), 1069–1088."
  },
  {
    "objectID": "publications/proceedings/vanoest2022.html#abstract",
    "href": "publications/proceedings/vanoest2022.html#abstract",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "Abstract",
    "text": "Abstract\nVan Oest (2019) developed a framework to assess interrater agreement for nominal categories and complete data. We generalize this framework to all four situations of nominal or ordinal categories and complete or incomplete data. The mathematical solution yields a chance-corrected agreement coefficient that accommodates any weighting scheme for penalizing rater disagreements and any number of raters and categories. By incorporating Bayesian estimates of the category proportions, the generalized coefficient also captures situations in which raters classify only subsets of items; that is, incomplete data. Furthermore, this coefficient encompasses existing chance-corrected agreement coefficients: the S-coefficient, Scott’s pi, Fleiss’ kappa, and Van Oest’s uniform prior coefficient, all augmented with a weighting scheme and the option of incomplete data. We use simulation to compare these nested coefficients. The uniform prior coefficient tends to perform best, in particular, if one category has a much larger proportion than others. The gap with Scott’s pi and Fleiss’ kappa widens if the weighting scheme becomes more lenient to small disagreements and often if more item classifications are missing; missingness biases play a moderating role. The uniform prior coefficient often performs much better than the S-coefficient, but the S-coefficient sometimes performs best for small samples, missing data, and lenient weighting schemes. The generalized framework implies a new interpretation of chance-corrected weighted agreement coefficients: These coefficients estimate the probability that both raters in a pair assign an item to its correct category without guessing. Whereas Van Oest showed this interpretation for unweighted agreement, we generalize to weighted agreement."
  },
  {
    "objectID": "publications/proceedings/vanoest2022.html#translational-abstract",
    "href": "publications/proceedings/vanoest2022.html#translational-abstract",
    "title": "Weighting schemes and incomplete data: A generalized Bayesian framework for chance-corrected interrater agreement",
    "section": "Translational Abstract",
    "text": "Translational Abstract\nMany studies and assessments require classification of subjective items (e.g., text) into categories (e.g., based on content). To assess whether the results are reproducible, it is good practice to let two or more raters independently classify the items, compute the proportion of pairwise rater agreement, and adjust for agreement expected by chance. Most chance-corrected agreement coefficients assume nominal categories and include only full agreements in which raters choose the same category. However, many situations (e.g., point scales) imply ordinal categories, where raters may receive partial credit for disagreements, based on the distance of their chosen categories and captured by a weighting scheme. Furthermore, raters often classify only subsets of items, where the missing data occur either by accident or by design. The present study develops a framework to estimate chance-corrected agreement for all four combinations of nominal or ordinal categories and complete or incomplete data. The resulting coefficient requires only a few lines of programming code and captures several existing coefficients via different values of its input parameters; it augments all nested coefficients with a weighting scheme and the option of missing item classifications. We use simulation to compare the coefficient performances for different weighting schemes, missing data mechanisms, and category proportions: The so-called uniform prior coefficient often (but not always) performs best. Furthermore, our framework implies that chance-corrected agreement coefficients, both unweighted and weighted, estimate the probability that both raters in a pair assign an item to its correct category without guessing."
  },
  {
    "objectID": "talks/pes_gm_2025/index.html",
    "href": "talks/pes_gm_2025/index.html",
    "title": "Transfer Learning in Deep Reinforcement Learning for Scalable VVC in Smart Grids",
    "section": "",
    "text": "About the talk\nWe developed a TL-DRL framework, as shown in Fig.~\\(\\ref{fig:A2C}\\), which involves transferring policy knowledge from one distribution grid to another. Additionally, we have created a policy reuse classifier to determine whether to transfer the policy knowledge from the IEEE-123 Bus to the IEEE-13 Bus system and conducted an impact analysis. \\[\\begin{equation}\\label{eq:ppo_objective}\n\\theta_{\\text{source}} = \\arg \\max_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t - \\beta \\text{CLIP}(\\theta) \\right]\n\\end{equation}\\] The \\(\\theta_{\\text{source}}\\) is trained with the DRL algorithm on the IEEE-123 Bus, which regulates the VVC voltage profiles within permissible limits.\nWhile \\(\\theta_{\\text{target}}\\) is the model which transferred the knowledge \\(\\theta_{\\text{source}}\\) and adapts well in the \\(\\theta_{\\text{target}}\\) domain. \\[\\begin{equation}\\label{eq:target_theta}\n\\theta_{\\text{target}} =\n\\begin{cases}\n\\begin{aligned}\n\\theta_{\\text{source}}\\; & \\text{if } P(\\text{Reuse}|\\text{Observation}) &gt; 0.5\n\\end{aligned} \\\\\n\\arg \\max_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right] & \\text{otherwise}\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#overview",
    "href": "blog1/technotes_20250703_research_guide/index.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "Overview",
    "text": "Overview\nThis guide provides a practical framework to prepare for Research Scientist roles in academia, industry research labs (FAANG, OpenAI, DeepMind, Anthropic, NVIDIA), and national labs. It blends personal experience with hiring-manager expectations and common evaluation rubrics.\nGitHub repository: Code\nData Science Notes: Data Science Intro\nStatistical Learning Notes: Statistical Analysis"
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#what-interviewers-evaluate",
    "href": "blog1/technotes_20250703_research_guide/index.html#what-interviewers-evaluate",
    "title": "Research Scientist Interview Guide",
    "section": "What interviewers evaluate",
    "text": "What interviewers evaluate\n\nResearch impact: novelty, citations/adoption, reproducibility, clarity of problem–method–evidence chain.\nTechnical depth: math/ML fundamentals, experimental rigor, ablation thinking, error analysis.\nSystems sense: how ideas become products—data, infra, metrics, reliability, safety & ethics.\nExecution: scope → plan → iterate → deliver (papers, open-source, patents, internal wins).\nCommunication & collaboration: explain complex work to varied audiences; cross-discipline work.\nCulture/LP fit: ownership, bias for action, frugality, customer/impact obsession.\n\n\n\n\n\n\n\nTip\n\n\n\nBe explicit about your contribution. For each project: problem → gap → idea → method → evidence → limitations → next steps → impact."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#interview-process-at-a-glance",
    "href": "blog1/technotes_20250703_research_guide/index.html#interview-process-at-a-glance",
    "title": "Research Scientist Interview Guide",
    "section": "Interview process at a glance",
    "text": "Interview process at a glance\nTypical stages\n1) Recruiter + HM intro → 2) Tech/Research screens (coding, ML/math, paper deep dive) →\n3) Onsite: research talk, systems/experimentation design, cross-functional, behavioral/bar raiser →\n4) Debrief → offer.\n(Use the SVG diagram you generated earlier or embed it with ![](research_scientist_interview_process.svg).)"
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#preparation-timeline-68-weeks",
    "href": "blog1/technotes_20250703_research_guide/index.html#preparation-timeline-68-weeks",
    "title": "Research Scientist Interview Guide",
    "section": "Preparation timeline (6–8 weeks)",
    "text": "Preparation timeline (6–8 weeks)\nWeeks 1–2: Foundations & portfolio - Curate 2–3 flagship projects; write 1-page project briefs (problem, novelty, 3 results, open questions). - Refresh ML math: gradients, likelihoods, bias–variance, generalization, off-policy vs on-policy RL. - DS&A 20–30 mins/day (arrays, hash maps, trees, graphs, DP—medium level). - Draft talk outline; collect figures; start a reproducible repo.\nWeeks 3–4: Research talk + deep dives - Build slides (30/45/60 min versions). Timebox: Motivation 10% → Method 35% → Evidence 40% → Limits + Roadmap 15%. - Prepare ablation stories and negative results; design a live error analysis demo if feasible. - Mock talks with labmates; iterate twice.\nWeeks 5–6: Systems & coding polish - 5 case studies: online inference, data pipelines, eval at scale, safety/guardrails, monitoring. - Practice 6–8 coding problems in 60-min sessions; review idioms (two-pointers, heap, BFS/DFS, topo sort). - Draft answers for 8–10 behavioral prompts using STAR(L).\nWeek 7+: Company-specific tuning - Read team papers/repos; align your roadmap slide to their charter. - Prepare 8–12 questions to ask (below). - Dry run full onsite (talk + 3 interviews + behavioral) in a single sitting."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#research-portfolio-deep-dive",
    "href": "blog1/technotes_20250703_research_guide/index.html#research-portfolio-deep-dive",
    "title": "Research Scientist Interview Guide",
    "section": "Research portfolio deep dive",
    "text": "Research portfolio deep dive\nFor each project, be ready to answer: - Gap: What prior SOTA did not address? Why now? - Assumptions: Distributional, structural, or operational assumptions—how validated? - Method: Key design choices (loss, architecture, training regime, priors/constraints). - Evidence: Metrics that matter (with CIs); strongest ablation; hardest failure case. - Impact: External adoption, dataset/code release, internal KPI movement, patents. - Next: What would you do with 3 months & a small team?\nArtifacts checklist - 10–12 figure slide deck (vector PDFs), 1-page PDF overview, GitHub README with quickstart, repro seed + script."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#technical-machine-learning-knowledge-what-to-refresh",
    "href": "blog1/technotes_20250703_research_guide/index.html#technical-machine-learning-knowledge-what-to-refresh",
    "title": "Research Scientist Interview Guide",
    "section": "Technical machine learning knowledge (what to refresh)",
    "text": "Technical machine learning knowledge (what to refresh)\n\nRL: policy gradient theorem; advantage estimation; PPO/TRPO constraints; off-policy (DQN/TD3/SAC); safe RL & constraints; exploration vs exploitation; eval instability & seeding.\nDeep learning: optimization (warmup, cosine decay, AdamW), regularization (dropout, mixup, label-smoothing), attention/transformers, LoRA/parameter-efficient finetuning.\nStatistics & probabilistic modeling: MLE/MAP; conjugacy; posterior predictive; calibration (ECE), uncertainty (epistemic vs aleatoric); A/B testing pitfalls.\nGenerative models: diffusion schedule & guidance, VAEs ELBO, GAN stability.\nLLMs: instruction tuning, RAG retrieval quality, eval (exact match, nDCG, win-rates), toxicity & safety filters, hallucination mitigation.\nVision/multimodal: contrastive learning, detection/segmentation metrics (mAP, IoU), data augmentations."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#ml-systems-experimentation-design",
    "href": "blog1/technotes_20250703_research_guide/index.html#ml-systems-experimentation-design",
    "title": "Research Scientist Interview Guide",
    "section": "ML systems & experimentation design",
    "text": "ML systems & experimentation design\n5-step template 1. Clarify goal (user KPI ↔︎ technical metric; online vs offline).\n2. Data (sources, labeling strategy, noise, sampling, privacy).\n3. Model & infra (baseline → candidate → serving path; latency, cost, reliability).\n4. Evaluation (offline metrics + counterfactual replays + online guardrails; slicing).\n5. Risk & safety (bias, misuse, red-teaming, rollback plan, observability).\nExample prompt (outline answer)\n“Design a near real-time anomaly detector for a power grid substation.”\n- KPI: reduce outage MTTR by 20%; constraints: &lt;200 ms latency, 99.9% uptime.\n- Data: PMU/SCADA streams; label via weak supervision + operator tags.\n- Baseline: statistical thresholds; Model: streaming autoencoder + EWMA residuals.\n- Eval: ROC-AUC offline, time-to-detect, false alarms/day; shadow deploy → phased rollout.\n- Safety: fail-open; human-in-the-loop; drift detector; incident playbook."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#coding-algorithmic-skills",
    "href": "blog1/technotes_20250703_research_guide/index.html#coding-algorithmic-skills",
    "title": "Research Scientist Interview Guide",
    "section": "Coding & algorithmic skills",
    "text": "Coding & algorithmic skills\n\nAim for clean, correct, then optimal. Speak invariants, test cases, and complexity out loud.\nPatterns to practice: two-pointers, sliding window, monotonic stack, BFS/DFS, topological sort, binary search on answer, Dijkstra/Union-Find, prefix sums, DP on sequences/trees.\nML-adjacent coding: vectorized NumPy, PyTorch modules/forward pass, dataloaders, batching, mixed precision, sanity checks."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#research-talk-structure-slides",
    "href": "blog1/technotes_20250703_research_guide/index.html#research-talk-structure-slides",
    "title": "Research Scientist Interview Guide",
    "section": "Research talk: structure & slides",
    "text": "Research talk: structure & slides\nSlide budget (45 min talk + Q&A) - Title & takeaway (1), Motivation (2), Problem/Gap (2), Method (5), Results (6), Ablations (3), Error analysis (2), Limits (1), Roadmap/fit (2).\nDos - One idea per slide; consistent color for your method; readable axes; include n and CI.\n- Put the thesis of each slide in the title: “Physics constraints cut violations by 38% at same cost.”\nDon’ts - Crowded plots, cherry-picked examples, unanchored qualitative claims, tiny captions."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#behavioral-star-research-flavors",
    "href": "blog1/technotes_20250703_research_guide/index.html#behavioral-star-research-flavors",
    "title": "Research Scientist Interview Guide",
    "section": "Behavioral: STAR + research flavors",
    "text": "Behavioral: STAR + research flavors\nUse STAR: Situation, Task, Action, Result, Learning.\nPrepare 6 stories: conflict, failure, leadership without authority, speed vs quality, mentoring, cross-team project.\nExample prompt\n“Tell me about a time your experiment invalidated a roadmap item.”\n- S/T: critical Q3 milestone hinged on SOTA surpassing baseline.\n- A: pre-registered analysis; ran holdout; flagged negative lift; proposed minimal viable alternative.\n- R: saved ~6 wks eng time; reallocated to data quality; shipped smaller win.\n- L: add “early stop” gates; improved pre-mortem checklist."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#recommended-resources",
    "href": "blog1/technotes_20250703_research_guide/index.html#recommended-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended resources",
    "text": "Recommended resources\n\nPapers: recent NeurIPS/ICLR/ICML tracks relevant to the team; read 2–3 team papers.\n\nBooks: Designing Machine Learning Systems (Huyen), Deep Learning (Goodfellow), ESL (HTF), Probabilistic ML (Barber/Murphy).\n\nPractice: LeetCode medium sets; pair-program ML design prompts; mock talks."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#example-technical-research-questions",
    "href": "blog1/technotes_20250703_research_guide/index.html#example-technical-research-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example technical & research questions",
    "text": "Example technical & research questions\n\nSummarize the core contribution of your latest paper in two sentences.\n\nWhich ablation most strongly supports your claim? Which one failed and why?\n\nHow would you adapt your method under 10× less data? Under severe shift?\n\nWhat is your evaluation blind spot today? How would you close it?\n\nExplain epistemic vs. aleatoric uncertainty with a concrete modeling choice.\n\nFor PPO, where does instability come from and how do you diagnose it?\n\nDesign a reliable RAG system for safety-critical queries—retrieval, scoring, guardrails, and evaluation."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#ask-the-interviewer",
    "href": "blog1/technotes_20250703_research_guide/index.html#ask-the-interviewer",
    "title": "Research Scientist Interview Guide",
    "section": "“Ask the interviewer”",
    "text": "“Ask the interviewer”\n\nUse the set that best fits the team. If time is short, ask the Top 3 in each block.\n\n\nHiring Manager (core, any research team)\n\nTop 3\n\nWhat research bets matter most in the next 6–12 months, and how will you measure success?\nWhat makes a “hell-yes” hire here after 90 days? What work would signal that?\nHow do ideas transition from a paper/prototype to production or a public result?\n\nHow is impact recognized—publications, product metrics, patents, internal adoption?\nWhat are examples of projects that didn’t land? Why, and what changed afterward?\nWhere are the biggest data/infra bottlenecks that a new scientist can unlock?\n\n\n\n\nResearch Scientists (peer scientists)\n\nTop 3\n\nWhich canonical datasets/eval harnesses are used for your area? How are baselines enforced?\nWhat’s a recent ablation or negative result that changed your roadmap?\nHow do you share/replicate experiments (internal tooling, seeds, result store)?\n\nHow are collaborations formed across teams? Any “platform” teams I should align with?\nWhat’s the cadence for paper reviews, reading groups, and internal talks?\n\n\n\n\nEngineers / MLEs (production & infra)\n\nTop 3\n\nWhat are the serving constraints (latency, throughput, cost) and reliability SLOs?\nWhat’s the path from notebook → feature store → online eval → rollout/rollback?\nHow do you monitor drift and failures in the wild? What’s the on-call/ownership model?\n\nWhat’s the CI/CD story for models (gating tests, shadow, canary, A/B infra)?\nWhat would you change in our current stack if you could?\n\n\n\n\nPM / Cross-functional (applied impact)\n\nTop 3\n\nWhich decision or workflow actually changes if this model improves by X%?\nWhat is the single metric you’d show leadership to justify continued investment?\nWhat risks (safety, bias, misuse) keep you up at night for this application?\n\nWhere does data come from and how does quality/coverage limit the roadmap?\n\n\n\n\nRecruiter / Compensation\n\nLevel targeting and calibration—what evidence best demonstrates readiness for this level?\nPublication & open-source policy (authors, timing, preprints), conference travel norms.\nVisa/relocation timeline; expected hire start window and interview re-try policy."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#example-domain-specific",
    "href": "blog1/technotes_20250703_research_guide/index.html#example-domain-specific",
    "title": "Research Scientist Interview Guide",
    "section": "Example Domain-specific",
    "text": "Example Domain-specific\n\nIf the team is RL / Control (energy, robotics, autonomy)\n\nWhat control horizon & loop latency are assumed (e.g., 200 ms, 1 s)? Any hard real-time constraints?\nHow are safety constraints enforced (e.g., physics-informed losses, shielded RL, reachability)?\nWhat are the reference environments (e.g., CityLearn/PowerGym, IEEE 13/34/123 bus, HIL)?\nHow do sim-to-real gaps show up, and how do you mitigate them (domain randomization, calibration)?\nWhich offline evaluation and counterfactual replay methods are trusted before online trials?\nWhat’s the bar for replacing a heuristic/OPF with an RL policy (guardrails, rollback, audits)?\n\n\n\nIf the team is LLM / RAG / GenAI\n\nRetrieval stack: index type, chunking, rerankers, eval (nDCG, recall@k, answer faithfulness).\nHallucination budget & safety: filters/guardrails, red-teaming, and incident handling.\nFinetuning strategy: SFT/LoRA vs. prompt-only; distillation plans; model/versioning policy.\nWhat constitutes “win” in offline eval vs. human eval? How do you resolve conflicts?\nData governance: PII/PHI handling, dedup, license compliance, auto-eval for drift.\n\n\n\nIf the team is Vision / Multimodal\n\nCanonical datasets & metrics (COCO mAP, IoU, retrieval R@k); how are domain shifts handled?\nLabeling strategy & quality control; synthetic data or augmentation policies.\nDeployment constraints: throughput on edge vs. server; quantization/compilation toolchain.\nFailure modes that matter most (false positives/negatives, OOD, adversarial artifacts).\n\n\n\n\nIf you only have time for 3 questions (universal)\n\nStrategy: What is the one result you’d want me to deliver in 6 months that proves this hire was a “yes”?\n\nExecution: What is the critical bottleneck (data, infra, evaluation) preventing faster progress today?\n\nFit: Which strengths would make me the complement to the current team’s skills?"
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#checklists",
    "href": "blog1/technotes_20250703_research_guide/index.html#checklists",
    "title": "Research Scientist Interview Guide",
    "section": "Checklists",
    "text": "Checklists\nDay-before technical Interview\n\nTalk readiness: 30/45/60-min versions; 1-sentence thesis; clearly mark your contributions.\nAblations & limits: 1 strongest ablation you can defend; 1 negative result and what it taught you.\nPaper deep dives: be ready to derive the key equation/algorithm; compare to 2 baselines with numbers.\nMath/ML refresh: RL (policy gradient, GAE, PPO/TRPO intuition), DL (optimizers, regularization), stats (MLE vs MAP, bias–variance, eval metrics).\nCoding drills: 2 timed mediums using core patterns (two-pointers, BFS/DFS, heap, topo sort, DP); practice test-first pseudocode.\nSystems prompts: outline 3 cases (data → model → eval → guardrails → rollout) you can walk through crisply.\nQ&A bank: 10 answers you can deliver fast (assumptions, failure modes, robustness, scalability, safety).\n\nDay-of Interview\n\nOpen strong: restate problem + constraints; define the success metric before proposing solutions.\nReasoning first: outline 2–3 approaches; justify trade-offs; state target time/space complexity.\nCoding round: implement incrementally; speak invariants and edge cases; analyze complexity after passing tests.\nResearch talk: show the central figure; defend an ablation; admit a limitation and the next experiment.\nDesign/experimentation: baseline → candidate → eval plan; define slices; propose risks & rollback.\nClose each round: 30–60s recap with decision, trade-offs, and “next step” you’d run."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#offer-debrief-negotiation",
    "href": "blog1/technotes_20250703_research_guide/index.html#offer-debrief-negotiation",
    "title": "Research Scientist Interview Guide",
    "section": "Offer, Debrief & Negotiation",
    "text": "Offer, Debrief & Negotiation\n1) Right after the loop\n\nKeep a short brag doc: 5–8 bullets tying your work to measurable impact (citations, benchmarks, improvements).\nSend thank-you notes; ask the recruiter for decision timeline and whether the team needs any follow-ups (extra slides, code pointers).\n\n2) Debrief (if you don’t get detailed feedback)\n\nAsk for 3 bullets: (i) strengths that stood out, (ii) top concern, (iii) what would change the decision next time.\nIf there’s a miscalibration (e.g., level/scope), propose a targeted follow-up (short tech screen or focused deep dive).\n\n3) Offer review (when it comes)\n\nBreak it down: base + bonus + equity/RSUs (grant, vesting, refreshers) + sign-on + extras (compute budget, conference travel, publication policy, patent bonus).\nClarify: level, title, team mandate, location policy, start date, performance review cycles, and conference/OSS policies.\n\n4) Negotiation (impact-first)\n\nAnchor with scope & impact (what you can deliver in 6–12 months), plus comparables (peer offers or market data for the same level/geo).\nPrioritize asks (pick 2–3): sign-on / equity / level / research budget / conf travel.\n\nSample script:\n“Given the scope (X) and the impact I’m positioned to deliver (Y), I’m targeting total comp of Z at level L. If level is fixed, increasing equity by A or adding a B sign-on would bridge the gap. I’d also value C (e.g., conference travel commitment).”\n\n5) Timelines\n\nIf you need time: “I’m very excited. To make a well-considered decision, could we set a reply date of ? I want to complete one pending loop and compare scopes fairly.”\nIf there’s an exploding deadline, ask for a short extension in exchange for a firm decision date and clear intent.\n\n6) If rejected\n\nRequest specific growth areas and ask whether a re-interview window (e.g., 6 months) with a targeted bar (e.g., systems/experiments) is possible."
  },
  {
    "objectID": "blog1/technotes_20250703_research_guide/index.html#mentorship",
    "href": "blog1/technotes_20250703_research_guide/index.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’d like feedback on your talk, paper deep dive, or a full mock onsite, reach me at cs.kundann@gmail.com."
  },
  {
    "objectID": "blog1/technotes_20231018_qt_styling/index.html",
    "href": "blog1/technotes_20231018_qt_styling/index.html",
    "title": "Styling your quarto project",
    "section": "",
    "text": "Useful references:\n\nTalk by Emil Hvitfeldt on Styling and Templating Quarto Documents"
  },
  {
    "objectID": "blog1/technotes_20230519_pkgcran/index.html",
    "href": "blog1/technotes_20230519_pkgcran/index.html",
    "title": "R package workflow",
    "section": "",
    "text": "This checklist is being updated over time. Mostly for my own use; but great if it helps you as well!\nFor a complete treatment, please refer to R Packages (2e) by Hadley Wickham and Jennifer Bryan."
  },
  {
    "objectID": "blog1/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "href": "blog1/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "title": "R package workflow",
    "section": "Initialize the project",
    "text": "Initialize the project\n\nusethis::create_package('path_to_pkg/pkgname') \n\nIt opens a new R project (directory) named pkgname, with the following items:\n\nDESCRIPTION\nNAMESPACE\ndirectory R/\n.Rbuildignore and .gitignore\nand the project icon, pkgname.Rproj.\n\nIf you have an existing R project but wish to build a package there, copy everything but pkgname.Rproj, and modify the files in your existing pkg directory. Pay extra attention to the hidden files like .Rbuildignore.\n\nusethis::use_mit_license() # modify name to yours\nusethis::use_readme_md() # if you do not have this already\nusethis::use_news_md()\nusethis::use_test()\n\n# create a folder for future data documentation\nx &lt;- 1 \nusethis::use_data() \n\nIn addition, URL and bug reports should be added in the DESCRIPTION."
  },
  {
    "objectID": "blog1/technotes_20230519_pkgcran/index.html#planning",
    "href": "blog1/technotes_20230519_pkgcran/index.html#planning",
    "title": "R package workflow",
    "section": "Planning",
    "text": "Planning\nIt is good practice to start with planning the package, rather than directly start coding.\nCreate a folder called dev. To prevent it from being built, add the following line in .Rbuildignore"
  },
  {
    "objectID": "blog1/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "href": "blog1/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "title": "R package workflow",
    "section": "Write, test and document",
    "text": "Write, test and document\nCreate exported functions in R/, development code in script/ (or somewhere else, such as dev/).\n\nData: raw and processed\nNeed to be clear in mind where the data files go. There are a few data related folders:\n\nraw data files, in the format of excel sheets or csv. Usually placed as inst/data_name.csv\nR scripts to process the raw data so that we create data object inside the package, put inside data-raw\ndata objects that can be called as pkg::data_name, are placed in data. These files are usually directly generated by executing write.rda().\ndata documentation, usually placed in R/data_documentation.R. These are Roxygen2 documents for the data.\n\n\n\nDocumentation\nYou need to configure the Build tools.\nThese three things should be done:\n\nFunction documentation\nCreate a function f1, and put your cursor on it. Go to Code -&gt; Insert Roxygen Skeleton to create the template.\nAlternatively, use #' to start.\n\n#' A simple placehold function \n#'\n#' @param x a numeric value\n#'\n#' @return a value 3 greater than the input\n#' @export\n#'\n#' @examples \n#' f1(5)\nf1 &lt;- function(x){\n  x+3\n}\n\n\n\nData documentation\nIt can be beneficial to create a separate file to document data only, say data_documentation.R under the R/ directory.\n\n#' Placeholder data x\n#'\n#' This dataset contains one value, x\n#'\n#' @format\n#' \\describe{\n#' \\item{x}{The placeholder data x}\n#' }\n#' @examples\n#' print(x)\n\"x\"\n\n\n\nVignette documentation\n\nusethis::use_vignette('your_vignette')\n\n\n\nDeploy to pkgdown\nCheck this reference here"
  },
  {
    "objectID": "blog1/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "href": "blog1/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "title": "R package workflow",
    "section": "Build package and check",
    "text": "Build package and check\nIt is possible that your checks don’t pass on the first try.\n\nWhat to ignore when build?\n^.*\\.Rproj$\n^\\.Rproj\\.user$\n^dev$\n^_pkgdown\\.yml$\n^license\\.md$\nMakefile\ndata-raw\ncran-comments.md\n^\\.github$"
  },
  {
    "objectID": "blog1/technotes_20230228_clinreport_part3/index.html",
    "href": "blog1/technotes_20230228_clinreport_part3/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog1/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "href": "blog1/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Principles and tools",
    "text": "Principles and tools\nReproducibility: Git (code versioning), dependencies (renv for r package dependencies, Docker for system dependencies)\n\nClean code\nCode comments: not recommended! Better to write code in a way that does not need additional comments.\nDRY: don’t repeat yourself (principle of software development), avoid copy and paste everywhere.\nSRP: single-responsibility prinicple, a function should do one thing: either plot a chart, saves a file, changes variables etc, but not all.\nNaming conventions\n\nReserve dots (.) for S3 methods (print.patient)\nReserve CamelCase for R6 classes or package names (OurPatients)\nUse snake cases (all_patients) for function names and arguments, use verb noun pattern (plot_this())\n\n\n\nCode smells\nA function might be too large: break into smaller ones (e.g. could fit in one screen)\nA function violates SRP: break into smaller ones, and be explicit in what result it is expected to return\nA function with multiple arguments: the scenarios to be tested increase rapidly. Recommended to minimize number of critical function arguments, and break the function into smaller ones.\nBad comments in the code: drop the unnecessary, unclear, outdated comments, write code that are self-explanatory.\n\n\nDevelopment workflow\nCode refactoring: change existing code without its functionality\nTDD: Test-Driven Development\n\nstart with writing a new (failing) test\nwrite code thtat passes the nenw tetst\nrefactor the code\nand repeat\n\nBenefits: your code is covered by tests; you think of testing scenarios first; “fail fast” - can immediately repair the code; more freedom to refactor (improve) the code.\nHow to test\n\nautomatically: CI/CD, after pushing Git commits\nmanually:\n\nrun all unit tests in the package (Build / Test package)\nrun tests in a selected test file (Run Tests)\nrun a single test in Rstudio console\n\n\nHow to check\n\nR CMD CHECK"
  },
  {
    "objectID": "blog1/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "href": "blog1/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Writing robust statistical software",
    "text": "Writing robust statistical software\nImplement complext statistical methods such that the software is reliable, and includes appropriate testing to ensure high quality and validity and ultimately credibility of statistical analysis results.\n\nchoose the right method and understand them\nsolve the core implementation problem with prototype code\n\nNeed to try a few different solutions, compare and select the best one. Might also need to involve domain experts.\n\nspend enough time on planning the design of the R package\n\nDon’t write the package right away; instead define the scope, discuss with users, and design the package.\nStart to draw a flow diagram, align names, arguments and classes; write prototype code.\n\nassume the package will evolve over time\n\nPackages you depend on will change; users will require new features\nWrite tests\n\nunit tests\nintegration tests\n\nMake the package extensible\n\nconsider object oriented package designs\ncombine functions in pipelines\n\nKeep it manageable\n\navoid too many arguments\navoid too large functions"
  },
  {
    "objectID": "blog1/technotes_20230228_clinreport_part3/index.html#key-components",
    "href": "blog1/technotes_20230228_clinreport_part3/index.html#key-components",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Key components",
    "text": "Key components\n\nDependency management\nInstall dependencies (system/OS level; R packages)\n\nSet repos (can be specified in options()) to e.g. CRAN, BioConductor\nrenv\ncontainer with dependencies pre-installed\n\n\n\nStatic code analysis\n\nLinting (for programmatic and syntax errors) via lintr package\nCode style enforcement via styler package\nSpell checks identifies misspelled words in vignettes, docs and R code via spelling package\n\n\n\nTesting\n\nR CMD build builds R packages as a installable artifact\nR CMD check runs 20+ checks including unit tests, reports errors, warnigns and notes\nTest coverage reports with covr, checks how many lines of code are covered with tests\nR CMD INSTALL tests R package installation\n\n\n\nDocumentation\nAuto-generated docs via Roxygen and pkgdown\n\n\nRelease and deployments\nRelease artifacts and deployments to target systems\n\nChangelog (features, bug fixes) in the NEWS.md\nRelease: create the package with R CMD build. Validation report with thevalidatoR\nPublishing: CRAN, BioConductor"
  },
  {
    "objectID": "blog1/technotes_20230222_clinreport_part2/index.html",
    "href": "blog1/technotes_20230222_clinreport_part2/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog1/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "href": "blog1/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Agile mindset and DevOps practices",
    "text": "Agile mindset and DevOps practices\n\nData science as a new way of thinking\nNew way of working means\n\nleverage standards and automation (CI/CD)\nadopt new data types quickly, reusing data for multiple purposes, pooling data, data marts\nopen-sourcing and collaborating cross pharma (small, readable, self-tested code)\ncoding for reusability, moving away from single-use programs\nrapidly re-arranginng re-usable components to meet analytical need at hand\n\nData scientist need to have hard skills, such as\n\nSAS, R, Python, JS, bash\ncloud, containers\nCI/CD tools\nvisualisation\nknowledge of various data types\n\nand also soft skills:\n\ncollaborative and inclusive\ntransparent and practical\ncreative and proactive\nasking the right questions\nable to wear many hats, be more flexible and resilient\n\n\n\nAgile\nProject management; a mindset: uncover better ways of working, by doing and helping others do it.\n1st principle: highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nImplementations: Kanban, Scrum, Lean, Extreme programming\nTools:\n\nbacklog\nkanban board (not started, in progress, done)\nWIP (work in progress limit)\nprogress measures: e.g. team velocity\n\n\n\nDevOps\nIncrease efficiency by improving the connection between Dev (software development) and Ops (IT operations).\nThe goal is continuous delivery and continuous improvement.\nPractices:\n\nmodular architecture\nversion control\nmerge into trunk daily\nautomated and continuous testing, continuous integration\nautomated deployment\n\n\nDevOps in clinical reporting\nRisks around production run:\n\nare all dependencies in production?\nwas all quality control completed and successful?\nis all documentation complete?\nwas the transfer to eDMS correct and successful?"
  },
  {
    "objectID": "blog1/technotes_20230222_clinreport_part2/index.html#version-control",
    "href": "blog1/technotes_20230222_clinreport_part2/index.html#version-control",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Version control",
    "text": "Version control\nFeature branch (as opposed to master branch): one task per branch\nname feature branch: issue number and description\nEach issue should have a clear description, short and specific; instead of being long and overarching.\n\nWorkflow for clinical reporting\nRestraints of clinical deliveries: timing annd multiple deliveries; resourcing challenges\nMight need to choose between feature and GitFlow."
  },
  {
    "objectID": "blog1/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "href": "blog1/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Reproducible projects in R",
    "text": "Reproducible projects in R\nTo reproduce your work:\n\nGit (version control)\nR libraries\nWell structured projects\nUnderlying dependencies (e.g. operating systems, C++/C)\n\n\nWell structured projects\nClear names\nGood documentation\n\n\nR libraries and versions\nCheck session info; but not the most practical way.\nUse global libraries, .libPaths(), this gives you the path where all the packages are installed. Global libraries is useful when using a server for multiple R sessions, where they look for the packages in the same place.\nSolutions\n\nrenv package: makes each project in R self-contained.\nCheckpoint: project level library paths based on snapshots of CRAN\n\nUse Docker images! Saves R version, operating system, underlying dependencies"
  },
  {
    "objectID": "blog1/technotes_20230205_clinreport_part1/index.html",
    "href": "blog1/technotes_20230205_clinreport_part1/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog1/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "href": "blog1/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Introduction to clinical trial",
    "text": "Introduction to clinical trial\nClinical trial: aim to demonstrate that drug is safe and effective (safety, efficacy)\n\nphase 1: 10-20 people, focus on safety (healthy volunteers)\nphase 2: 100, study of side effects, determine best dose\nphase 3: 1000, demonstrate drug efficacy, fuller safety profile (common across multiple regions, ethnicities)\n\ncollecting data from different hospitals, hence important to ensure standards are being followed\n\nevidence must be submitted to health authorities (FDA, EMA European medicines agency)\nhealth authorities determine whether the drug is submitted to market\n\nSubmit the analysis plan in advance"
  },
  {
    "objectID": "blog1/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "href": "blog1/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Why share data",
    "text": "Why share data\n\nregulatory requirements\nscientific community interest\ncompany internal research interest\nmarketing materials\n\n\nData and results sharing\n\nRegulatory req (e.g. EMA require sharing clinical trial results to gain marketing authorization for pharma products, FDA require sharing data)\nscientific community (peer review check accuracy, perform additional analyses, derive new hypothesis)\nCDISC standards\n\nCDASH (clinical data acquisition standards harmonization)\nSDTM (study data tabulation model)\n\nformat for ‘raw’ data, define datasets, structures, contents, variable attributes\n\nSEND (standard for exchange of non clinical data)\nADaM (analysis data model)\n\ndata format for data processed for analysis (e.g. converted, imputed, derived)\n\n\nDictionary\n\ne.g. nose congestion, stuffy nose, … need to be standardized\nMedDRA: standard dictionary for medical conditions, events and procedures\nWHO drug dictionary (for pharma agents)\n\nSAP statistical analysis plan\n\nbased on study protocol, focus on statistical methodology, is regulated\n\nProgramming specification\n\nbased on SAP, provides additional details on datasets and tables, listing and figures (TLFs) required for statistical analysis. focus on programming details. Not regulated\n\n\n\n\nQuality assurance\n\nGood clinical practice (GCP), issued by ICH\npurpose: prevent mistakes, reduce inefficiencies/waste in a process, increase reliability/trustworthiness of the product of a process\nClinical monitoring: performed by a clinical research assistant (CRA) at investigator sites, checks that study protocols are executed as intended, and site processes result in accurate data capture. Focus on trial subjects’ safety. Traditional goal: 100% source data verification\nData quality checks (more relevant for data scientists). Checks data for technical conformance, and data plausibility. Focus on data quality. Traditional goal: 100% accurate and format compliant data\nCode review\nDouble programming\n\n\n\nData access restrictions\n\nreasons\n\ndata collected is very sensitive (health data), need data protection\nclinical trial data is a key asset and revenue predictor for pharma companies, high confidentiality levels\nscientific validity, data is ideally double blinded, no-one should know whether a subjecttreatment is as long as the data is still being collected\n\npseudonymization: data de-identification\n\nuse pseudonym (ID), link is recorded to allow re-identification\n\nanonymization: limit the risk of re-identification\n\nremove variables, remove values, replace more precise values with more general categories, replace personal identifiers with random identifiers\n\nFSP, CRO (out-sourcing), personnels require data access at different levels\nUnblinding"
  },
  {
    "objectID": "blog1/readnotes_20240606_bad_pharma/index.html",
    "href": "blog1/readnotes_20240606_bad_pharma/index.html",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog1/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "href": "blog1/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog1/readnotes_2023010x_preventable_sridhar/index.html",
    "href": "blog1/readnotes_2023010x_preventable_sridhar/index.html",
    "title": "Preventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar",
    "section": "",
    "text": "Advice on some measures to prepare for the next pandemic\n(From Five ways to prepare for the next pandemic by Prof. Devi Sridhar)\n\nMonitor zoonoses. Identify patogens with pandemic potential, regulate better wet markets\nSequence globally. Investment in genetic-sequencing capability\nStrengthen manufacturing. Vaccine inequality, fragility of vaccine production. Private and public sector work together - vaccine research, production and distribution.\nVaccine preparedness. For known diseases (e.g. influenza), invest in vaccines that protect against a wide range of variants. New technology and research for unknown threats\nStop the spread (long enough for the vaccines) to save lives."
  },
  {
    "objectID": "blog1/index.html",
    "href": "blog1/index.html",
    "title": "Blogs",
    "section": "",
    "text": "AI Agentic Design Patterns\n\n\n\nAI-Agentic\n\nDesign Patterns\n\n\n\nGuide for orchestrating LLMs, tools and memory into AI agentic systems.\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Scientist Interview Guide\n\n\n\nData science\n\nInterview Guide\n\n\n\nResearch Scientist Interview Guide\n\n\n\n\n\nJul 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to handle class-unbalanced data?\n\n\n\nData science\n\nMachine Learning\n\n\n\nIn class-imbalanced datasets, the majority class dominates while the minority class is underrepresented, leading models to bias their predictions toward the majority class.\n\n\n\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSmall Datasets in Machine Learning\n\n\n\nNotes\n\n\n\nComprehensive Techniques of Machine Learning Models and techniqies tailorec for small datasets\n\n\n\n\n\nJan 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUse Quarto, Make Friends: a two-year journey\n\n\n\nQuarto\n\nWebsite\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre\n\n\n\nNotes\n\n\n\nReading notes on some of the chapters. Book on Amazon: link\n\n\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports with Quarto: R-Ladies Abuja Workshop\n\n\n\nR\n\nQuarto\n\nparameterized reports\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal\n\n\n\nNotes\n\n\n\nReading notes on some of the chapters. Book on Amazon: link\n\n\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports with Quarto: R-Ladies DC Workshop\n\n\n\nR\n\nQuarto\n\nparameterized reports\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStyling your quarto project\n\n\n\nWebsite\n\n\n\nLearning notes on how to customize a quarto project, such as website.\n\n\n\n\n\nOct 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUse WebR in your existing quarto website\n\n\n\nQuarto\n\nWebsite\n\n\n\nYou might get stuck when you try to add the trending webR to quarto extension in your website. This is one way to fix it.\n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Quarto reports improve understanding of soil health\n\n\n\nR\n\nQuarto\n\nparameterized reports\n\nagriculture\n\nsoil health\n\n\n\nCreating custom soil health reports with Quarto\n\n\n\n\n\nSep 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Highlights: Positconf 2023\n\n\n\nData science\n\n\n\nCuration of content to check out when I’ve got time\n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Highlights: CEN2023\n\n\n\nBiostatistics\n\n\n\nSelected summaries on the 5th Conference of the Central European Network of the International Biometric Society (IBS).\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nShiny optimization of climate benefits from a statewide agricultural grant program\n\n\n\nR\n\nshiny\n\nagriculture\n\nclimate\n\n\n\nDevelopment process of {WaCSE} shiny app\n\n\n\n\n\nAug 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\n\nQuarto\n\nEducation\n\n\n\nSome reflection on the experimental 8-day introductory statisics course with new teaching methods. Visit the course website here.\n\n\n\n\n\nJul 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Soil Health Initative and Climate Smart Estimator\n\n\n\nagriculture\n\nsoil health\n\nclimate\n\n\n\nOverview of the WA Soil Health Initative & WA Climate Smart Estimator {WaCSE} shiny app\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR package workflow\n\n\n\nRpkg\n\nRSE\n\n\n\nA step-to-step guide to make CRAN-worthy R packages\n\n\n\n\n\nMay 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping & Mapping {orcas} Encounters\n\n\n\nR\n\nweb scraping\n\nleaflet\n\n\n\nWeb scraping with {rvest} & mapping with {leaflet}\n\n\n\n\n\nApr 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPreventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar\n\n\n\nNotes\n\n\n\nReading notes on the book. Also serves as a collection of notes from Professor Devi Sridhar’s articles.\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCourse review: making DS work for clinical reporting\n\n\n\nReporting\n\nData science\n\nRSE\n\n\n\nA review of the Coursera course provivded by Genentech and Roche, on “Making data science work for clinical reporting”.\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 4\n\n\n\nClinical trial\n\nData science\n\nReporting\n\n\n\nThis is the Part 4 of a four-part course on Coursera.\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 3\n\n\n\nClinical trial\n\nData science\n\nReporting\n\n\n\nThis is the Part 3 of a four-part course on Coursera. In this part, innerSource and OpenSource concepts are introduced, and R package development is discussed.\n\n\n\n\n\nFeb 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Shiny app and deploy to shinyapps.io\n\n\n\nShiny\n\nWebsite\n\n\n\nNotes on how to set up the free shinyapp.io to deploy a demo shiny app.\n\n\n\n\n\nFeb 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 2\n\n\n\nClinical trial\n\nData science\n\nReporting\n\n\n\nThis is the Part 2 of a four-part course on Coursera. In this part, agile and DevOps practices are introduced, along with version control with Git and reproducible R projects.\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR package website with pkgdown\n\n\n\nRpkg\n\nWebsite\n\n\n\nA workflow that worked for me: when you have a few vignette documents, and want to display them nicely in a website format.\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 1\n\n\n\nClinical trial\n\nData science\n\nReporting\n\n\n\nThis is the Part 1 of a four-part course on Coursera. In this part, there is an introduction to clinical trial phases, and motivation to share data. In addition, some terms (such as CDISC standard) have been introduced.\n\n\n\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOpen source reporting with R: clinical, public health, RSE and embrace the change\n\n\n\nReporting\n\nData science\n\nEducation\n\nRSE\n\n\n\nMy thoughts on the open source transition in pharma, public (health) sector and academia. A culture change is needed, and it’s done better at some places than others. As educators and researchers, there are many things that can be done.\n\n\n\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPublishing Quarto Website with GitHub Pages\n\n\n\nQuarto\n\nWebsite\n\n\n\nA workflow that worked for me. This is the third time that I go through the Quarto website publishing with GitHub Pages - even more reason to note it down!\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nqtwAcademic: a quick and easy way to start your Quarto website\n\n\n\nQuarto\n\nWebsite\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWebsite reboot: switching from Blogdown to Quarto\n\n\n\nQuarto\n\nWebsite\n\n\n\nTime to reboot the personal website. Now, with Quarto\n\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Climate Smart Estimator: Using ArcGIS Dashboards and Experience Builder\n\n\n\nArcGIS\n\nagriculture\n\nclimate\n\n\n\nOverview of WA Climate Smart Estimator ArcGIS Experience web app\n\n\n\n\n\nMay 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\nagriculture\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\nagriculture\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog1/blog_20240923_quartofriends/index.html",
    "href": "blog1/blog_20240923_quartofriends/index.html",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "",
    "text": "I wrote a blog back in early 2023 when I first switched from blogdown to Quarto on my initial impression (read here), and this is a two-year follow-up on my journey since I started using Quarto, for my personal website, teaching, scientific works and collaborative community projects."
  },
  {
    "objectID": "blog1/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "href": "blog1/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a teaching tool",
    "text": "Quarto as a teaching tool\n\nFrom personal to workshop website\nI switched from blogdown to Quarto in late 2022, right after my PhD. It was initially a cure for a severe burnout from a combination of work-related stressors, when I desperately needed something other than research. My mental state was like the famous painting by Norwegian artist Edvard Munch:\n\n\n\n\n\nThe experience of the switch was explained in the previously mentioned blog. Briefly, it was light like a feather. Since I was quite satisfied, I thought, why don’t I make a workshop website? So I did.\nThe result was quite good, I made the (as far as I knew) first quarto workshop website at University of Oslo for the Oslo Bioinformatics Workshop Week 2022. Feedback from students were positive, and the instructor team thought it hosts the material in a more organized way.\n\n\nSingle day workshop -&gt; two week course\nI was greatly encouraged by the experience, so when I got a 50% position at University of Oslo as biostatistics lecturer, I thought, why don’t we have the same thing for the course?\n\n\n\n\n\nOh well, the workload is crushing. There were a few key differences:\n\nR scripts and material were unavailable since the course was originally in STATA. Everything need to be done from scratch, for at least 12 lab sessions;\nThe students generally have little IT skills, which means more effort need to be done to guide them through the ‘get started’ part.\n\n\n\n\n\n\nIt took one month to create the first version of the website. More details about the experience can be read here.\n\n\nAdding WebR to the course\nOne year later, as technology advances, we added new content to some parts of the website. Most notably is the interactivity achieved through WebR. For example, I made this page on randomness and statistical distribution where students can interactively modify code chunks in a web browser."
  },
  {
    "objectID": "blog1/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "href": "blog1/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a collaboration tool",
    "text": "Quarto as a collaboration tool\nA static (or even interactive) website is not exactly what you call ‘collaborative tool’. However, if you work as a group towards something cool, Quarto might just be the tool you need. Check out the CAMIS project to find out what I mean by this!"
  },
  {
    "objectID": "blog1/blog_20240923_quartofriends/index.html#what-else",
    "href": "blog1/blog_20240923_quartofriends/index.html#what-else",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "What else?",
    "text": "What else?\nThe associated talk is available on YouTube, check it out!"
  },
  {
    "objectID": "blog1/blog_20230904_cen2023/index.html",
    "href": "blog1/blog_20230904_cen2023/index.html",
    "title": "Personal Highlights: CEN2023",
    "section": "",
    "text": "The IBS (International Biometric Society) conference of the Central European Network, CEN2023 has been a great opportunity to keep myself up to date with the latest development of biostatistics, both in academia and industry. Thanks to the great effort made by the organizing committee and almighty Google Meet/Zoom, I have been able to follow the talks without any issue, and have definitely learned a lot.\nGiven my background, I paid more attention on talks and workshops on\n\nStatistical software, R programming and simulation\nCausal inference\n\nThere were also two topics that drew my attention: one is on statistical education towards medial professionals, the other is on a Data Challenge using RCT data.\n\nStatistical software\n\nSoftware Engineering Working Group (SWE WG), MMRM\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nDaniel Sabanes Bove (Roche). First year of the Software Engineering working group - working together across organizations\nGonzalo Duran-Pacheco (Roche). Comparing R libraries with SAS’s PROC MIXED for the analysis of longitudinal continuous endpoints using MMRM\n\n\n\n\nThe ASA Biopharmaceutical Section (BIOP) Software Engineering Working Group SWE WG was established in 2022. Currently they have 3 work streams:\n\nmmrm implements MMRM (mixed models with repeated measures)\nbrms.mmrm, the Bayesian version of MMRM\nHealth Technology Assessment with R\n\nAt a later talk, mmrm was compared with SAS’s PROC MIXED and R’s nlme, glmmTMB for analyses of longitudinal continuous endpoints. In terms of speed and convergence, mmrm is superior than others; while the estimate prodouced by mmrm is very close to PROC MIXED and glmmTMB.\nThis looks like a very interesting tool to try out! Vignette\n\n\nSimulation tools and RWD\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMichael Kammer (Medical University of Vienna). An overview of R software tools to support simulation studies: towards standardizing coding practices.\n\n\n\n\nKammer and colleagues did a review on R packages for simulation, and selected 14 top simulation packages, including simstudy, simdata, synthpop, bigsimr and others. The full list is made available here.\nA real-world dataset, NHANES was also introduced here. The data can be accessed with R package nhanesA.\n\n\n\nCausal Inference\n\n\n\n\n\n\nInformation\n\n\n\n\n\nWorkshop: Implementing the estimand framework in global drug development: Application of causal inference approaches (Mouna Akacha, Björn Bornkamp, Alex Ocampo, Jiawei Wei at Novartis)\nKeynote: Ruth Keogh (LSHTM). Causal inference with observational data: A survival guide\n\n\n\nThese two workshop / talk cover slightly different scenarios: one in RCT, one for observational data. It deserves a whole article or more to elaborate on this topic, so I’m only putting some resources here.\nCausal inference is definitely gaining traction in recent years in both academia and industry. Techniques such as g-computation, IPW and doubly robust estimation are starting to become mainstream. It is fascinating that these techniques themselves are not bound to a fixed model.\nResources:\n\nWorkshop repository Code\nBook: Causal Inference: What If by Hernán and Robins (2020)\nPrincipal stratum strategy, Bornkamp et al. (2021)\nTime-dependent covariates, Keogh et al. (2023)\nTarget Trial Emulation (TTE), Hernán and Robins (2016)\n\n\n\nCovariate adjustment and data challenge with RCT data\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nKelly Van Lancker (Ghent University). Improving Power in Randomized Trials by Leveraging Baseline Variables\nDominic Magirr (Novartis). Organizing a Data Challenge on Covariate Adjustment in RCTs\nCraig Wang (Novartis). Participating in a Data Challenge on Covariate Adjustment in RCTs\n\nPanel discussion: Jonathan Bartlett (LSHTM)\n\n\n\n23 teams at Novartis participated in a Data Challenge on Covariate Adjustment. They were given a fixed outcome model, and 5 prior studies trial data, and their task was to create the design matrices that improve the precision compared to unadjusted data.\nIf I were to select talks based on the category titles, I would probably missed the whole session. However, it is surprisingly similar to using not trial, but real-world data (such as EHR) to make predictions. The conclusion were similar as well: using “supercovariates” created by ML isn’t gaining much compared to simple models such as ANCOVA. Possible reasons:\n\nsmall to moderate data size\nlinear relationship between covariates and outcome\ngood enough prognostic variables\n\nIt was also mentioned that the winning team did some trick to reduce the variance among the covariates. Would be interesting to read about it.\nSome resources:\n\nLancker et al. The use of covariate adjustment in randomized controlled trials: an overview Paper\nCovariate adjustment tutorial, link\n\n\n\nStatistical education\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMaren Vens et al (University of Lübeck). Biostatistics/Biometrics for physicians – essential or unnecessary? How do practicing physicians and dentists evaluate biostatistics? A cross-sectional survey\n\n\n\n\nStatistical education to students / professionals who are not used to working with data has always been tricky. Students generally think statistics is difficult, and need help from a statistician. However there are only limited number of statisticians. The talk by Vens and colleagues confirms what practicing statisticians know, but can’t do much about: most (87%) physicians and dentists in the survey need a statistician to help with their work.\nHow to improve the statistical competency is an important and relevant topic for discussion, and might require systematic changes in how it is taught. Use of modern technology can help, yet it’s only helpful when students start to not fear, or not find math and technology boring."
  },
  {
    "objectID": "blog1/blog_20230301_ds_clinreport/index.html",
    "href": "blog1/blog_20230301_ds_clinreport/index.html",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera (course link). It is not necessary to have a paid coursera membership to view the course, everyone could access it.\nIt is a 4 part course released one month ago (Jan/Feb 2023), and it seems that a follow-up will be released in the future.\nOverall I think it strikes a good balance between high-level introduction of the good practices, and examples with how they are implemented. Even though the course focuses on clinical reporting in the pharmaceutical industry, the practices are highly relevant in other sectors as well (e.g. public health, academia, other industries that use open-source software).\nSpecific statistical methods, packages are introduced only at a high-level; which means the course is not for learning how to use this or that packages; but good practice guidelines.\nIn my opinion,\n\nit would be useful if the learner has some experience with software development and/or statistics; otherwise learners might not know how to practice them.\nmost of the examples are related to R packages (understandable), so some experience with R package (use or develop) is useful.\nit could be a very good study material for university students in related subjects.\n\n\n\n\nModule 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software\n\n\n\n\nI have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog1/blog_20230301_ds_clinreport/index.html#each-module",
    "href": "blog1/blog_20230301_ds_clinreport/index.html#each-module",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "Module 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software"
  },
  {
    "objectID": "blog1/blog_20230301_ds_clinreport/index.html#highlight",
    "href": "blog1/blog_20230301_ds_clinreport/index.html#highlight",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "I have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog1/blog_20230104_qtwAcademic/index.html",
    "href": "blog1/blog_20230104_qtwAcademic/index.html",
    "title": "qtwAcademic: a quick and easy way to start your Quarto website",
    "section": "",
    "text": "qtwAcademic stands for Quarto Websites for Academics, which provides a few Quarto templates for Quarto website that are commonly used by academics.\nThe templates are designed to make it quick and easy for users with little or no Quarto experience to create a website for their personal portfolio or courses. Each template is fully customizable once the user is more familiar with Quarto.\nRead more about the package here.\nMore details about the package is being written …"
  },
  {
    "objectID": "blog1/2024-02-21_rladies-abuja-quarto-params/index.html",
    "href": "blog1/2024-02-21_rladies-abuja-quarto-params/index.html",
    "title": "Parameterized Reports with Quarto: R-Ladies Abuja Workshop",
    "section": "",
    "text": "Course website  Slides  Code  Video \n\nDetails\n📆 February 21, 2024 // 4:30 pm - 6:30 pm WAT\n🏨 Virtual\n🆓 FREE with registration\n🎥 Recording\n🏡 Workshop website\n🔖 Code\n\n\nAbstract\nTired of manually adjusting Quarto reports for different regions, time periods, or clients? Dreaming of using just one template to generate both interactive HTML and static Word/PDF versions of your reports?\nJoin our workshop to unlock the power of parameterized reporting with Quarto and leave with your own template and examples to modify for your own projects.\nGet a sneak preview of what you’ll learn by checking out the slides from my posit::conf(2023) talk.\nWe welcome everyone! However, if you’re new to Quarto or functional programming with {purrr}, take a look at the pre-work for some background videos/tutorials.\n\n\nSlides\n\n\n\n\nRecording"
  },
  {
    "objectID": "blog1/2023-09-25_posit_parameterized-quarto/index.html",
    "href": "blog1/2023-09-25_posit_parameterized-quarto/index.html",
    "title": "Parameterized Quarto reports improve understanding of soil health",
    "section": "",
    "text": "Slides  Code  Video \n\nDetails\n📆 September 25, 2023 // 5:30 pm - 5:40 pm CDT 🏨 Chicago, IL\n🌠 posit::conf(2023)\n\n\nAbstract\nSoil sampling data are notoriously challenging to tidy and effectively communicate to farmers. We used functional programming with the tidyverse to reproducibly streamline data cleaning and summarization. To improve project outreach, we developed a Quarto project to dynamically create interactive HTML reports and printable PDFs. Custom to every farmer, reports include project goals, measured parameter descriptions, summary statistics, maps, tables, and graphs.\nOur case study presents a workflow for data preparation and parameterized reporting, with best practices for effective data visualization, interpretation, and accessibility.\nSee an example HTML report.\nLearn more about the Washington Soil Health Initiative State of the Soils Assessment.\n\n\nSlides\n\n\n\n\nRecording"
  },
  {
    "objectID": "blog1/2023-06-13_wade_washi-wacse/index.html",
    "href": "blog1/2023-06-13_wade_washi-wacse/index.html",
    "title": "Washington Soil Health Initative and Climate Smart Estimator",
    "section": "",
    "text": "Slides  Video \n\nDetails\n📆 June 13, 2023 // 1:30 pm - 2:20 pm PT\n🏨 Leavenworth, WA\n🌠 Washington Association of District Employees (WADE) conference\n\n\nAbstract\nWashington Soil Health Initiative overview and updates.\nHow to get the most of the Sustainable Farms and Fields Washington Climate Smart Estimator (WaCSE) tool.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording"
  },
  {
    "objectID": "blog1/2022-05-25_wagisa_arcgis-wacse/index.html",
    "href": "blog1/2022-05-25_wagisa_arcgis-wacse/index.html",
    "title": "Washington Climate Smart Estimator: Using ArcGIS Dashboards and Experience Builder",
    "section": "",
    "text": "Slides  Recording \n\nDetails\n📆 May 25, 2022 // 2:30 pm - 3:00 pm PT\n🏨 Leavenworth, WA\n🌠 Washington GIS Association (WAGISA) conference\n\n\nAbstract\nWashington’s Sustainable Farms and Fields (SFF) program provides financial incentives to growers who implement climate-smart practices that sequester soil carbon or reduce greenhouse gas emissions. The climate change mitigation potential of different on-farm practices depends on many site-specific factors such as geography, climate, soil type, and management history.\nTo maximize the climate change mitigation potential of the SFF program, and to optimize the use of every dollar, a decision support tool is required. This presentation demonstrates how two existing spatial datasets (NRCS’ COMET-Planner and WSDA’s Agricultural Land Use) can be integrated into a decision support tool using ArcGIS Dashboards and Experience Builder. This tool, called the Washington Climate Smart Estimator (WaCSE), allows users to compare the climate benefits of different agricultural practices across different counties in Washington.\nBy utilizing the intuitive user interface of ArcGIS Dashboards, WaCSE enables swift, science-based estimates of climate benefits, while remaining accessible for audiences of varied technical backgrounds.\nSee the old app built with ArcGIS.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording"
  },
  {
    "objectID": "blog1/2021-11-09_awra_insecticides_water/index.html",
    "href": "blog1/2021-11-09_awra_insecticides_water/index.html",
    "title": "Aquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters",
    "section": "",
    "text": "Slides\n\nDetails\n📆 November 9, 2021 // 1:30 pm - 1:50 pm PT\n🏨 Virtual\n🌠 American Water Resources Association (AWRA)\n\n\nAbstract\nEcological risk assessments often do not consider potential additive, synergistic, or antagonistic effects from mixtures of chemicals and instead typically base risk on a single chemical. In the last decade, more tools and models have been developed to consider the interactive effects of chemicals within a mixture when conducting risk assessments. Therefore, this study uses actual environmental concentrations measured in 2018 and 2019 from the Washington State Department of Agriculture’s Surface Water Monitoring Program. Aquatic risk from exposure was assessed from chlorpyrifos, diazinon, and malathion (as individual chemicals and as binary and ternary mixtures) using the concentration addition model. These pesticides were selected because they have a common mechanism of toxicity, are frequently detected in surface waters in Washington, and were recently evaluated in a biological opinion by the National Marine Fisheries Service.\nAll detected concentrations of chlorpyrifos and malathion, assessed as individual chemicals, exceeded the predicted no effect concentration, indicating potential for adverse effects on aquatic life. Further, risk quotients for all binary and ternary mixtures were greater than one, also indicative of potential for adverse effects on aquatic life. In all samples containing a mixture, the maximum cumulative ratio suggested that a single insecticide contributed &gt;50% of the overall toxicity of each mixture. Based on the individual and mixture risk quotients, chlorpyrifos and malathion were the primary drivers of the toxicity of each mixture.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead."
  },
  {
    "objectID": "blog1/2021-11-14_setac_insecticides_water/index.html",
    "href": "blog1/2021-11-14_setac_insecticides_water/index.html",
    "title": "Aquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters",
    "section": "",
    "text": "Slides\n\nDetails\n📆 November 14, 2021 // 12-minute recording\n🏨 Virtual\n🌠 Society of Environmental Toxicology and Chemistry (SETAC) North America 42nd Annual Meeting\n\n\nAbstract\nEcological risk assessments often do not consider potential additive, synergistic, or antagonistic effects from mixtures of chemicals and instead typically base risk on a single chemical. In the last decade, more tools and models have been developed to consider the interactive effects of chemicals within a mixture when conducting risk assessments. Therefore, this study uses actual environmental concentrations measured in 2018 and 2019 from the Washington State Department of Agriculture’s Surface Water Monitoring Program. Aquatic risk from exposure was assessed from chlorpyrifos, diazinon, and malathion (as individual chemicals and as binary and ternary mixtures) using the concentration addition model. These pesticides were selected because they have a common mechanism of toxicity, are frequently detected in surface waters in Washington, and were recently evaluated in a biological opinion by the National Marine Fisheries Service.\nAll detected concentrations of chlorpyrifos and malathion, assessed as individual chemicals, exceeded the predicted no effect concentration, indicating potential for adverse effects on aquatic life. Further, risk quotients for all binary and ternary mixtures were greater than one, also indicative of potential for adverse effects on aquatic life. In all samples containing a mixture, the maximum cumulative ratio suggested that a single insecticide contributed &gt;50% of the overall toxicity of each mixture. Based on the individual and mixture risk quotients, chlorpyrifos and malathion were the primary drivers of the toxicity of each mixture.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead."
  },
  {
    "objectID": "blog1/2023-04-20_rladies_orcas-web-scraping/index.html",
    "href": "blog1/2023-04-20_rladies_orcas-web-scraping/index.html",
    "title": "Web Scraping & Mapping {orcas} Encounters",
    "section": "",
    "text": "Slides  Code \n\nDetails\n📆 April 20, 2023 // 15-minute talk\n🏨 Seattle, WA\n🆓 R-Ladies Seattle and Seattle useR Group\n\n\nAbstract\nR-Ladies Seattle invited me to give a talk for the ‘R in the Outdoors’ meetup. This was my first in-person talk of my professional career! I used this as an opportunity to learn new skills through a personal project. I’ve always had an affinity for the Southern Resident Killer Whales in the Salish Sea. The Center for Whale Research does a lot of really fascinating and important work monitoring their population. They post their survey data on their website; each encounter with the orcas is a separate webpage. Lately, I’ve been curious and intimidated by web scraping so I decided this would make a great case study and personal project.\nI ended up also going to the Seattle useR Group lightning talks meetup afterwards and spontaneously gave the same presentation there!\n\n\nSlides"
  },
  {
    "objectID": "blog1/2023-08-19_cascadia_shiny-wacse/index.html",
    "href": "blog1/2023-08-19_cascadia_shiny-wacse/index.html",
    "title": "Shiny optimization of climate benefits from a statewide agricultural grant program",
    "section": "",
    "text": "Slides  Code  Video \n\nDetails\n📆 August 19, 2023 // 2:05 pm - 2:20 pm PT\n🏨 Seattle, WA\n🌠 Cascadia R Conf\n\n\nAbstract\nWashington’s Sustainable Farms and Fields program provides grants to growers to increase soil carbon or reduce greenhouse gas (GHG) emissions on their farms. To optimize the climate benefits of the program, we developed the Washington Climate Smart Estimator {WaCSE} using R and Shiny.\nIntegrating national climate models and datasets, this intuitive, regionally specific user interface allows farmers and policymakers to compare the climate benefits of different agricultural practices across Washington’s diverse counties and farm sizes. Users can explore GHG estimates in interactive tables and plots, download results in spreadsheets and figures, and generate PDF reports. In this talk, we present the development process of {WaCSE} and discuss the lessons we learned from creating our first ever Shiny app.\n\n\nSlides\n\n\n\n\nRecording"
  },
  {
    "objectID": "blog1/2024-01-18_rladies-dc_quarto-params/index.html",
    "href": "blog1/2024-01-18_rladies-dc_quarto-params/index.html",
    "title": "Parameterized Reports with Quarto: R-Ladies DC Workshop",
    "section": "",
    "text": "Course website  Slides  Code  Video \n\nDetails\n📆 January 18, 2024 // 6:30 pm - 8:30 pm EDT\n🏨 Virtual\n🆓 FREE with registration\n🏡 Workshop website\n🔖 Code\n\n\nAbstract\nTired of manually adjusting Quarto reports for different regions, time periods, or clients? Dreaming of using just one template to generate both interactive HTML and static Word/PDF versions of your reports?\nJoin our workshop to unlock the power of parameterized reporting with Quarto and leave with your own template and examples to modify for your own projects.\nGet a sneak preview of what you’ll learn by checking out the slides for my posit::conf(2023) talk.\nEveryone is welcome to attend. If you’re new to Quarto, we recommend watching Tom Mock’s excellent 2-hour introduction to Quarto.\n\n\nSlides\n\n\n\n\nRecording"
  },
  {
    "objectID": "blog1/blog_20230103_blogdown2quarto/index.html",
    "href": "blog1/blog_20230103_blogdown2quarto/index.html",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog1/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "href": "blog1/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog1/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "href": "blog1/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "Time to try Quarto",
    "text": "Time to try Quarto\nNow that I’ve finally completed the more pressing tasks in October 2022, I can catch up to the cool kids on twitter: create a website with Quarto!\nThere were quite a lot of discussions about Quarto in the summer 2022. I wasn’t following the discussions closely, but I remember there were quite a few talks in the Rstudio conference this year. Then more and more people switched to Quarto on Twitter. Then people I know also switched to Quarto. What’s the fuzz about?\nMy experience with Quarto is focused on websites. I have not tried other forms of publishing. So far I have created:\n\na workshop website for my colleagues\na personal website (the one you are reading right now)\nan R package (qtwAcademic)that wraps three Quarto website templates for beginners\n\nHere are a few things I like about Quarto. Given that I’m not very experienced in front-end development, these comments are going to be about ease-of-use and design, rather than the technicalities.\n\nClean look for both personal and workshop/courses\nWhen I was using “academic” template in blogdown, I liked the structure of the site: projects, talks, blog, softwares and publications sections are clearly displayed at the top. What I didn’t like is that the default homepage was a very long single page; yet its customisation wasn’t the easist. Other templates were either too simple (for blog only), or more suitable for image display (photography projects). I wanted a website that keep the good structure of “academic”, which is quite suitable for academics (hence the name); while keeping each section independent.\nWith distill I could achieve the structure I wanted; but I didn’t enjoy it too much as a personal website (at least it wasn’t as flexible as Quarto). distill is still pretty decent for organisations or documentation site.\nWith Quarto, I can achieve the desired looks for not only a personal website (with or without blogs), but also a workshop, event or even course website. This is fantastic! The top, sidebar or hybrid navigation makes the site structure very clear, especially when there are lots of content. As an aspiring lecturer at university, this is really One Quarto Rules Them All.\n\n\nFlexible yet not overwhelming\nAs I mentioned above, hacking “academic” in blogdown was not that easy - simply because there were too many folders that you are not actually supposed to modify. It was confusing to know what to change in order to achieve the desired output, and multiple folders were having the same names, making it very challenging for beginners. Ironically, this is usually the first template beginners start with!\nThat’s why I immediately fell for Quarto: you only need 4 components to make a decent minimalistic website work:\n\n_quarto.yml to control the overall layout\nindex.qmd at the root folder to control the homepage\nabout.qmd for some basic information about the creator or the website\nproject.qmd for projects or any other content that the creator wants to display\n\nThe way that _quarto.yml clearly specifies the .qmd files really helps beginners to understand where things are. This has been extremely useful for me when I wanted to learn how people made their website by reading the source code - I could understand exactly where to find the information I needed. The clear structure greatly helps the creators themselves, and also those who want to learn.\n\n\nGreat community\nRstats people have a great community. I wouldn’t be able to make my site the way I wanted if people haven’t been sharing their works. I have learned a lot by reading the source code by Dr Emi Tanaka, Dr David Schoch, Bea Milz, Prof Mine Cetinnkaya-Rundel’s STA 210 - Regression Analysis course.\nI also made my own R package that wraps three templates to create Quarto websites that are frequently used by academics, qtwAcademic. In the following days I plan to write up more detailed explanations on how to use the package, along with some new features."
  },
  {
    "objectID": "blog1/blog_20230112_roche_opensource/index.html",
    "href": "blog1/blog_20230112_roche_opensource/index.html",
    "title": "Open source reporting with R: clinical, public health, RSE and embrace the change",
    "section": "",
    "text": "Two days ago (Jan 11 2023) I watched a presentation by data scientists at Roche about why they are making their clinical trials in 2023 open source with R. As someone who uses R for most of the time and has done similar works (not in pharma, but in public health surveillance and reporting: watch my talk, Code to find out what we do), I watched the presentation with great interest. Here are my notes, combined with some thoughts on open-source in the industry, public sector and academia.\n\nThree reasons for why I am writing this blog\n\nNote down some of the technology which points towards the future of the field\nRelate to my experience of open-source applied in public health, specifically public health reporting\nShare some thoughts in statistical education of applied students/researchers (e.g. medicine), and training Research Software Engineers\n\n\n\nMy experience with statistical software\nTo put my opinions in perspective,\n\nI do not have experience with SAS or pharma, so I do not have first-hand knowledge on the functionality, ease-of-use or the popularity of commercial softwares in the industry.\nI did my MSc and PhD in statistics/biostatistics/medical informatics and R had always been a default choice.\nI worked in public health for a few years, where Excel is possibly the most common tool, and STATA and R are scarcely used (statisticians, epidemiologists, bioinformaticians).\nIn the past few years, my university has made the switch from SPSS to STATA for intro statistics for medical students (while students at higher level, or doing advanced analyses might use R/python), and a test-run with R might be in motion.\n\n\n\n\nClinical Reporting\nIn drug development at pharmaceutical companies (and/or research institutes and hospitals), these data related tasks are very common:\n\nsummarise safety and efficacy data\nprovide accurate picture of trial outcomes\nmanage data collection across different sites\n\nCompleting these tasks in a correct, efficient and reproducible manner is crucial for patient safety. However, these tasks are also highly resource intensive: highly trained scientist, statisticians and technincians must be involved in the process. Historically, pharma use commercial software such as SAS.\n\nRegulation and exploration needs\nThere are requirements for clinical reporting: both regulartory and exploratory. From the regulatory side, there exist industry standards (CDISC) in the clinical research process, such as SDTM (Model for Tabulation of Study Data) and ADaM (Analysis Data Model). Statistical analyses, tables, listings and graphs (TLGs) also fall into this cateogory.\nFrom the exploratory side, clinical data are highly context dependent, and new formats of data such as imaging are more and more used in prediction modeling and drug development.\nIn addition, it is not hard to imagine that the technical competency of employees differ, especially in large organizations. Enabling people with less experience to analyse trial data in a reproducible manner is helpful for not only the learning and growth of employees, but also the productivity of organizations.\nThe existing commercial tools are not able to adapt to the rapid changes in the field.\n\n\nTransition into Open-Source\nIn this talk, Dr Kieran Martin at Roche introduced that they started using R as their core data science tool, aiming to move their codebase to having a core R. In the future, they plan to have something that is lanugage agnostic: meaning that python, Stan, C++, Julia and beyond can be used for different tasks.\nI only noted down a few of the things they mentioned on the infrastructure side:\n\nOCEAN - a lanugage agnostic computing platform on AWS (docker)\nGit, Gitlab for version control and collaboration\nRstudio connect server\nSnakemake for orchestrate production\n\n\n\nR and Shiny\nThere are obvious benefits of using R. It is convenient to install and use (if you used python and R, you’d probably agree), and the latest development in Shiny made it very easy to develop interactive visualizations, suitable for exploration. Package development is critical for reproducibility and distributing works - which R does it very well. A few packages developed by pharma are Code and admiral: the ADaM in R, which I intend to check out at one point.\nR has deep roots in academia which means the newest statistical methods are well covered; which also affects the skill sets that talents own - fresh graduates probably already learned it at university. R being open source means that collaboration with external partners is much more efficient, and transparent. Strong community support is another positive thing that encourages beginners to enter the field and learn.\n\n\n\n\nOpen Sourcing Public Health\n\nSurveillance and reporting\nOne key functionality of public health (PH) authorities is stay informed and inform. They collect data from labs, hospitals and clinics across the country, summarize into useful statistics in tables and graphs, make reports, then inform the policy makers to make decisions (such as vaccination campaigns).\nCompared to clinical reporting (in my understanding), there are many similarities - we make TLG (tables, listings and graphs). There are also features that make reporting in public health unique:\n\nPH surveillance and reporting are dynamic and real-time, which can change in a matter of days. That is because the situation of different infectious diseases can evolve rapidly, so PH authorities need to make appropriate adjustments.\nTime and location (spatial-temporal) are important. Different time granularity (daily, weekly) and geographical units (nation, county, municipality, city districts) are typically required for reporting.\n\n\n\nScale up and automate with open source tools\nTraditionally, these reports are made manually - one location, one graph per time on a certain disease. When a global pandemic hits, this is definitely not fast enough. At my team (Sykdomspulsen team at the Norwegian Institute of Public Health), we tried a different approach. Details of what we did can be found in this talk(Code), but to make it brief:\n\nWe developed a fully automated pipeline that connects 15 registries (vaccination, lab, hospital and intensive care and many more). The data is gathered, censored, cleaned and pre-processed for down-stream analysis\nStatistical analysis, tables, graphs and maps are made for all locations in Norway for various outcomes of interest, such as Covid, influenza, respiratory and gastrointestinal infections\nOver 1000 customized reports with over 30 graphs and tables are produced daily and sent to local PH officials, where we also had a shiny website (Kommunehelsetjenester for Kommunelege) for over 300 PH officials to get most up-to-date information about their own municipality\n\nBy automation, every year Sykdomspulsen can save 700 000 NOK (roughly 70k USD) while making 400 times more real-time reports for public health. Even better, with reproducibility and quality control.\n\n\nToolbox\nSykdomspulsen is a small team (8 people, 3 are statisticians and 1 engineer), and our infrastructure was built upon R packages, which we call splverse. Our infrastructure is not fundamentally different from the one Roche introduced, basically:\n\nR does the task planning and project organization. On top of this, the data cleaning, statistical analysis are implemented. Graphs, tables and maps are made with appropriate R packages\nRmarkdown does automated reporting into .docx and .xlsx. Some reports are also in .html tables to be embedded into customized emails\nRstudio Workbench and GitHub help with teamwork\n\nDocker, GoCD and Airflow do the CI/CD and orchestration\n\n\n\n\nEmbrace the transition\n\nCulture change needed\nUnfortunately, not all organizations are eager to abandon the old way. Even at our own institute where researchers are the majority, open source and modern day programming is hardly practiced (by my observation). Even worse, under the budget cuts in 2023-24, a large number of younger employees who have the technical skills have left - which left the public health surveillance even more vulnerable now that Covid is far from over.\nIn my opinion, public health needs open-source and good programming even more than pharmaceutical companies. Both save lifes - and PH has less money to invest in softwares, infrastructures and talents. In this situation, resources should be spent in fields that are critical and most cost-effective; yet in reality this is often not the case.\nThe slow culture change at big organizations can happen, but only if there is a sufficient amount of employees who are willing to embrace the new technology. In the talk by Roche they about about their training strategy. It is not possible to train all users, and not everyone has the same needs at the same time. Therefore, self study with certain study paths is encouraged and supported.\n\n\nTeach programming to students in various fields\nBased on my experience in the UK and Norway, students (myself included) learn R programming in one of the two ways\n\nLearning by Googling (self-taught): a university degree needs to use it: provides a short introduction, then students learn by using. This is how I learned R at my MSc Statistics degree, and this is probably the most common way\nWorkshops at university: organizations such as the Carpentries provide course material and teaching a few times per year, where interested students (usually from subjects such as biology) come and learn. These classes are quite popular, and usually have a long waiting list.\n\nFrom learning by googling to some organized teaching - that is already some good progress. However, if not, can we improve?\nIn my experience with statistical advising with the university hospital, clinical researchers and medical students are enthusastic to get their statistics done, some are also eager to do some analysis themselves. That is good. Yet, there is generally lack of capacity - either knowledge or software skills. Once the statistician who helps with the project stops, the project ends. There is the need to have in-house statistical capacity. To this end, open-source softwares such as R, and good programming practice (reproducibility for example) can help a lot: the license doesn’t end, and everything is documented so that the next person can continue the work.\nI’m glad that my university has made some transitional efforts in this regard: STATA instead of SPSS is being taught to medical students as part of their statistics course. There might be a test-run in R soon, which is very exciting (since I’ll be involved in the teaching)!\n\n\nStatistical engineering and RSEs\nThat was the capacity building to get beginners more independent. On the other side, there is also the need for better programming practice for researchers at more advanced level. Research Software Engineering (RSE) is starting to get more and more attention, because it is not only relevant for research (i.e. getting papers published), but in broader applications.\nFor example, in the talk by Roche, they mentioned that “RSE teams need to accelerate adoption of new statistical methods and biomarker data analysis”, and the implementation with R packages and templates is at its core. In the future more languages would be included such as Python, Stan, C++ and Julia.\nHowever, RSE as a job title or career path is still a new thing. I know two RSEs at my university, and RSE is definitely not your typical academic faculty position: only departments that think it’s important makes positions, often not permanent. To get any new methods actually used in either industry or the public sector outside research, translating methods into tools is must-do. In the future I hope RSE becomes a stable and common career path, and more exciting things can happen."
  },
  {
    "objectID": "blog1/blog_20230717_teaching/index.html",
    "href": "blog1/blog_20230717_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Earlier this year (2023) I wrote a blog about my thoughts on the role of open source software in statisical education. Naturally, I advocate for more use of open source tools such as R/python in teaching introductory statistics to applied scientists. Nonetheless, how the material is taught will make a huge difference in the understanding and interest in the material.\nI was taught statistics in the classic way: lectures with tons of mathematical formulae and proofs, while programming and data analyses were left for students themselves to figure out. Those who were the fastest learners were the ones who already had a degree in computer science, which probably doesn’t sound surprising. I, for one, definitely struggled."
  },
  {
    "objectID": "blog1/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "href": "blog1/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Does statistics have to be daunting?",
    "text": "Does statistics have to be daunting?\nFor applied scientists in various fields, data analysis is a core task, and also a challenging one. You must have met clinicians or biologists who would love their data to be analysed yet don’t know how to. Yes, statistics and data skills can take some time to learn; but with the right method, they don’t have to be daunting. It is up to the educator to find a way that benefits the most students. An observation is that many researchers do not know or remember advanced math; yet do they need advanced math to grasp many fundamental statistical concepts?\nI believe that it is far more important and useful to teach basic IT skills and exploratory data analysis so that students can develop an understanding of their own data; rather than using a test blindly."
  },
  {
    "objectID": "blog1/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "href": "blog1/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Rebooting MF9130E classroom",
    "text": "Rebooting MF9130E classroom\nWhen I heard that the teaching team at Biostatistics Department, Faculty of Medicine was thinking about trying a novel pedagogical method on the MF9130E (2023 spring) class, I was more than excited to contribute. This is a PhD level course of 8 days long, offered three times a year (twice in Norwegian language). Students come from a variey of backgrounds in health and life sciences. Since this is an introductory course, the topics are broad rather than specialised.\nA few years ago, statistical software for the course made the transition from SPSS to Stata. To be more precise, students were introduced to, but not really explained to, or elaborated on how to use Stata proficiently. Why? The course is about statistics so only statistics is taught. Data skills such as manipulation are not part of statistics. \nWell, we will change that by starting to use R.\n\nThree open source musketeers\nR, quarto and GitHub the three musketeers in facilitating the transformation. We build a quarto course website where all the material are public, hosted with GitHub Pages. Having a course website is beneficial for students to have an overview of the course, in contrast to many scattered lecture notes and exercises to be downloaded.\nThe biggest advantage of using quarto is the rendered output from code. From a student’s perspective, it is reassuring to see the same result and plots using the data and code provided by the instructor. For the instructor, it is also convenient to see whether the code functions as expected. When we do not want to show the output, it is also very easy to suppress. We have created one copy with and one withtout rendered output as exercises, and are glad to see some students challenging themselves by attempting to solve the problems without solution.\nUsing Github and quarto together to build a course website is rather straightforward. I think the site structure is simple yet flexible enough to navigate. Collaboration across a small teaching team is also manageable. Github Pages was easy to set up, and changes made on the main branch is deployed within the minute. This proved to be useful in quite a few moments (where we had to replace some datasets or add some notice).\n\n\nThe Carpentries pedagogical model\nThe Carpentries is an organisation that teaches foundational coding and data science skills to researchers. I myself benefited from their workshop on version control and git taught at University of Oslo, and I think the traditional classroom could use some of the methods at these data science workshops.\nTo put simply, there are two things I tried with the course setup for MF9130E:\n\nLive coding demonstration, plenty of it\nSticky-notes flag and helper (teaching assistant) in class\n\nIn the live coding demonstration (which I was responsible for), I made sure that students were taught the most commonly used R commands for data manipulation and exploration. Quarto webpages on introduction to R, basic EDA, intermediate EDA have been created and guided through in class, mixed with statistical concepts and visualizations. Without knowing how your data looks like, blindly using statistical tests is dangerous - that is the motivation for doing so.\nWhether students feel supported can make a huge difference in their willingness to learn. Taking it slow at the beginning, and solve the problems on an individual basis can prevent early drop-outs, especially when programming and IT systems are involved. Naturally, when we don’t have helpers we can not help everyone; this is a limitation for this model. Students should be encouraged to help each other.\n\n\nLet them explore\nThe last important change in the class was to give time to students themselves. We reduced the lecturing on theory and computation, and added time for practice and discussion. The guided practice with live demo also came with solution and comments, so students could explore at their own pace. We left plenty of time for them to ask questions, and made sure most people can follow the exercises."
  },
  {
    "objectID": "blog1/blog_20230717_teaching/index.html#how-did-it-go",
    "href": "blog1/blog_20230717_teaching/index.html#how-did-it-go",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "How did it go?",
    "text": "How did it go?\nAfter the 8 day course we carried out a small survey among the ~50 students in the spring 2023 class. Student backgrounds are diverse, they work on lab data, clinical data or observational/epidemiological data:\n\nobservational study on humans 36%\nRCT 18%\nin vitro research 15%\nothers are in animal research, meta analysis or something else\n\nStatistical competency (method, software) among students are generally on the basic end. Over 75% of the cohort report themselves to have basic to very basic knowledge of statistics; 33% do not use any statistical software, around 45% have used SPSS or Stata. On the other hand, some students (7%) report to have advanced knowledge and have some R experience.\n\nSome feedback\nThis is the first time we do the course with R, live demo and put an emphasis on basic data manipulation and exploration - which means we do not have enough data, it is just an initial impression.\nHere’s what we have received. On the positive side, 86% find the course useful for their own PhD research. 75% felt they are able to use the correct methods for their analyses, which is quite encouraging. Most felt the examples and exercises were able to demonstrate the theory. Students have generally positive experience with the live demo, and find the instructors supportive. This is good!\nIn the meantime, it is only natural that some are dissatisfied (21%) in some ways. Common complaints are: R is not user friendly to absolute beginners; the leap from no software to a programming language is too big for some.\nAs for whether students have really mastered the knowledge intended, we do not have enough data to draw a conclusion. We do observe that the take home project show somewhat better understanding, but can not say for sure just yet.\nThis is a class with very diverse backgrounds, hence it is challenging to cater to everyone’s needs. Yet, we are satisfied with the trial-transformation with our introductory statistics class, and we plan to gradually implement more classes with R, and possibly hands-on practice (depending on capacity)."
  },
  {
    "objectID": "blog1/blog_20230921_positconf2023/index.html",
    "href": "blog1/blog_20230921_positconf2023/index.html",
    "title": "Personal Highlights: Positconf 2023",
    "section": "",
    "text": "The yearly party of Positconf (formerly Rstudio conf) has come to an end. I joined the virtual experience at home, it is of course not the same as attending in-person, yet the atmosphere in discord was still great!\nIt’s hard to choose which talks to watch since multiple were scheduled at the same time, so one has to prioritize. I definitely will re-visit some of the talks at a later point, so this blog acts as a placeholder for links so that I can find them in the future."
  },
  {
    "objectID": "blog1/blog_20230921_positconf2023/index.html#make-interactive-things",
    "href": "blog1/blog_20230921_positconf2023/index.html#make-interactive-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make interactive things",
    "text": "Make interactive things\nWebDev is definitely a big thing at this year’s positconf. If I’m learning one thing from the conference, I’d check out webR.\nI still remember when R was mainly for statistical analysis and computing back when I learned it. Now it’s become much more fun! Strictly speaking, webRand quarto are not R per se. However, they’ve become the gateway drugs for R programmers to dabble in WebDev. With web assembly (wasm), now one can execute R code in a browser and even run shiny app.\nUnlock the power of dataViz animation and interactivity in quarto by Deepsha Menghani used a super fun example (F-bomb) to demonstrate how to add interactivity to your barplot (or other plots) with Crosstalk. Check out the talk Code. The presentation was as interactive as the quarto slides, good job Deepsha!\nRunning shiny without a server by Joe Cheng (Code): this was a big announcement. I used shiny at work, but for my own projects or smaller teaching projects I tried to stay away from shiny - I was concerned about the fee. This looks like a promising thing to try out once it’s stable, although I’d probably do webR first."
  },
  {
    "objectID": "blog1/blog_20230921_positconf2023/index.html#make-pretty-things",
    "href": "blog1/blog_20230921_positconf2023/index.html#make-pretty-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make pretty things",
    "text": "Make pretty things\nIt is fascinating to see so many organizations and individual R developers make their own themes for better branding, recognition and storytelling. More and more peple have realized that making beautiful plots is important, and totally possible as well. Work on layout, color, font and sizes!\n\nThemes\nAdding a touch of glitr: Developing a package of themes on top of ggplot by Aaron Chafetz, Karishma Srikanth and colleagues at USAID. Code\n\n\nTables\nMaking tables with gt has been on my to-do list for a while now. It is very inspiring to see so many cool tables that makes you wonder, “is it really JUST a table?” For example, check out this gallery by Posit community.\nThe book Creating beautiful tables in R with gt by Albert Rapp would be a good place to learn how to make nice tables. Actually the reason why I wanted to use gt is that it seems to be the mainsteam in clinical reporting in pharma. I bumped into this blog post some time ago, and this would be my starting point.\n\n\nQuarto\nIf you want to go one step further and start making your quarto project pretty, there are a few things to try out.\nAlbert Rapp in his talk HTML and CSS for R Users stated that quarto is a gateway drug to WebDev. It reminds me of my very first presentation at my local R users community (2019) was about building a website with blogdown, and when I really spent a lot of time to make my markdown documentation colorful with span style - and that was about everything I knew.\nNow I want more. Learning HTML and CSS can make your dataviz, tables, slides and dashboards look not only professional but also special. I’m going to check out the scss variables in quarto which defines the theme, theme_file.scss. Emil Hvitfeldt (Styling and templating quarto documents) showed us how to make really pretty and animated (!) quarto sldies themes, and shared this template with us, quarto-revealjs-earth. I really like how revealjs slides look like, just that the MacOS Keynote (or MS ppt) drag-and-drop seems more flexible to me (?) Guess it’s something I should get used to over time.\nRichard Iannone (Extending quarto) introduced quarto shortcode extensions to add a bunch of fancy-looking icons to quarto files. To create extensions in general: https://quarto.org/docs/extensions/creating. This is for more pro-users since you needs to learn lua."
  },
  {
    "objectID": "blog1/blog_20230921_positconf2023/index.html#quarto-updates",
    "href": "blog1/blog_20230921_positconf2023/index.html#quarto-updates",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Quarto updates",
    "text": "Quarto updates\nQuarto is definitely one of the most discussed topics in the year 2022-2023 in the R community. For good reasons. I need to catch up the the latest developments annd use-cases:\n\nWhat’s new in quarto? by Charlotte Wickham\nReproducible manuscripts with Quarto by Mine Çetinkaya-Rundel\nParametrized quarto reports improves understanding of soil health by Jadey Ryan\n\nand so many more. I couldn’t follow all the talks and I’m sure there are lots of great examples of how quarto is better than traditional ways of reporting."
  },
  {
    "objectID": "blog1/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "href": "blog1/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "title": "Personal Highlights: Positconf 2023",
    "section": "A few other things to check out",
    "text": "A few other things to check out\nBeyond the web and quarto topics, I think there are some existing and new tools that can be useful for my work. For example,\n\nI should review Hadley and Jenny’s R package book (2e).\nthis package targets for pipeline automation and management look like something that can be used for my analysis\n…\n\nIt will take a while to digest the latest developments. But little by little, we’ll get there! People in the R community are doing great things."
  },
  {
    "objectID": "blog1/blog_20241105_positconf2024/index.html",
    "href": "blog1/blog_20241105_positconf2024/index.html",
    "title": "AI Agentic Design Patterns",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog1/blog_20241105_positconf2024/index.html#quarto",
    "href": "blog1/blog_20241105_positconf2024/index.html#quarto",
    "title": "AI Agentic Design Patterns",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog1/blog_20241105_positconf2024/index.html#python",
    "href": "blog1/blog_20241105_positconf2024/index.html#python",
    "title": "AI Agentic Design Patterns",
    "section": "Python",
    "text": "Python\nEmily Riederer: Python Rgonomics\nPython alternatives to R. Worth rewatching!"
  },
  {
    "objectID": "blog1/blog_20241105_positconf2024/index.html#teaching-and-education",
    "href": "blog1/blog_20241105_positconf2024/index.html#teaching-and-education",
    "title": "AI Agentic Design Patterns",
    "section": "Teaching and education",
    "text": "Teaching and education\nAndrew Gard: Teaching and learning data science in the era of AI\nStudents don’t know enough to be able to edit the prompt to reach a sensible code chunk, AI guessed and guessed wrong. We should not expect AI to guess information that we do not provide!\nStudents should still learn to code, and teachers should ask better questions - instead of asking for the final result (create a bar plot), ask students to critically think: why doesn’t the AI-generated code work? what information is missing? how do you improve the prompt?\nJames Wade: Posit Academy in the Age of Generative AI - Lessons from the Frontlines\nchattr, gptstudio, github copilot\nPosit Academy learners (over half) give AI code assistants 2 star rating or less\nRewarding, high-growth period. Threshold concepts: once understood, transforms your perception and approach of a discipline, and these must be encountered not told.\nTC in DS:\n\ntidy data enables efficient analysis\nmodular code enhances re-usablity and clarity\nvisualization as a tool for exploration and communication\n\nHow to incorporate AI code assistants (in DS class)\n\nearly stage: explain this code piece by piece\nmid stage: add a roxygen skeleton to my code\nlate stage: try code assistants in the IDE\n\nTC for code assistants:\n\ndrive faster but don’t forget to steer\nprompting matters, learning how to use these tools is a skill"
  },
  {
    "objectID": "blog1/blog_20241105_positconf2024/index.html#statistics",
    "href": "blog1/blog_20241105_positconf2024/index.html#statistics",
    "title": "AI Agentic Design Patterns",
    "section": "Statistics",
    "text": "Statistics\nHannah Frick: tidymodels for time-to-event data\nMax Kuhn: Evaluating time-to-event models is hard\nDemetri Pananos - Making sense of marginal effects"
  },
  {
    "objectID": "blog1/readnotes_2023010x_pandemic_gates/index.html",
    "href": "blog1/readnotes_2023010x_pandemic_gates/index.html",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing\n\n\n\n\nThere should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients.\n\n\n\n\n\nPassive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy.\n\n\n\n\nThe effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor\n\n\n\n\nInfodemic\n(these two chapters are highly technical, and they deserve a separate note)\n\n\n\nDisaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting\n\n\n\n\nThe impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal.\n\n\n\n\n\nInvest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog1/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "href": "blog1/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing"
  },
  {
    "objectID": "blog1/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "href": "blog1/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "There should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients."
  },
  {
    "objectID": "blog1/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "href": "blog1/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Passive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy."
  },
  {
    "objectID": "blog1/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "href": "blog1/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "The effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor"
  },
  {
    "objectID": "blog1/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "href": "blog1/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Infodemic\n(these two chapters are highly technical, and they deserve a separate note)"
  },
  {
    "objectID": "blog1/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "href": "blog1/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Disaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting"
  },
  {
    "objectID": "blog1/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "href": "blog1/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "The impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal."
  },
  {
    "objectID": "blog1/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "href": "blog1/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "title": "Small Datasets in Machine Learning",
    "section": "",
    "text": "Invest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog1/readnotes_20240218_open_source/index.html",
    "href": "blog1/readnotes_20240218_open_source/index.html",
    "title": "Working in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal",
    "section": "",
    "text": "Github as a platform\n\nOn contribution\nNearly half of all contributors only contributed once; which accounted for less than 2% of total commits.\nThe pattern that one or a few developers do most of the work, followed by many casual contributors and even more passive users is the norm, not exception in open source.\nOn casual contributors: they primarily see themselves as users of the project, rather than a part of a contributor community.\nChallenge for maintainers: not how to get more contributors, but how to manage high volume of frequent, low-touch interactions (directing air traffic)\n\nGithub’s open source developers have more in common with solo creators on Twitter, Instagram, YouTube or Twitch.\n\nComparing early internet and social platform nowadays: the early online communities have mailing lists, online forums, membership groups, operated like villages that have their own culture, history and norms. Nowadays creators have much bigger potential audience but the relationship is one-sided, and can be overwhelming.\n\n\nOn free software and hacker\n“Free” means you are able to do what you want with the software, rather than the cost. Libre rather than gratis. At least at the beginning.\nBravado, showmanship, mischievousness, deep mistrust of authority. This culture in the 1980s and 90s was closely linked to the early open source software.\n“Bazaar”: highly participatory, versus “Cathedral”: restricted to a smaller group\nToday’s developer hardly even notice “open source” as a concept anymore, they just want to write and publish their code. They prioritize convenience over freedom or openness.\n\n\nOn licensing\nThe widespread use of permissive licensing is popularized by GH.\nCopyleft licensing (e.g. GNU General Public License GPL) is not commercial friendly as it requires companies to license their software that depend on open source GPL software to have the same license. However GPL gives developers more control over how others use their code in the long run.\n\nAs with any other online content today, sharing is the default.\n\n\n\n\nThe structure of an open source software\n\nOn how projects evolve\nCreate -&gt; Promote and distribute -&gt; Grow\nProjects are promoted like a founder would promote a startup: share on the relevant channels online, give talks at conference and meetups, encourage others to write and talk about it\nA sign that the software is used widely: when the maintainer starts doing more non-code (triage issues, review pull requests) rather than code work.\n\n\nContributor and users\nDepends on technical scope (whether there is much to do), support required (code and admin work), ease of participation (whether on Github) and user adoption (potential contributor base).\nFour types of projects\n\nhigh user growth, high contributor growth: federations. Rare, impactful, the ‘ideal’ of open source project. Roughly 3% of open source projects. Examples: Rust, Node.js, Linux\nhigh user growth, low contributor growth: stadiums: powered by one or a few developers. Centralized.\nlow user growth, high contributor growth: club. Similar to meetup or hobby groups, do not have a wide reach but are loved and built by enthusiasts.\nlow user growth, low contributor growth: toys. Personal project, isn’t trying to grow its user base. Projects on Github with less than 10 stars. Authors do not expect to receive contributions nor do they care about whether people are watching.\n\nDecentralized communities (clubs and federations) have the potential for high user growth - recruit new contributors, reduce contribution friction.\nCentralized communities (stadium) depends on the creators to manage user demand - automation, elimination of noise\n\n\n\nRoles, incentives and relationships\n\nFirms or communities\nFirms (companies, organizations): centralized resources; from a coordination standpoint, managing resources would be more efficient within the same organization - which does not explain why open source developers make software together without formal contracts and financial compensation.\n\n\nThe commons and peer production\nTragedy of the commons: resources depleted by people acting in their own self-interest rather than in the collective interest.\n(One of the 8 design principles by Ostrom on) successful commons:\n\nThose who are affecteed by the rules can participate in modifying them.\n\nStrong sense of group identity maks rules, dispute resolution more meaningful.\nCoordination cost is lower when self-organized based on who wants to do the work most, anyone can do the advertised work and volunteer.\nIn contrast, in companies - solicit, evaluate, hire, manage employees; only employees can do the work limited by their job functions.\nPeople collaborating online for no obvious reason beyond personal satisfaction (intrinsic motivation)\nModular and granular tasks: how tasks are organized, and how big each task is.\nLow coordination costs: quality control over thee modules, integrate the contributions into the finished product\n\n\nContribution beyond code\nSome users do not consider them a contributor, but do actually contribute by education, spreading the word, support (forum), bug reports and more.\nThese active users are similar to contributors but operate independently from project’s contributor community."
  },
  {
    "objectID": "blog1/technotes_20230111_deployqt/index.html",
    "href": "blog1/technotes_20230111_deployqt/index.html",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog1/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "href": "blog1/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog1/technotes_20230111_deployqt/index.html#create-quarto-project",
    "href": "blog1/technotes_20230111_deployqt/index.html#create-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "2. Create Quarto project",
    "text": "2. Create Quarto project\nThis can be a website, a book (a specific type of website) or something else.\nTest compilation by quarto render, or click the Render button."
  },
  {
    "objectID": "blog1/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "href": "blog1/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "3. Configure Quarto project",
    "text": "3. Configure Quarto project\nIn _quarto.yml, change the project configuration to use docs as the output-dir:\nproject:\n  type: website\n  output-dir: docs\n\n\n\n\n\nThen add .nojekyll to the root of the repository. Can do this by (in terminal)\ntouch .nojekyll\nPush everything to your repository."
  },
  {
    "objectID": "blog1/technotes_20230111_deployqt/index.html#configure-github-pages",
    "href": "blog1/technotes_20230111_deployqt/index.html#configure-github-pages",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "4. Configure GitHub Pages",
    "text": "4. Configure GitHub Pages\nGo to Settings &gt; Pages, publish from docs of the main branch.\n\n\n\n\n\nCan check GitHub Action and deployment status.\n\n\n\n\n\n\n\n\n\n\nAfter the deployment is successful, go to view deployment, and a successful website should be published."
  },
  {
    "objectID": "blog1/technotes_20230220_pkgdown/index.html",
    "href": "blog1/technotes_20230220_pkgdown/index.html",
    "title": "R package website with pkgdown",
    "section": "",
    "text": "1. Create the website skeleton.\nBefore editing the details, we need to create the skeleton for the website. It can be done with usethis and pkgdown packages.\nIn R, run this:\nusethis::use_pkgdown()\nThis creates the _pkgdown.yml file, which is the place you configure your site.\nTo view the initial package website, use the following command:\npkgdown::build_site()\nThis creates docs/ directory containing a website\n\nREADME.md becomes the homepage,\ndocumentation in man/ generates a function reference,\nvignettes are rendered into articles/.\n\n\n\n2. Edit the vignette documentation\nMake sure that the vignette index is consistent with Title, otherwise it will not render.\n\n\n3. Build and preview your site\nNow check if the site looks good, and contents are correctly positioned.\npkgdown::preview_site()\npkgdown::build_site()\nYou can also do this to build the site.\npkgdown::build_site_github_pages()\n\n\n4. Deploy site with GitHub Pages\nThere seems to be two options:\n\nusethis::use_pkgdown_github_pages(), this function should take care of everything after pushing changes to GH.\nif you used pkgdown::build_site_github_pages() and pushed everything to GitHub, it might not automatically deploy your site to GH pages. I tried to go to Settings -&gt; Pages -&gt; Deploy from a branch -&gt; main -&gt; /docs, this makes Action deploy your site from the docs folder.\n\ndouble check if you have .nojekyll file\nif a website does not show, check whether you have docs in the .gitignore file; since you are deploying from that folder."
  },
  {
    "objectID": "blog1/technotes_20230225_shinyappsio/index.html",
    "href": "blog1/technotes_20230225_shinyappsio/index.html",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "",
    "text": "Useful references:"
  },
  {
    "objectID": "blog1/technotes_20230225_shinyappsio/index.html#considerations",
    "href": "blog1/technotes_20230225_shinyappsio/index.html#considerations",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Considerations",
    "text": "Considerations\nA few ways to do it: Shiny Server (free), shinyapps.io (free and premium), and professional Rstudio Connect (paid).\nI choose to test out the second option, since it allows more possibilities compared to the free open-source Shiny Server.\nThe free option should allow me to create 5 apps, which is more than enough for personal use. It also allows 25 active hours per month; a note on that at the end."
  },
  {
    "objectID": "blog1/technotes_20230225_shinyappsio/index.html#configuration",
    "href": "blog1/technotes_20230225_shinyappsio/index.html#configuration",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Configuration",
    "text": "Configuration\nSign up with GitHub account; or something else. It is possible to change account name afterwards.\nIn Rstudio,\n\nfirst install.packages('rsconnect')\nthen, configure the account. It can be done with rsconnect::setAccountInfo() with information provided in your own shinyapps.io page.\n\nBefore the last step, it is necessary to have an app to deploy!"
  },
  {
    "objectID": "blog1/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "href": "blog1/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Create my first shiny project",
    "text": "Create my first shiny project\nHere I use my usual workflow of creating a new R project:\n\nCreate a new repo on GitHub;\nClone the repo locally, by opening a new R project with version control.\n\nNow copy the two R scripts from the demo example:\n\nserver.R\nui.R\n\nTest locally by running shiny::runApp(). This should render the app."
  },
  {
    "objectID": "blog1/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "href": "blog1/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Deploy to shinyapps.io",
    "text": "Deploy to shinyapps.io\nrsconnect::deployApp() will deploy the app, with an automatically generated url that links to your account.\nThe demo app is deployed here.\n\nNote on active hours\nAfter deployment, the site seems to be active until you shut it down manually; or timeout. The default timeout is 15 minutes, which can be reduced to 5 minutes.\n25 hours per month suggests that I can open the site for 300 times (without manually shuting it down). It might be necessary to start using the paid options, if I have more than one site, or multiple users want to access it …"
  },
  {
    "objectID": "blog1/technotes_20230301_clinreport_part4/index.html",
    "href": "blog1/technotes_20230301_clinreport_part4/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog1/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "href": "blog1/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Open source packages",
    "text": "Open source packages\nExmample:\n\nsurvival: 8 developers, &gt;18 years\nadmiral: 25 developers, &gt;1 year\ntern: 77 developers, 5 years\nrtables: 21 developers, 4 years\n\nEngagement across these packages is different, some receive more issues and comments, some receive more code contributions.\nStale: stable? abandoned?\nContribution is highly skewed, a few contributors write the majority of the code.\nR package life cycles (indicative, not guaranteed)\n\nexperimental (ready to use?)\nstable (safe to use?)\ndeprecated, no longer maintained\nsuperseded, something better exists\n&lt;1.0: big changes likely; &gt;=v1.0: is it safe to use?"
  },
  {
    "objectID": "blog1/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "href": "blog1/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Risk mitigation for R packages",
    "text": "Risk mitigation for R packages\nCombine external and internal packages (CI/CD release)\n-&gt; automated package data collection\n-&gt; automated quality checks: if not pass, assess\n-&gt; package repo integration tests\n-&gt; publish to package repo, generate package validation report"
  },
  {
    "objectID": "blog1/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "href": "blog1/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Assess external packages for statistical methods",
    "text": "Assess external packages for statistical methods\nDoes it provide the required functionality?\n\nCorrect statistical method?\nCould you extend it?\nCorrect results? (compare with another software)\nDo you understand the method? (check the paper linked with package)\n\nDoes it work reliably?\n\nPublished? (e.g. on CRAN)\nDifferent inputs?\nFast?\nDo other people use it? (downloads)\nDoes other software use it? (reverse dependencies)\n\nDoes the code look robust and well tested?\n\nHow are the functions implemented\nIs the source code readable\nCoverage with unit tests\nMature package?\n\nIs it well documented?\n\nDocumented functions?\nVignettes?\nPublished?\nInformative NEWS entry?\n\nWho are the authors, are they responsive?\n\nDid they publish statistics papers on this topic\nIs a github site with issues available"
  },
  {
    "objectID": "blog1/technotes_20230301_clinreport_part4/index.html#tools",
    "href": "blog1/technotes_20230301_clinreport_part4/index.html#tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Tools",
    "text": "Tools\ncovr and unit tests\nriskmetric and the R Validation Hub\npharmaverse.org, with end-to-end examples"
  },
  {
    "objectID": "blog1/technotes_20231001_qt_webr/index.html",
    "href": "blog1/technotes_20231001_qt_webr/index.html",
    "title": "Use WebR in your existing quarto website",
    "section": "",
    "text": "WebR is the new hot topic in the R community. Coupled with Quarto, you can run R code interactively in a web browser. This is achieved with the great quarto extension, Code developed by James J Balamuta.\nIn the Paper, Code and YouTube, James introduced how to make a webR empowered quarto document. It is simple enough, and you can make it work quite smoothly."
  },
  {
    "objectID": "blog1/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "href": "blog1/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "title": "Use WebR in your existing quarto website",
    "section": "When your render gets stuck",
    "text": "When your render gets stuck\nBut there is a twist. This works perfectly fine with a new quarto project, where no output-dir is specified yet. When I tried to replicate the same thing for my existing quarto website (with output-dir: docs so that I could deploy it with GitHub Pages), my rendered html file got stuck:\n\nIf you read the troubleshooting documentation, you’ll see that it’s a problem with the two js files. This agrees with what Rstudio Background Jobs tells us.\n\nI moved the two files (manually..) around, then render again, nothing changed.\n\nSolution: set channel-type option\nThis is a solution provided by the authors, although I don’t quite understand what it did, but it did the magic. (Thanks to Linh’s help!)\nThis is where you specify this option.\n\nRender again, now it works! WebR status turns green, and I can run code interactively in the browser."
  },
  {
    "objectID": "blog1/technotes_20240506_ohdsi_part1/index.html",
    "href": "blog1/technotes_20240506_ohdsi_part1/index.html",
    "title": "How to handle class-unbalanced data?",
    "section": "",
    "text": "There are three systematic different ways to handle class imbalanced data."
  },
  {
    "objectID": "blog1/technotes_20240506_ohdsi_part1/index.html#characterization",
    "href": "blog1/technotes_20240506_ohdsi_part1/index.html#characterization",
    "title": "How to handle class-unbalanced data?",
    "section": "Characterization",
    "text": "Characterization\n\nWhat happened to the patients.\n\nChapter 11 Characterization\nTypical characterization questions:\n\nHow many patients…?\nHow often does…? What proportion of patients …?\nWhat is the distribution of values for …?\nWhat is the median length of exposure for patients on …?\nOther drugs the patient is using?\n\nDesired output:\n\ncount, percentage\naverages and other descriptive statistics\nprevalence, incidence rate\nrule-based phenotype\ndrug utilization, adherence, treatment pathways, line of therapy\ndisease natural history, co-morbidity profile"
  },
  {
    "objectID": "blog1/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "href": "blog1/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "title": "How to handle class-unbalanced data?",
    "section": "Population-level estimation",
    "text": "Population-level estimation\n\nWhat are the causal effects\n\nChapter 12 Population-level Estimation\nTypical questions:\n\nWhat is the effect of …?\nWhich treatment works better?\nWhat is the risk of X on Y?\nWhat is the time-to-event of …?\n\nDesired output:\n\nRR, HR, OR\nAssociation, correlation\nATE, causal effect"
  },
  {
    "objectID": "blog1/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "href": "blog1/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "title": "How to handle class-unbalanced data?",
    "section": "Patient-level prediction",
    "text": "Patient-level prediction\n\nWhat will happen to A?\n\nChapter 13 Patient-level Prediction\nTypical questions:\n\nWhat is the chance that this patient will…?\nWho are the candidate for…?\n\nDesired output:\n\nprobability for an individual\nprediction model\nhigh/low risk groups\nprobabilistic phenotype\n\nhttps://imbalanced-learn.org/stable/user_guide.html#user-guide\nhttps://imbalanced-learn.org/stable/over_sampling.html"
  },
  {
    "objectID": "blog1/technotes_20240506_ohdsi_part1/index.html#selected-previous-talks",
    "href": "blog1/technotes_20240506_ohdsi_part1/index.html#selected-previous-talks",
    "title": "How to handle class-unbalanced data?",
    "section": "Selected previous talks",
    "text": "Selected previous talks\nEvaluation-Level Approaches\nAvoid accuracy as the metric (it will be misleading).\nPrefer metrics that account for imbalance:\nPrecision, Recall, F1-score\nROC-AUC, PR-AUC (especially useful with rare positives)\nMatthews Correlation Coefficient (MCC)\nBalanced Accuracy\n\nIf I face class imbalance, I’d start by understanding how severe it is. At the data level, I could resample — oversample the minority with SMOTE or undersample the majority. At the algorithm level, I’d use class weights or imbalance-aware ensembles like Balanced Random Forest. I’d also focus on the right metrics — like precision, recall, F1, or PR-AUC — since accuracy can be misleading. In deep learning, I might use focal loss. For example, in a fraud detection project, I combined SMOTE with class-weighted XGBoost, which improved recall significantly while controlling false positives.\n\nThe Book of OHDSI written by the OHDSI community.\nWhat is required to go from origin (source data) to destination (evidence):\n\nunderstanding of health informatics, patient and provider interaction through administrative and clinical systems into final depository\nappreciation of the biases that can arise in the data curation processes\nmastery of epidemiological principles and statistical methods to translate clinical questions into an observational study design properly\ntechnical ability to implement and execute computationally-efficient data science algorithms\nclinical knowledge to synthesize knowledge learned, and determine how the new knowledge should impact health policy and clinical practice.\n\nOMOP: Observational Medical Outcomes Partnership, aims to identify true drug safety association.\nOMOP CDM: common data model, a mechanism to standardize the structure, content and semantics to make it possible to write statistical code that can be reused at every data site.\nOHDSI community (2014) has created libraries of open-source analytics tools atop OMOP CDM to support:\n\nclinical characterisation for disease natural history, treatment utilisation, quality improvement\npopulation level effect estimation to apply causal inference for medical product safety surveillance and comparative effectiveness\npatient level prediction to apply machine learning for precision medicine and disease interception"
  },
  {
    "objectID": "blog1/technotes_20240506_ohdsi_part1/index.html#data-level-approaches",
    "href": "blog1/technotes_20240506_ohdsi_part1/index.html#data-level-approaches",
    "title": "How to handle class-unbalanced data?",
    "section": "Data-Level Approaches",
    "text": "Data-Level Approaches\nOversampling the minority class\nRandom Oversampling: duplicate minority class examples until balance is reached.\nSMOTE (Synthetic Minority Over-sampling Technique): generates synthetic data points for the minority class by interpolating between nearest neighbors.\nADASYN: similar to SMOTE but focuses on generating harder-to-learn examples.\nUndersampling the majority class\nRandom Undersampling: randomly remove majority class examples.\nCluster Centroids / Tomek Links / NearMiss: more informed undersampling to preserve useful structure.\nHybrid methods\nCombine oversampling and undersampling to avoid overfitting or losing too much information."
  },
  {
    "objectID": "blog1/technotes_20240506_ohdsi_part1/index.html#algorithm-level-approaches",
    "href": "blog1/technotes_20240506_ohdsi_part1/index.html#algorithm-level-approaches",
    "title": "How to handle class-unbalanced data?",
    "section": "Algorithm-Level Approaches",
    "text": "Algorithm-Level Approaches\nClass Weights / Cost-Sensitive Learning\nAssign higher misclassification cost to minority class (many libraries like scikit-learn allow class_weight=‘balanced’).\nAnomaly Detection / One-Class Models\nTreat minority class as “rare events” and use anomaly detection approaches.\nEnsemble Techniques\nUse Bagging/Boosting with imbalance-aware modifications (e.g., Balanced Random Forest, EasyEnsemble, RUSBoost)."
  },
  {
    "objectID": "blog1/technotes_20240506_ohdsi_part1/index.html#evaluation-level-approaches",
    "href": "blog1/technotes_20240506_ohdsi_part1/index.html#evaluation-level-approaches",
    "title": "How to handle class-unbalanced data?",
    "section": "Evaluation-Level Approaches",
    "text": "Evaluation-Level Approaches\n\nAvoid accuracy as the metric (it will be misleading).\nPrefer metrics that account for imbalance:\n\nPrecision, Recall, F1-score\nROC-AUC, PR-AUC (especially useful with rare positives)\nMatthews Correlation Coefficient (MCC)\nBalanced Accuracy\n\nIf I face class imbalance, I’d start by understanding how severe it is. At the data level, I could resample — oversample the minority with SMOTE or undersample the majority. At the algorithm level, I’d use class weights or imbalance-aware ensembles like Balanced Random Forest. I’d also focus on the right metrics — like precision, recall, F1, or PR-AUC — since accuracy can be misleading. In deep learning, I might use focal loss. For example, in a fraud detection project, I combined SMOTE with class-weighted XGBoost, which improved recall significantly while controlling false positives.\n\nThe Book of OHDSI written by the OHDSI community.\nWhat is required to go from origin (source data) to destination (evidence):\n\nunderstanding of health informatics, patient and provider interaction through administrative and clinical systems into final depository\nappreciation of the biases that can arise in the data curation processes\nmastery of epidemiological principles and statistical methods to translate clinical questions into an observational study design properly\ntechnical ability to implement and execute computationally-efficient data science algorithms\nclinical knowledge to synthesize knowledge learned, and determine how the new knowledge should impact health policy and clinical practice.\n\nOMOP: Observational Medical Outcomes Partnership, aims to identify true drug safety association.\nOMOP CDM: common data model, a mechanism to standardize the structure, content and semantics to make it possible to write statistical code that can be reused at every data site.\nOHDSI community (2014) has created libraries of open-source analytics tools atop OMOP CDM to support:\n\nclinical characterisation for disease natural history, treatment utilisation, quality improvement\npopulation level effect estimation to apply causal inference for medical product safety surveillance and comparative effectiveness\npatient level prediction to apply machine learning for precision medicine and disease interception"
  },
  {
    "objectID": "cv.html#skills-1",
    "href": "cv.html#skills-1",
    "title": "Curriculum Vitae",
    "section": " Skills",
    "text": "Skills\n\n\n\nProgramming\n\n\nPython, R, Java, C++, SAS, MATLAB, SQL, HTML/CSS, JavaScript (Node.js, React)\n\n\nMachine/Deep Learning\n\n\nscikit-learn, TensorFlow, PyTorch, pandas, Matplotlib, Seaborn, Gym, RLlib\n\n\nLLMs & NLP\n\n\nHugging Face Transformers, LangChain, RAG, Prompt Engineering\n\n\nHPC & Big Data\n\n\nHadoop, Hive, Spark, Kafka, Kinesis, SLURM, MPI, OpenMP\n\n\nSimulation & Modeling\n\n\nOPAL-RT, OpenDSS (Power), CARLA (Autonomous Driving)\n\n\nOptimization\n\n\nGurobi, Pyomo, BoTorch, Optuna, Hyperopt\n\n\nVisualization & GIS\n\n\nTableau, ArcGIS, Leaflet\n\n\nCloud & DevOps\n\n\nAWS (EC2, S3, Lambda), GCP, Docker, Kubernetes, Git, Terraform, Jenkins, CircleCI"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "A Quarto Page Layout Example",
    "section": "",
    "text": "This document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte’s styles in RMarkdown documents (Xie, Allaire, and Grolemund 2018). The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte’s style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. “Tufte Handouts.” In, 137–46. Chapman; Hall/CRC. https://doi.org/10.1201/9781138359444-6.\n1 To learn more, you can read more about Quarto or visit Code.---\ntitle: \"An Example Using the Tufte Style\"\nauthor: \"John Smith\"\nformat:\n  html:\n    grid:\n1      margin-width: 350px\n  pdf: default\n2reference-location: margin\ncitation-location: margin\n---\n\n1\n\nIncreases the width of the margin to make more room for sidenotes and margin figures (HTML only).\n\n2\n\nPlaces footnotes and cited sources in the margin. Other layout options (for example placing a figure in the margin) will be set per element in examples below.\n\n\nThese layout features are designed with two important goals in mind:\n\nTo produce both PDF and HTML output with similar styles from the same Quarto document;\nTo provide simple syntax to write elements of the Tufte style such as side notes and margin figures. If you’d like a figure placed in the margin, just set the option fig-column: margin for your code chunk, and we will take care of the details for you2.\n\n2 You never need to think about \\begin{marginfigure} or &lt;span class=\"marginfigure\"&gt;; the LaTeX and HTML code under the hood may be complicated, but you never need to learn or write such code.If you have any feature requests or find bugs in these capabilities, please do not hesitate to file them to Code."
  },
  {
    "objectID": "test.html#margin-figures",
    "href": "test.html#margin-figures",
    "title": "A Quarto Page Layout Example",
    "section": "Margin Figures",
    "text": "Margin Figures\nImages and graphics play an integral role in Tufte’s work. To place figures in the margin you can use the Quarto chunk option column: margin. For example:\n\n```{r}\n#| label: fig-margin\n#| fig-cap: \"MPG vs horsepower, colored by transmission.\"\n#| column: margin\n#| message: false\nlibrary(ggplot2)\n```\n\nWarning: package 'ggplot2' was built under R version 4.5.1\n\n```{r}\n#| label: fig-margin\n#| fig-cap: \"MPG vs horsepower, colored by transmission.\"\n#| column: margin\n#| message: false\nmtcars2 &lt;- mtcars\nmtcars2$am &lt;- factor(\n  mtcars$am, labels = c('automatic', 'manual')\n)\nggplot(mtcars2, aes(hp, mpg, color = am)) +\n  geom_point() + geom_smooth() +\n  theme(legend.position = 'bottom')\n```\n\n\n\n\n\n\n\n\nFigure 1: MPG vs horsepower, colored by transmission.\n\n\n\n\nNote the use of the fig-cap chunk option to provide a figure caption. You can adjust the proportions of figures using the fig-width and fig-height chunk options. These are specified in inches, and will be automatically scaled down to fit within the handout margin."
  },
  {
    "objectID": "test.html#arbitrary-margin-content",
    "href": "test.html#arbitrary-margin-content",
    "title": "A Quarto Page Layout Example",
    "section": "Arbitrary Margin Content",
    "text": "Arbitrary Margin Content\nYou can include anything in the margin by places the class .column-margin on the element. See an example on the right about the first fundamental theorem of calculus.\n\n\nWe know from the first fundamental theorem of calculus that for \\(x\\) in \\([a, b]\\):\n\\[\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).\\]"
  },
  {
    "objectID": "test.html#full-width-figures",
    "href": "test.html#full-width-figures",
    "title": "A Quarto Page Layout Example",
    "section": "Full Width Figures",
    "text": "Full Width Figures\nYou can arrange for figures to span across the entire page by using the chunk option fig-column: page-right.\n\n```{r}\n#| label: fig-fullwidth\n#| fig-cap: \"A full width figure.\"\n#| fig-width: 11\n#| fig-height: 3\n#| fig-column: page-right\n#| warning: false\nggplot(diamonds, aes(carat, price)) + geom_smooth() +\n  facet_grid(~ cut)\n```\n\n\n\n\n\n\n\nFigure 2: A full width figure.\n\n\n\n\n\nOther chunk options related to figures can still be used, such as fig-width, fig-cap, and so on. For full width figures, usually fig-width is large and fig-height is small. In the above example, the plot size is \\(11 \\times 3\\)."
  },
  {
    "objectID": "test.html#arbitrary-full-width-content",
    "href": "test.html#arbitrary-full-width-content",
    "title": "A Quarto Page Layout Example",
    "section": "Arbitrary Full Width Content",
    "text": "Arbitrary Full Width Content\nAny content can span to the full width of the page, simply place the element in a div and add the class column-page-right. For example, the following code will display its contents as full width.\n::: {.fullwidth}\nAny _full width_ content here.\n:::\nBelow is an example:\n\nR is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "test.html#main-column-figures",
    "href": "test.html#main-column-figures",
    "title": "A Quarto Page Layout Example",
    "section": "Main Column Figures",
    "text": "Main Column Figures\nBesides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output.\n\n```{r}\n#| label: fig-main\n#| fig-cap: \"A figure in the main column.\"\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n```\n\n\n\n\n\n\n\nFigure 3: A figure in the main column."
  },
  {
    "objectID": "test.html#margin-captions",
    "href": "test.html#margin-captions",
    "title": "A Quarto Page Layout Example",
    "section": "Margin Captions",
    "text": "Margin Captions\nWhen you include a figure constrained to the main column, you can choose to place the figure’s caption in the margin by using the cap-location chunk option. For example:\n\n```{r}\n#| label: fig-main-margin-cap\n#| fig-cap: \"A figure with a longer caption. The figure appears in the main column, but the caption is placed in the margin. Captions can even contain elements like a citation such as @xie2018.\"\n#| cap-location: margin\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n```\n\n\n\n\n\n\n\nFigure 4: A figure with a longer caption. The figure appears in the main column, but the caption is placed in the margin. Captions can even contain elements like a citation such as Xie, Allaire, and Grolemund (2018).\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. “Tufte Handouts.” In, 137–46. Chapman; Hall/CRC. https://doi.org/10.1201/9781138359444-6."
  },
  {
    "objectID": "llms.html",
    "href": "llms.html",
    "title": "LLMs",
    "section": "",
    "text": "Doing real-world projects is, I think, the best way to learn and also to engage the world and find out what the world is all about.\n\n-Ray Kurzweil\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaboration and Competition\n\n\nTrain a pair of agents to play tennis.\n\n\n\n\n\n\n\n\n\n\n\n\nAI-Powered Patient Education System\n\n\nDevelop an AI agent to enhance patient education by delivering personalized, on-demand health information through summaries, comprehension checks, and quizzes about relevant…\n\n\n\n\n\n\n\n\n\n\n\n\nChatbot Design using Retrieval‑Augmented Generation (RAG)\n\n\nBuilt a domain‑specific chatbot integrating vector‑based retrieval with GPT models to provide accurate, context‑aware responses\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Agent Travel Assistant System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\n\n\n\n\n\nCongressional Policy Analysis using ML and HPCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt Injection Attacks on LLM Medical Diagnosis by Symptom Elaboration\n\n\n\nR package\n\n\n\nAttacker is to trick the LLM to generate inappropriate possible medical diagnosis which could mislead the end use\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Of Ventilated and Control Flies\n\n\n\nR package\n\n\n\nHypothesis Analysis of Life Expectancy of Flies in Normal vs ill Ventilated Bottles\n\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification of Foods Based on their Quality\n\n\n\nArcGIS\n\nPython\n\n\n\nML model to assess the quality of fruit from an data set, which could be integrated into a product for use in home kitchens\n\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nP2P File Sharing Protocol\n\n\n\nShiny app\n\n\n\nBuild a peer-to-peer file sharing protocol that keeps track of which peers are sharing and what files are being shared in the network\n\n\n\nJan 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotion Prediction and Detection for Autonomous Vehicles\n\n\n\nAutonomous Systems\n\nDeep Learning\n\nComputer Vision\n\n\n\nDevelop a framework for vehicle detection and motion planning of vehicles in complex driving scenarios\n\n\n\nDec 7, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "robo.html",
    "href": "robo.html",
    "title": "Robotics",
    "section": "",
    "text": "Doing real-world projects is, I think, the best way to learn and also to engage the world and find out what the world is all about.\n\n-Ray Kurzweil\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaboration and Competition\n\n\nTrain a pair of agents to play tennis.\n\n\n\n\n\n\n\n\n\n\n\n\nAI-Powered Patient Education System\n\n\nDevelop an AI agent to enhance patient education by delivering personalized, on-demand health information through summaries, comprehension checks, and quizzes about relevant…\n\n\n\n\n\n\n\n\n\n\n\n\nChatbot Design using Retrieval‑Augmented Generation (RAG)\n\n\nBuilt a domain‑specific chatbot integrating vector‑based retrieval with GPT models to provide accurate, context‑aware responses\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Agent Travel Assistant System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\n\n\n\n\n\nCongressional Policy Analysis using ML and HPCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt Injection Attacks on LLM Medical Diagnosis by Symptom Elaboration\n\n\n\nR package\n\n\n\nAttacker is to trick the LLM to generate inappropriate possible medical diagnosis which could mislead the end use\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Of Ventilated and Control Flies\n\n\n\nR package\n\n\n\nHypothesis Analysis of Life Expectancy of Flies in Normal vs ill Ventilated Bottles\n\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Of Ventilated and Control Flies\n\n\n\nR package\n\n\n\nHypothesis Analysis of Life Expectancy of Flies in Normal vs ill Ventilated Bottles\n\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Of Ventilated and Control Flies\n\n\n\nR package\n\n\n\nHypothesis Analysis of Life Expectancy of Flies in Normal vs ill Ventilated Bottles\n\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Of Ventilated and Control Flies\n\n\n\nR package\n\n\n\nHypothesis Analysis of Life Expectancy of Flies in Normal vs ill Ventilated Bottles\n\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification of Foods Based on their Quality\n\n\n\nArcGIS\n\nPython\n\n\n\nML model to assess the quality of fruit from an data set, which could be integrated into a product for use in home kitchens\n\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nP2P File Sharing Protocol\n\n\n\nShiny app\n\n\n\nBuild a peer-to-peer file sharing protocol that keeps track of which peers are sharing and what files are being shared in the network\n\n\n\nJan 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotion Prediction and Detection for Autonomous Vehicles\n\n\n\nAutonomous Systems\n\nDeep Learning\n\nComputer Vision\n\n\n\nDevelop a framework for vehicle detection and motion planning of vehicles in complex driving scenarios\n\n\n\nDec 7, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "robo/wacse.html",
    "href": "robo/wacse.html",
    "title": "P2P File Sharing Protocol",
    "section": "",
    "text": "App  Code \nThe Washington State Department of Agriculture developed WaCSE for the Washington State Conservation Commission to use in the Sustainable Farms and Fields (SFF) program. Intended users are the Conservation Commission, conservation districts, growers, and anyone interested in reducing agricultural greenhouse gas (GHG) emissions. This interactive tool estimates the reduction of GHG emissions from different conservation practices across Washington’s diverse counties."
  },
  {
    "objectID": "robo/roadmap.html",
    "href": "robo/roadmap.html",
    "title": "Classification of Foods Based on their Quality",
    "section": "",
    "text": "App  Code \nThe Soil Health Roadmap is a science-based guide to maintaining and improving soil health in eight focus areas across Washington state.\nFor each focus area, I created crop and soil maps using the arcpy Python package. I then summarized the crop acreage and soil properties in interactive ArcGIS dashboards to complement the Paper."
  },
  {
    "objectID": "robo/nor_mortality/index.html",
    "href": "robo/nor_mortality/index.html",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "robo/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "robo/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "robo/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "robo/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "robo/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "robo/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "robo/llm_nor_mortality/index.html",
    "href": "robo/llm_nor_mortality/index.html",
    "title": "Multi-Agent Travel Assistant System",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "robo/llm_nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "robo/llm_nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Multi-Agent Travel Assistant System",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "robo/llm_nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "robo/llm_nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Multi-Agent Travel Assistant System",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "robo/llm_nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "robo/llm_nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Multi-Agent Travel Assistant System",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "robo/ehr-title/index.html",
    "href": "robo/ehr-title/index.html",
    "title": "AI-Powered Patient Education System",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr"
  },
  {
    "objectID": "llms/washi.html",
    "href": "llms/washi.html",
    "title": "Motion Prediction and Detection for Autonomous Vehicles",
    "section": "",
    "text": "Google Colab  YOLOv5 Code  PyTorch Hub \nInspired by other open-source deep learning projects such as YOLO, ResNet, and Code, this project demonstrates object detection and motion prediction for autonomous vehicles using the Kaggle Lyft motion prediction dataset. We use YOLOv5 for real-time object detection and ResNet-50 for motion trajectory prediction."
  },
  {
    "objectID": "llms/robo/washi.html",
    "href": "llms/robo/washi.html",
    "title": "Survival Of Ventilated and Control Flies",
    "section": "",
    "text": "{pkgdown} site  Code  CRAN \nInspired by other branding R packages such as Code, Code, and Code, washi provides color palettes and themes consistent with Washington Soil Health Initiative (WaSHI) branding. This package is to be used only by direct collaborators within WaSHI, though you are welcome to adapt the package to suit your own organization’s branding."
  },
  {
    "objectID": "llms/orcas.html",
    "href": "llms/orcas.html",
    "title": "Prompt Injection Attacks on LLM Medical Diagnosis by Symptom Elaboration",
    "section": "",
    "text": "Code \nThe goal of orcas is to scrape orca sighting data from the web and visualize it in maps and tables.\nI’ve always had an affinity for the Southern Resident Killer Whales in the Salish Sea. The Center for Whale Research does a lot of really fascinating and important work monitoring their population. They post their survey data on their website; each encounter with the orcas is a separate webpage. I was both curious and intimidated by web scraping so I decided this would make a great case study and personal project. I also learned how to use custom icons in leaflet maps! 🐋"
  },
  {
    "objectID": "llms/llm_noreden/index.html",
    "href": "llms/llm_noreden/index.html",
    "title": "Chatbot Design using Retrieval‑Augmented Generation (RAG)",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "drl/washi.html",
    "href": "drl/washi.html",
    "title": "Motion Prediction and Detection for Autonomous Vehicles",
    "section": "",
    "text": "Google Colab  YOLOv5 Code  PyTorch Hub \nInspired by other open-source deep learning projects such as YOLO, ResNet, and Code, this project demonstrates object detection and motion prediction for autonomous vehicles using the Kaggle Lyft motion prediction dataset. We use YOLOv5 for real-time object detection and ResNet-50 for motion trajectory prediction."
  },
  {
    "objectID": "drl/robo/washi.html",
    "href": "drl/robo/washi.html",
    "title": "Survival Of Ventilated and Control Flies",
    "section": "",
    "text": "{pkgdown} site  Code  CRAN \nInspired by other branding R packages such as Code, Code, and Code, washi provides color palettes and themes consistent with Washington Soil Health Initiative (WaSHI) branding. This package is to be used only by direct collaborators within WaSHI, though you are welcome to adapt the package to suit your own organization’s branding."
  },
  {
    "objectID": "drl/orcas.html",
    "href": "drl/orcas.html",
    "title": "Prompt Injection Attacks on LLM Medical Diagnosis by Symptom Elaboration",
    "section": "",
    "text": "Code \nThe goal of orcas is to scrape orca sighting data from the web and visualize it in maps and tables.\nI’ve always had an affinity for the Southern Resident Killer Whales in the Salish Sea. The Center for Whale Research does a lot of really fascinating and important work monitoring their population. They post their survey data on their website; each encounter with the orcas is a separate webpage. I was both curious and intimidated by web scraping so I decided this would make a great case study and personal project. I also learned how to use custom icons in leaflet maps! 🐋"
  },
  {
    "objectID": "drl/noreden/index.html",
    "href": "drl/noreden/index.html",
    "title": "Noreden",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "drl/llm_noreden/index.html",
    "href": "drl/llm_noreden/index.html",
    "title": "Chatbot Design using Retrieval‑Augmented Generation (RAG)",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "DRL",
    "section": "",
    "text": "Doing real-world projects is, I think, the best way to learn and also to engage the world and find out what the world is all about.\n\n-Ray Kurzweil\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaboration and Competition\n\n\nTrain a pair of agents to play tennis.\n\n\n\n\n\n\n\n\n\n\n\n\nAI-Powered Patient Education System\n\n\nDevelop an AI agent to enhance patient education by delivering personalized, on-demand health information through summaries, comprehension checks, and quizzes about relevant…\n\n\n\n\n\n\n\n\n\n\n\n\nChatbot Design using Retrieval‑Augmented Generation (RAG)\n\n\nBuilt a domain‑specific chatbot integrating vector‑based retrieval with GPT models to provide accurate, context‑aware responses\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Agent Travel Assistant System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\n\n\n\n\n\nCongressional Policy Analysis using ML and HPCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt Injection Attacks on LLM Medical Diagnosis by Symptom Elaboration\n\n\n\nR package\n\n\n\nAttacker is to trick the LLM to generate inappropriate possible medical diagnosis which could mislead the end use\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Of Ventilated and Control Flies\n\n\n\nR package\n\n\n\nHypothesis Analysis of Life Expectancy of Flies in Normal vs ill Ventilated Bottles\n\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification of Foods Based on their Quality\n\n\n\nArcGIS\n\nPython\n\n\n\nML model to assess the quality of fruit from an data set, which could be integrated into a product for use in home kitchens\n\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nP2P File Sharing Protocol\n\n\n\nShiny app\n\n\n\nBuild a peer-to-peer file sharing protocol that keeps track of which peers are sharing and what files are being shared in the network\n\n\n\nJan 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotion Prediction and Detection for Autonomous Vehicles\n\n\n\nAutonomous Systems\n\nDeep Learning\n\nComputer Vision\n\n\n\nDevelop a framework for vehicle detection and motion planning of vehicles in complex driving scenarios\n\n\n\nDec 7, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drl/ehr-title/index.html",
    "href": "drl/ehr-title/index.html",
    "title": "AI-Powered Patient Education System",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr"
  },
  {
    "objectID": "drl/llm_nor_mortality/index.html",
    "href": "drl/llm_nor_mortality/index.html",
    "title": "Multi-Agent Travel Assistant System",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "drl/llm_nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "drl/llm_nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Multi-Agent Travel Assistant System",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "drl/llm_nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "drl/llm_nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Multi-Agent Travel Assistant System",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "drl/llm_nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "drl/llm_nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Multi-Agent Travel Assistant System",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "drl/nor_mortality/index.html",
    "href": "drl/nor_mortality/index.html",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "drl/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "drl/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "drl/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "drl/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "drl/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "drl/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Congressional Policy Analysis using ML and HPCA",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "drl/roadmap.html",
    "href": "drl/roadmap.html",
    "title": "Classification of Foods Based on their Quality",
    "section": "",
    "text": "App  Code \nThe Soil Health Roadmap is a science-based guide to maintaining and improving soil health in eight focus areas across Washington state.\nFor each focus area, I created crop and soil maps using the arcpy Python package. I then summarized the crop acreage and soil properties in interactive ArcGIS dashboards to complement the Paper."
  },
  {
    "objectID": "drl/wacse.html",
    "href": "drl/wacse.html",
    "title": "P2P File Sharing Protocol",
    "section": "",
    "text": "App  Code \nThe Washington State Department of Agriculture developed WaCSE for the Washington State Conservation Commission to use in the Sustainable Farms and Fields (SFF) program. Intended users are the Conservation Commission, conservation districts, growers, and anyone interested in reducing agricultural greenhouse gas (GHG) emissions. This interactive tool estimates the reduction of GHG emissions from different conservation practices across Washington’s diverse counties."
  },
  {
    "objectID": "llms/llm_nor_mortality/index.html",
    "href": "llms/llm_nor_mortality/index.html",
    "title": "Multi-Agent Travel Assistant System",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "llms/llm_nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "llms/llm_nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Multi-Agent Travel Assistant System",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "llms/llm_nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "llms/llm_nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Multi-Agent Travel Assistant System",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "llms/llm_nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "llms/llm_nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Multi-Agent Travel Assistant System",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "llms/roadmap.html",
    "href": "llms/roadmap.html",
    "title": "Classification of Foods Based on their Quality",
    "section": "",
    "text": "App  Code \nThe Soil Health Roadmap is a science-based guide to maintaining and improving soil health in eight focus areas across Washington state.\nFor each focus area, I created crop and soil maps using the arcpy Python package. I then summarized the crop acreage and soil properties in interactive ArcGIS dashboards to complement the Paper."
  },
  {
    "objectID": "llms/wacse.html",
    "href": "llms/wacse.html",
    "title": "P2P File Sharing Protocol",
    "section": "",
    "text": "App  Code \nThe Washington State Department of Agriculture developed WaCSE for the Washington State Conservation Commission to use in the Sustainable Farms and Fields (SFF) program. Intended users are the Conservation Commission, conservation districts, growers, and anyone interested in reducing agricultural greenhouse gas (GHG) emissions. This interactive tool estimates the reduction of GHG emissions from different conservation practices across Washington’s diverse counties."
  },
  {
    "objectID": "robo/llm_noreden/index.html",
    "href": "robo/llm_noreden/index.html",
    "title": "Chatbot Design using Retrieval‑Augmented Generation (RAG)",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "robo/noreden/index.html",
    "href": "robo/noreden/index.html",
    "title": "Noreden",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "robo/orcas.html",
    "href": "robo/orcas.html",
    "title": "Prompt Injection Attacks on LLM Medical Diagnosis by Symptom Elaboration",
    "section": "",
    "text": "Code \nThe goal of orcas is to scrape orca sighting data from the web and visualize it in maps and tables.\nI’ve always had an affinity for the Southern Resident Killer Whales in the Salish Sea. The Center for Whale Research does a lot of really fascinating and important work monitoring their population. They post their survey data on their website; each encounter with the orcas is a separate webpage. I was both curious and intimidated by web scraping so I decided this would make a great case study and personal project. I also learned how to use custom icons in leaflet maps! 🐋"
  },
  {
    "objectID": "robo/robo/washi.html",
    "href": "robo/robo/washi.html",
    "title": "Survival Of Ventilated and Control Flies",
    "section": "",
    "text": "{pkgdown} site  Code  CRAN \nInspired by other branding R packages such as Code, Code, and Code, washi provides color palettes and themes consistent with Washington Soil Health Initiative (WaSHI) branding. This package is to be used only by direct collaborators within WaSHI, though you are welcome to adapt the package to suit your own organization’s branding."
  },
  {
    "objectID": "robo/washi.html",
    "href": "robo/washi.html",
    "title": "Motion Prediction and Detection for Autonomous Vehicles",
    "section": "",
    "text": "Google Colab  YOLOv5 Code  PyTorch Hub \nInspired by other open-source deep learning projects such as YOLO, ResNet, and Code, this project demonstrates object detection and motion prediction for autonomous vehicles using the Kaggle Lyft motion prediction dataset. We use YOLOv5 for real-time object detection and ResNet-50 for motion trajectory prediction."
  },
  {
    "objectID": "rpkg/drl_vvc/index.html",
    "href": "rpkg/drl_vvc/index.html",
    "title": "DRL for Smart Energy Systems",
    "section": "",
    "text": "Anxperiments\nCode"
  },
  {
    "objectID": "rpkg/sim2real/index.html",
    "href": "rpkg/sim2real/index.html",
    "title": "csalert",
    "section": "",
    "text": "https://github.com/csids/csalert"
  },
  {
    "objectID": "rpkg/physics-informed-actor-critic/index.html",
    "href": "rpkg/physics-informed-actor-critic/index.html",
    "title": "Physics Informed Actor Critic",
    "section": "",
    "text": "https://github.com/"
  },
  {
    "objectID": "rpkg/safe/index.html",
    "href": "rpkg/safe/index.html",
    "title": "Safe DRL",
    "section": "",
    "text": "This oding.\n\nPlanned content:\n\nE\n2\nEu"
  },
  {
    "objectID": "rpkg/uncertainty/index.html",
    "href": "rpkg/uncertainty/index.html",
    "title": "uncertainity",
    "section": "",
    "text": "cstime provides convenient and consistent conversion between\n\nisoyear\nisoweek\ncalyear\nseason week (u)"
  },
  {
    "objectID": "rpkg/meta_rl/index.html",
    "href": "rpkg/meta_rl/index.html",
    "title": "csdata",
    "section": "",
    "text": "https://github.com/csids/csdata"
  },
  {
    "objectID": "rpkg/domain_adaptation/index.html",
    "href": "rpkg/domain_adaptation/index.html",
    "title": "Domain Adaptation",
    "section": "",
    "text": "https://github.com/csids/csmaps"
  },
  {
    "objectID": "rpkg/carla/index.html",
    "href": "rpkg/carla/index.html",
    "title": "Carla",
    "section": "",
    "text": "This package provides discovery."
  },
  {
    "objectID": "rpkg/llm_control/index.html",
    "href": "rpkg/llm_control/index.html",
    "title": "nowcast",
    "section": "",
    "text": "https://github.com/csids/nowcast"
  },
  {
    "objectID": "rpkg/multimodal/index.html",
    "href": "rpkg/multimodal/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr"
  },
  {
    "objectID": "publications/articles/pesgm2025.html#ami-semi-supervised-learning-framework",
    "href": "publications/articles/pesgm2025.html#ami-semi-supervised-learning-framework",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "AMI Semi-Supervised Learning Framework",
    "text": "AMI Semi-Supervised Learning Framework\nWe formulate SSL as a regularized optimization problem:Equation 1\n\\[\n\\min_{f \\in \\mathcal{F}} \\left[ \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\ell(f(x_i), y_i) + \\lambda R_u(f, \\mathcal{D}_U) \\right]\n\\tag{1}\\]\nWhere:\n\n( (, ) ): Supervised loss (cross-entropy)\n\n( R_u(f, _U) ): Unsupervised regularization term\n\n( ): Trade-off parameter\n\nThe challenge is designing ( R_u(f, _U) ) to effectively leverage unlabeled data.\n\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  (\\#eq:binom)\n\\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation @ref(eq:binom).\nWe formulate SSL as a regularized optimization problem:\n\\[\n\\min_{f \\in \\mathcal{F}} \\left[ \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\ell(f(x_i), y_i) + \\lambda R_u(f, \\mathcal{D}_U) \\right]\n\\] Where \\[a^2 + b^2 = d^2\\] is the Unsupervised regularization term\nWhere: \\[\nR_u(f, \\mathcal{D}_U)\n\\] is the - ( (, ) ): Supervised loss (cross-entropy)\n- ( R_u(f, _U) ): Unsupervised regularization term\n- ( ): Trade-off parameter\nThe challenge is designing ( R_u(f, _U) ) to effectively leverage unlabeled data.\n\n\\[\\ell(\\cdot, \\cdot)\\]: Supervised loss (cross-entropy)\n\\[R_u(f, \\mathcal{D}_U)\\]: Unsupervised regularization term\n\\[\\lambda\\]: Trade-off parameter\n( (, ) ): Supervised loss (cross-entropy)\n\n( R_u(f, _U) ): Unsupervised regularization term\n\n( ): Trade-off parameter\n\nWe formulate SSL as a regularized optimization problem: min_{f∈ℱ} [ (1/n_L) ∑^{n_L}_{i=1} ℓ(f(x_i), y_i) + λR_u(f, 𝒟_U) ] Where:\nℓ(·,·): Supervised loss (cross-entropy) R_u(f, 𝒟_U): Unsupervised regularization term λ: Trade-off parameter\nThe challenge: designing R_u(f, 𝒟_U) that effectively exploits unlabeled data structure ## Methodology\nWe define the binomial distribution as:(Equation 2)\n\\[\nf(k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\tag{2}\\]\nAs shown in Equation @ref(eq:binom), the binomial function defines the probability…\nBlack-Scholes (Equation 3) is a mathematical model that seeks to explain the behavior of financial derivatives, most commonly options:\n\\[\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm S^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C\n\\tag{3}\\]\n\n\nProposed SSL Framework Applied to AMI Data\n\n\n\nDistribution Feeder Topology\n\n\n\nTraining and Testing Data Partitions\n\n\n\nAccuracy Comparison of SSL Methods\n\n\n\n\n\n\nPresentation\n\n\n\n\n View Fullscreen Poster"
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#overview",
    "href": "blog/technotes_20250925_research_guide/index.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "Overview",
    "text": "Overview\nThis guide provides a practical framework to prepare for Research Scientist roles in academia, industry research labs (FAANG, OpenAI, DeepMind, Anthropic, NVIDIA), and national labs. It blends personal experience with hiring-manager expectations and common evaluation rubrics.\nGitHub repository: Research Scientist Preparation Guide\nData Science Notes: Data Science Intro\nStatistical Learning Notes: Statistical Analysis LLM : Statistical Analysis —"
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#what-interviewers-evaluate",
    "href": "blog/technotes_20250925_research_guide/index.html#what-interviewers-evaluate",
    "title": "Research Scientist Interview Guide",
    "section": "What interviewers evaluate",
    "text": "What interviewers evaluate\n\nResearch impact: novelty, citations/adoption, reproducibility, clarity of problem–method–evidence chain.\nTechnical depth: math/ML fundamentals, experimental rigor, ablation thinking, error analysis.\nSystems sense: how ideas become products—data, infra, metrics, reliability, safety & ethics.\nExecution: scope → plan → iterate → deliver (papers, open-source, patents, internal wins).\nCommunication & collaboration: explain complex work to varied audiences; cross-discipline work.\nCulture/LP fit: ownership, bias for action, frugality, customer/impact obsession.\n\n\n\n\n\n\n\nTip\n\n\n\nBe explicit about your contribution. For each project: problem → gap → idea → method → evidence → limitations → next steps → impact."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#interview-process-at-a-glance",
    "href": "blog/technotes_20250925_research_guide/index.html#interview-process-at-a-glance",
    "title": "Research Scientist Interview Guide",
    "section": "Interview process at a glance",
    "text": "Interview process at a glance\nTypical stages\n1) Recruiter + HM intro → 2) Tech/Research screens (coding, ML/math, paper deep dive) →\n3) Onsite: research talk, systems/experimentation design, cross-functional, behavioral/bar raiser →\n4) Debrief → offer.\n(Use the SVG diagram you generated earlier or embed it with ![](research_scientist_interview_process.svg).)"
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#preparation-timeline-68-weeks",
    "href": "blog/technotes_20250925_research_guide/index.html#preparation-timeline-68-weeks",
    "title": "Research Scientist Interview Guide",
    "section": "Preparation timeline (6–8 weeks)",
    "text": "Preparation timeline (6–8 weeks)\nWeeks 1–2: Foundations & portfolio - Curate 2–3 flagship projects; write 1-page project briefs (problem, novelty, 3 results, open questions). - Refresh ML math: gradients, likelihoods, bias–variance, generalization, off-policy vs on-policy RL. - DS&A 20–30 mins/day (arrays, hash maps, trees, graphs, DP—medium level). - Draft talk outline; collect figures; start a reproducible repo.\nWeeks 3–4: Research talk + deep dives - Build slides (30/45/60 min versions). Timebox: Motivation 10% → Method 35% → Evidence 40% → Limits + Roadmap 15%. - Prepare ablation stories and negative results; design a live error analysis demo if feasible. - Mock talks with labmates; iterate twice.\nWeeks 5–6: Systems & coding polish - 5 case studies: online inference, data pipelines, eval at scale, safety/guardrails, monitoring. - Practice 6–8 coding problems in 60-min sessions; review idioms (two-pointers, heap, BFS/DFS, topo sort). - Draft answers for 8–10 behavioral prompts using STAR(L).\nWeek 7+: Company-specific tuning - Read team papers/repos; align your roadmap slide to their charter. - Prepare 8–12 questions to ask (below). - Dry run full onsite (talk + 3 interviews + behavioral) in a single sitting."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#research-portfolio-deep-dive",
    "href": "blog/technotes_20250925_research_guide/index.html#research-portfolio-deep-dive",
    "title": "Research Scientist Interview Guide",
    "section": "Research portfolio deep dive",
    "text": "Research portfolio deep dive\nFor each project, be ready to answer: - Gap: What prior SOTA did not address? Why now? - Assumptions: Distributional, structural, or operational assumptions—how validated? - Method: Key design choices (loss, architecture, training regime, priors/constraints). - Evidence: Metrics that matter (with CIs); strongest ablation; hardest failure case. - Impact: External adoption, dataset/code release, internal KPI movement, patents. - Next: What would you do with 3 months & a small team?\nArtifacts checklist - 10–12 figure slide deck (vector PDFs), 1-page PDF overview, GitHub README with quickstart, repro seed + script."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#technical-machine-learning-knowledge-what-to-refresh",
    "href": "blog/technotes_20250925_research_guide/index.html#technical-machine-learning-knowledge-what-to-refresh",
    "title": "Research Scientist Interview Guide",
    "section": "Technical machine learning knowledge (what to refresh)",
    "text": "Technical machine learning knowledge (what to refresh)\n\nRL: policy gradient theorem; advantage estimation; PPO/TRPO constraints; off-policy (DQN/TD3/SAC); safe RL & constraints; exploration vs exploitation; eval instability & seeding.\nDeep learning: optimization (warmup, cosine decay, AdamW), regularization (dropout, mixup, label-smoothing), attention/transformers, LoRA/parameter-efficient finetuning.\nStatistics & probabilistic modeling: MLE/MAP; conjugacy; posterior predictive; calibration (ECE), uncertainty (epistemic vs aleatoric); A/B testing pitfalls.\nGenerative models: diffusion schedule & guidance, VAEs ELBO, GAN stability.\nLLMs: instruction tuning, RAG retrieval quality, eval (exact match, nDCG, win-rates), toxicity & safety filters, hallucination mitigation.\nVision/multimodal: contrastive learning, detection/segmentation metrics (mAP, IoU), data augmentations."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#ml-systems-experimentation-design",
    "href": "blog/technotes_20250925_research_guide/index.html#ml-systems-experimentation-design",
    "title": "Research Scientist Interview Guide",
    "section": "ML systems & experimentation design",
    "text": "ML systems & experimentation design\n5-step template 1. Clarify goal (user KPI ↔︎ technical metric; online vs offline).\n2. Data (sources, labeling strategy, noise, sampling, privacy).\n3. Model & infra (baseline → candidate → serving path; latency, cost, reliability).\n4. Evaluation (offline metrics + counterfactual replays + online guardrails; slicing).\n5. Risk & safety (bias, misuse, red-teaming, rollback plan, observability).\nExample prompt (outline answer)\n“Design a near real-time anomaly detector for a power grid substation.”\n- KPI: reduce outage MTTR by 20%; constraints: &lt;200 ms latency, 99.9% uptime.\n- Data: PMU/SCADA streams; label via weak supervision + operator tags.\n- Baseline: statistical thresholds; Model: streaming autoencoder + EWMA residuals.\n- Eval: ROC-AUC offline, time-to-detect, false alarms/day; shadow deploy → phased rollout.\n- Safety: fail-open; human-in-the-loop; drift detector; incident playbook."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#coding-algorithmic-skills",
    "href": "blog/technotes_20250925_research_guide/index.html#coding-algorithmic-skills",
    "title": "Research Scientist Interview Guide",
    "section": "Coding & algorithmic skills",
    "text": "Coding & algorithmic skills\n\nAim for clean, correct, then optimal. Speak invariants, test cases, and complexity out loud.\nPatterns to practice: two-pointers, sliding window, monotonic stack, BFS/DFS, topological sort, binary search on answer, Dijkstra/Union-Find, prefix sums, DP on sequences/trees.\nML-adjacent coding: vectorized NumPy, PyTorch modules/forward pass, dataloaders, batching, mixed precision, sanity checks."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#research-talk-structure-slides",
    "href": "blog/technotes_20250925_research_guide/index.html#research-talk-structure-slides",
    "title": "Research Scientist Interview Guide",
    "section": "Research talk: structure & slides",
    "text": "Research talk: structure & slides\nSlide budget (45 min talk + Q&A) - Title & takeaway (1), Motivation (2), Problem/Gap (2), Method (5), Results (6), Ablations (3), Error analysis (2), Limits (1), Roadmap/fit (2).\nDos - One idea per slide; consistent color for your method; readable axes; include n and CI.\n- Put the thesis of each slide in the title: “Physics constraints cut violations by 38% at same cost.”\nDon’ts - Crowded plots, cherry-picked examples, unanchored qualitative claims, tiny captions."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#behavioral-star-research-flavors",
    "href": "blog/technotes_20250925_research_guide/index.html#behavioral-star-research-flavors",
    "title": "Research Scientist Interview Guide",
    "section": "Behavioral: STAR + research flavors",
    "text": "Behavioral: STAR + research flavors\nUse STAR: Situation, Task, Action, Result, Learning.\nPrepare 6 stories: conflict, failure, leadership without authority, speed vs quality, mentoring, cross-team project.\nExample prompt\n“Tell me about a time your experiment invalidated a roadmap item.”\n- S/T: critical Q3 milestone hinged on SOTA surpassing baseline.\n- A: pre-registered analysis; ran holdout; flagged negative lift; proposed minimal viable alternative.\n- R: saved ~6 wks eng time; reallocated to data quality; shipped smaller win.\n- L: add “early stop” gates; improved pre-mortem checklist."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#recommended-resources",
    "href": "blog/technotes_20250925_research_guide/index.html#recommended-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended resources",
    "text": "Recommended resources\n\nPapers: recent NeurIPS/ICLR/ICML tracks relevant to the team; read 2–3 team papers.\n\nBooks: Designing Machine Learning Systems (Huyen), Deep Learning (Goodfellow), ESL (HTF), Probabilistic ML (Barber/Murphy).\n\nPractice: LeetCode medium sets; pair-program ML design prompts; mock talks."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#example-technical-research-questions",
    "href": "blog/technotes_20250925_research_guide/index.html#example-technical-research-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example technical & research questions",
    "text": "Example technical & research questions\n\nSummarize the core contribution of your latest paper in two sentences.\n\nWhich ablation most strongly supports your claim? Which one failed and why?\n\nHow would you adapt your method under 10× less data? Under severe shift?\n\nWhat is your evaluation blind spot today? How would you close it?\n\nExplain epistemic vs. aleatoric uncertainty with a concrete modeling choice.\n\nFor PPO, where does instability come from and how do you diagnose it?\n\nDesign a reliable RAG system for safety-critical queries—retrieval, scoring, guardrails, and evaluation."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#ask-the-interviewer",
    "href": "blog/technotes_20250925_research_guide/index.html#ask-the-interviewer",
    "title": "Research Scientist Interview Guide",
    "section": "“Ask the interviewer”",
    "text": "“Ask the interviewer”\n\nUse the set that best fits the team. If time is short, ask the Top 3 in each block.\n\n\nHiring Manager (core, any research team)\n\nTop 3\n\nWhat research bets matter most in the next 6–12 months, and how will you measure success?\nWhat makes a “hell-yes” hire here after 90 days? What work would signal that?\nHow do ideas transition from a paper/prototype to production or a public result?\n\nHow is impact recognized—publications, product metrics, patents, internal adoption?\nWhat are examples of projects that didn’t land? Why, and what changed afterward?\nWhere are the biggest data/infra bottlenecks that a new scientist can unlock?\n\n\n\n\nResearch Scientists (peer scientists)\n\nTop 3\n\nWhich canonical datasets/eval harnesses are used for your area? How are baselines enforced?\nWhat’s a recent ablation or negative result that changed your roadmap?\nHow do you share/replicate experiments (internal tooling, seeds, result store)?\n\nHow are collaborations formed across teams? Any “platform” teams I should align with?\nWhat’s the cadence for paper reviews, reading groups, and internal talks?\n\n\n\n\nEngineers / MLEs (production & infra)\n\nTop 3\n\nWhat are the serving constraints (latency, throughput, cost) and reliability SLOs?\nWhat’s the path from notebook → feature store → online eval → rollout/rollback?\nHow do you monitor drift and failures in the wild? What’s the on-call/ownership model?\n\nWhat’s the CI/CD story for models (gating tests, shadow, canary, A/B infra)?\nWhat would you change in our current stack if you could?\n\n\n\n\nPM / Cross-functional (applied impact)\n\nTop 3\n\nWhich decision or workflow actually changes if this model improves by X%?\nWhat is the single metric you’d show leadership to justify continued investment?\nWhat risks (safety, bias, misuse) keep you up at night for this application?\n\nWhere does data come from and how does quality/coverage limit the roadmap?\n\n\n\n\nRecruiter / Compensation\n\nLevel targeting and calibration—what evidence best demonstrates readiness for this level?\nPublication & open-source policy (authors, timing, preprints), conference travel norms.\nVisa/relocation timeline; expected hire start window and interview re-try policy."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#example-domain-specific",
    "href": "blog/technotes_20250925_research_guide/index.html#example-domain-specific",
    "title": "Research Scientist Interview Guide",
    "section": "Example Domain-specific",
    "text": "Example Domain-specific\n\nIf the team is RL / Control (energy, robotics, autonomy)\n\nWhat control horizon & loop latency are assumed (e.g., 200 ms, 1 s)? Any hard real-time constraints?\nHow are safety constraints enforced (e.g., physics-informed losses, shielded RL, reachability)?\nWhat are the reference environments (e.g., CityLearn/PowerGym, IEEE 13/34/123 bus, HIL)?\nHow do sim-to-real gaps show up, and how do you mitigate them (domain randomization, calibration)?\nWhich offline evaluation and counterfactual replay methods are trusted before online trials?\nWhat’s the bar for replacing a heuristic/OPF with an RL policy (guardrails, rollback, audits)?\n\n\n\nIf the team is LLM / RAG / GenAI\n\nRetrieval stack: index type, chunking, rerankers, eval (nDCG, recall@k, answer faithfulness).\nHallucination budget & safety: filters/guardrails, red-teaming, and incident handling.\nFinetuning strategy: SFT/LoRA vs. prompt-only; distillation plans; model/versioning policy.\nWhat constitutes “win” in offline eval vs. human eval? How do you resolve conflicts?\nData governance: PII/PHI handling, dedup, license compliance, auto-eval for drift.\n\n\n\nIf the team is Vision / Multimodal\n\nCanonical datasets & metrics (COCO mAP, IoU, retrieval R@k); how are domain shifts handled?\nLabeling strategy & quality control; synthetic data or augmentation policies.\nDeployment constraints: throughput on edge vs. server; quantization/compilation toolchain.\nFailure modes that matter most (false positives/negatives, OOD, adversarial artifacts).\n\n\n\n\nIf you only have time for 3 questions (universal)\n\nStrategy: What is the one result you’d want me to deliver in 6 months that proves this hire was a “yes”?\n\nExecution: What is the critical bottleneck (data, infra, evaluation) preventing faster progress today?\n\nFit: Which strengths would make me the complement to the current team’s skills?"
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#checklists",
    "href": "blog/technotes_20250925_research_guide/index.html#checklists",
    "title": "Research Scientist Interview Guide",
    "section": "Checklists",
    "text": "Checklists\nDay-before technical Interview\n\nTalk readiness: 30/45/60-min versions; 1-sentence thesis; clearly mark your contributions.\nAblations & limits: 1 strongest ablation you can defend; 1 negative result and what it taught you.\nPaper deep dives: be ready to derive the key equation/algorithm; compare to 2 baselines with numbers.\nMath/ML refresh: RL (policy gradient, GAE, PPO/TRPO intuition), DL (optimizers, regularization), stats (MLE vs MAP, bias–variance, eval metrics).\nCoding drills: 2 timed mediums using core patterns (two-pointers, BFS/DFS, heap, topo sort, DP); practice test-first pseudocode.\nSystems prompts: outline 3 cases (data → model → eval → guardrails → rollout) you can walk through crisply.\nQ&A bank: 10 answers you can deliver fast (assumptions, failure modes, robustness, scalability, safety).\n\nDay-of Interview\n\nOpen strong: restate problem + constraints; define the success metric before proposing solutions.\nReasoning first: outline 2–3 approaches; justify trade-offs; state target time/space complexity.\nCoding round: implement incrementally; speak invariants and edge cases; analyze complexity after passing tests.\nResearch talk: show the central figure; defend an ablation; admit a limitation and the next experiment.\nDesign/experimentation: baseline → candidate → eval plan; define slices; propose risks & rollback.\nClose each round: 30–60s recap with decision, trade-offs, and “next step” you’d run."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#offer-debrief-negotiation",
    "href": "blog/technotes_20250925_research_guide/index.html#offer-debrief-negotiation",
    "title": "Research Scientist Interview Guide",
    "section": "Offer, Debrief & Negotiation",
    "text": "Offer, Debrief & Negotiation\n1) Right after the loop\n\nKeep a short brag doc: 5–8 bullets tying your work to measurable impact (citations, benchmarks, improvements).\nSend thank-you notes; ask the recruiter for decision timeline and whether the team needs any follow-ups (extra slides, code pointers).\n\n2) Debrief (if you don’t get detailed feedback)\n\nAsk for 3 bullets: (i) strengths that stood out, (ii) top concern, (iii) what would change the decision next time.\nIf there’s a miscalibration (e.g., level/scope), propose a targeted follow-up (short tech screen or focused deep dive).\n\n3) Offer review (when it comes)\n\nBreak it down: base + bonus + equity/RSUs (grant, vesting, refreshers) + sign-on + extras (compute budget, conference travel, publication policy, patent bonus).\nClarify: level, title, team mandate, location policy, start date, performance review cycles, and conference/OSS policies.\n\n4) Negotiation (impact-first)\n\nAnchor with scope & impact (what you can deliver in 6–12 months), plus comparables (peer offers or market data for the same level/geo).\nPrioritize asks (pick 2–3): sign-on / equity / level / research budget / conf travel.\n\nSample script:\n“Given the scope (X) and the impact I’m positioned to deliver (Y), I’m targeting total comp of Z at level L. If level is fixed, increasing equity by A or adding a B sign-on would bridge the gap. I’d also value C (e.g., conference travel commitment).”\n\n5) Timelines\n\nIf you need time: “I’m very excited. To make a well-considered decision, could we set a reply date of ? I want to complete one pending loop and compare scopes fairly.”\nIf there’s an exploding deadline, ask for a short extension in exchange for a firm decision date and clear intent.\n\n6) If rejected\n\nRequest specific growth areas and ask whether a re-interview window (e.g., 6 months) with a targeted bar (e.g., systems/experiments) is possible."
  },
  {
    "objectID": "blog/technotes_20250925_research_guide/index.html#mentorship",
    "href": "blog/technotes_20250925_research_guide/index.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’d like feedback on your talk, paper deep dive, or a full mock onsite, reach me at cs.kundann@gmail.com."
  },
  {
    "objectID": "publications/articles/pesgm2025.html#semi-supervised-learning-framework-for-ami",
    "href": "publications/articles/pesgm2025.html#semi-supervised-learning-framework-for-ami",
    "title": "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems",
    "section": "Semi-Supervised Learning Framework for AMI",
    "text": "Semi-Supervised Learning Framework for AMI\nWe formulate SSL as a regularized optimization problem:Equation 1\n\\[\n\\min_{f \\in \\mathcal{F}} \\left[ \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\ell(f(x_i), y_i) + \\lambda R_u(f, \\mathcal{D}_U) \\right]\n\\tag{1}\\]\nWhere:\n\n( (, ) ): Supervised loss (cross-entropy)\n\n( R_u(f, _U) ): Unsupervised regularization term\n\n( ): Trade-off parameter\n\nThe challenge is designing ( R_u(f, _U) ) to effectively leverage unlabeled data.\n\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  (\\#eq:binom)\n\\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation @ref(eq:binom).\nWe formulate SSL as a regularized optimization problem:\n\\[\n\\min_{f \\in \\mathcal{F}} \\left[ \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\ell(f(x_i), y_i) + \\lambda R_u(f, \\mathcal{D}_U) \\right]\n\\] Where \\[a^2 + b^2 = d^2\\] is the Unsupervised regularization term\nWhere: \\[\nR_u(f, \\mathcal{D}_U)\n\\] is the - ( (, ) ): Supervised loss (cross-entropy)\n- ( R_u(f, _U) ): Unsupervised regularization term\n- ( ): Trade-off parameter\nThe challenge is designing ( R_u(f, _U) ) to effectively leverage unlabeled data.\n\n\\[\\ell(\\cdot, \\cdot)\\]: Supervised loss (cross-entropy)\n\\[R_u(f, \\mathcal{D}_U)\\]: Unsupervised regularization term\n\\[\\lambda\\]: Trade-off parameter\n( (, ) ): Supervised loss (cross-entropy)\n\n( R_u(f, _U) ): Unsupervised regularization term\n\n( ): Trade-off parameter\n\nWe formulate SSL as a regularized optimization problem: min_{f∈ℱ} [ (1/n_L) ∑^{n_L}_{i=1} ℓ(f(x_i), y_i) + λR_u(f, 𝒟_U) ] Where:\nℓ(·,·): Supervised loss (cross-entropy) R_u(f, 𝒟_U): Unsupervised regularization term λ: Trade-off parameter\nThe challenge: designing R_u(f, 𝒟_U) that effectively exploits unlabeled data structure ## Methodology\nWe define the binomial distribution as:(Equation 2)\n\\[\nf(k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\tag{2}\\]\nAs shown in Equation @ref(eq:binom), the binomial function defines the probability…\nBlack-Scholes (Equation 3) is a mathematical model that seeks to explain the behavior of financial derivatives, most commonly options:\n\\[\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm S^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C\n\\tag{3}\\]\n\n\nProposed SSL Framework Applied to AMI Data\n\n\n\nDistribution Feeder Topology\n\n\n\nTraining and Testing Data Partitions\n\n\n\nAccuracy Comparison of SSL Methods\n\n\n\n\n\n\nPresentation\n\n\n\n\n View Fullscreen Poster"
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#data-level-approaches-resampling-techniques",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#data-level-approaches-resampling-techniques",
    "title": "How to handle class-unbalanced data?",
    "section": "Data-Level Approaches (Resampling Techniques)",
    "text": "Data-Level Approaches (Resampling Techniques)\n\nOversampling the minority class\n\nRandom Oversampling: duplicate minority class examples until balance is reached.\nSMOTE (Synthetic Minority Over-sampling Technique): generates synthetic data points for the minority class by interpolating between nearest neighbors.\nADASYN: similar to SMOTE but focuses on generating harder-to-learn examples.\n\n\n\nUndersampling the majority class\n\nRandom Undersampling: randomly remove majority class examples.\nCluster Centroids / Tomek Links / NearMiss: more informed undersampling to preserve useful structure.\n\n\n\nHybrid methods\n\nCombine oversampling and undersampling to avoid overfitting or losing too much information.\n\nAlgorithm-Level Approaches\nClass Weights / Cost-Sensitive Learning\nAssign higher misclassification cost to minority class (many libraries like scikit-learn allow class_weight=‘balanced’).\nAnomaly Detection / One-Class Models\nTreat minority class as “rare events” and use anomaly detection approaches.\nEnsemble Techniques\nUse Bagging/Boosting with imbalance-aware modifications (e.g., Balanced Random Forest, EasyEnsemble, RUSBoost).\nEvaluation-Level Approaches Confusion Matrix: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned). Precision: A measure of a classifiers exactness. Recall: A measure of a classifiers completeness F1 Score (or F-score): A weighted average of precision and recall. Avoid accuracy as the metric (it will be misleading).\nPrefer metrics that account for imbalance:\nPrecision, Recall, F1-score\nROC-AUC, PR-AUC (especially useful with rare positives)\nMatthews Correlation Coefficient (MCC)\nBalanced Accuracy\n. Data Collection & Domain Knowledge\nGather more examples of the minority class if possible (often the best long-term solution).\nUse domain knowledge to engineer features that help separate classes better.\nActive learning: selectively label more examples from uncertain regions.\n\nAdvanced / Modern Techniques\n\nGenerative Models (GANs, VAEs): to synthesize minority samples.\nSemi-supervised or Self-supervised Learning: leverage unlabeled data to improve representation of minority class.\nFocal Loss (in deep learning): gives higher weight to hard-to-classify (often minority) samples.\nCourse link"
  }
]