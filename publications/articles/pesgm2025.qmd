---
title: "Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems"
author: 
  - name: Kundan Kumar
year: "2025"
publication: "IEEE Power & Energy Society General Meeting"
doi: ""
image: "figures/ssl_techniques.png"
toc: false
categories:
  - Bayesian Neural Network
  - Semi Supervised Learning
  - Uncertainty Estimation
---
<div style="display:flex; justify-content:flex-end; margin-top:-0.5rem; gap:0.5rem;">
  <a class="btn btn-primary" href="semi_supervised_pes_gm_2025/docs/SSL_PESGM_poster.pdf" target="_blank" rel="noopener">
    Open Poster (PDF)
  </a>
</div>

### Citation (IEEE)

> **K. Kumar**, K. Utkarsh, J. Wang, and H. V. Padullaparti, “Advanced Semi-Supervised Learning With Uncertainty Estimation for Phase Identification in Distribution Systems,” Proc. IEEE Power & Energy Society General Meeting (PESGM), 2025.


## Abstract

The integration of advanced metering infrastructure (AMI) into power distribution networks generates valuable data for tasks such as phase identification; however, the limited and unreliable availability of labeled data in the form of customer phase connectivity presents challenges. To address this issue, we propose a semi-supervised learning (SSL) bayesian framework that effectively leverages both limited labeled and unlimited unlabeled data.


1. Why Phase Identification Needs a New Approach ?

**Problem:** Utilities don’t know which phase customers are connected to this affects
voltage regulation, DER integration, and fault localization.

<table style="width:100%; border-collapse: collapse;">
  <tr>
    <td style="width:50%; text-align:center; vertical-align:top; padding-right:15px;">
      <img src="semi_supervised_pes_gm_2025/figures/ssl_techniques.png" style="width:90%; height:auto;">
      <div><strong>Fig. 1:</strong> Illustration of Semi-Supervised Learning Techniques</div>
    </td>
    <td style="width:50%; vertical-align:top; font-size:0.95em;">
      <p><strong>Challenges & Motivation</strong></p>
      <ul>
        <li><strong>Challenge:</strong> Ground truth phase data is scarce, unreliable, and costly to collect.</li>
        <li><strong>Problem:</strong> Supervised ML methods require large amounts of labeled data and often unavailable or unreliable.</li>
        <li><strong>Motivation:</strong> How do we scale phase identification without needing tons of labeled data?</li>
      </ul>
    </td>
  </tr>
</table>

<!--  <img src="semi_supervised_pes_gm_2025/figures/ssl_techniques.png" alt="Illustration of Semi-Supervised Learning Techniques" style="width: 60%; height: auto;">
  <figcaption><strong>Fig. 1:</strong> Illustration of Semi-Supervised Learning Techniques</figcaption>
</figure>


• Challenge: Ground truth phase data
is scarce, unreliable, and costly to collect.
• Supervised learning ML methods need lots of labeled data – often unavailable
or unreliable.
• Motivation: How do we scale phase
identification without needing tons of labeled
data?
<table style="width:100%; border-collapse: collapse;">
  <tr>
    <td style="width:50%; text-align:center; vertical-align:top; padding-right:15px;">
      <img src="semi_supervised_pes_gm_2025/figures/ssl_techniques.png" style="width:90%; height:auto;">
      <div><strong>Fig. 1:</strong> Illustration of Semi-Supervised Learning Techniques</div>
    </td>
    <td style="width:50%; vertical-align:top; font-size:0.95em;">
      <p><strong>Challenges & Motivation</strong></p>
      <ul>
        <li><strong>Challenge:</strong> Ground truth phase data is scarce, unreliable, and costly to collect.</li>
        <li><strong>Problem:</strong> Supervised ML methods require large amounts of labeled data – often unavailable or unreliable.</li>
        <li><strong>Motivation:</strong> How do we scale phase identification without needing tons of labeled data?</li>
      </ul>
    </td>
  </tr>
</table>

add techniques -->

## Contribution
Our approach incorporates:

- Self-training with an ensemble of multilayer perceptron classifiers.
- Label spreading to propagate labels based on data similarity.
- Bayesian Neural Networks (BNNs) for uncertainty estimation, improving confidence and reducing phase identification errors.

Key Highlights:*

- Achieved **~98% ± 0.08** accuracy on real utility data (Duquesne Light Company) using minimal and unreliable labeled data.
- Uncertainty-aware predictions reduce misclassification risk and improve smart grid reliability.
- Combines pseudo-labeling, graph-based SSL, and probabilistic modeling to handle data scarcity in real-world distribution networks.

Our "SSL + Uncertainty Estimation" approach provides an efficient and scalable solution for phase identification in AMI data, enabling utilities to improve modeling, simulation, and operational decision-making.


## 3. Problem Formulation of Framework for AMI

  We define phase identification as a **semi-supervised classification problem**,`\@ref(eq:black-scholes2)` where the dataset $D = D_L \cup D_U$ consists of a small labeled subset $D_L$ and a large unlabeled subset $D_U$.
  
  The SSL objective is a **regularized minimization**:
  
$$
  \min_{f \in \mathcal{F}} \left[ 
    \frac{1}{n_L} \sum_{i=1}^{n_L} \ell(f(x_i), y_i) 
    + \lambda R(f, \mathcal{D}_U)
  \right]
$${#eq:black-scholes2}
  
  where:  
  - $\ell$ is the supervised loss (e.g., cross-entropy)  
  - $R(f, \mathcal{D}_U)$ is the regularization term capturing structure in the unlabeled data,
  - $\lambda$ : trade-off parameter controlling the influence of unlabeled data

This formulation encourages the model to learn a decision boundary consistent with both labeled examples and the structure of the unlabeled feature space. c'


## Methodology


<!--Black-Scholes (@eq-black-scholes) is a mathematical model that seeks to explain the behavior of financial derivatives, most commonly options:

$$
\frac{\partial \mathrm C}{ \partial \mathrm t } + \frac{1}{2}\sigma^{2} \mathrm S^{2}
\frac{\partial^{2} \mathrm C}{\partial \mathrm S^2}
  + \mathrm r \mathrm S \frac{\partial \mathrm C}{\partial \mathrm S}\ =
  \mathrm r \mathrm C 
$$ {#eq-black-scholes}
-->

### 4.1 Self-Training with MLP Ensembles

The MLP classifier f(x; $\theta$) is trained on $D_L$ to minimize cross-entropy loss (@eq-self-training):

$$
\theta = \arg\min_\theta \sum_{(x_i, y_i) \in D_L} \mathcal{L}(f(x_i; \theta), y_i)
$$ {#eq-self-training}


Unlabeled samples with high prediction confidence $p_j$ > $\tau$ receive pseudo-labels:

$$
D^{\text{new}}_L = \{(x_j, \hat{y}_j) \mid p_j > \tau\}
$$

The process repeats iteratively, enriching the labeled dataset.

---

### 4.2 Label Spreading (Graph-Based SSL)

We construct a similarity matrix $W$ where edge weights encode feature similarity:

$$
W_{ij} = 
\begin{cases}
\exp\!\left(-\frac{\|x_i - x_j\|^2}{\sigma^2}\right), & i \neq j \\
0, & i = j
\end{cases}
$$

Label distributions are updated iteratively as:

$$
Y^{(t+1)} = (1 - \alpha)Y^{(t)} + \alpha D^{-1}WY^{(t)}
$$

This propagates known labels through the data manifold, smoothing class boundaries.

### 4.3 Bayesian Neural Networks (BNNs)

BNNs treat weights as random variables, assigning a Gaussian prior:

$$
p(W) = \mathcal{N}(W | \mu_{W}, \sigma_W^2)
$$

Given training data $D_L$, the posterior distribution is:

$$
p(W | D_L) \propto p(D_L | W) \, p(W)
$$

The predictive distribution integrates over all possible weight configurations:

$$
p(y^* | x^*, D_L) = \int p(y^* | x^*, W) \, p(W | D_L) \, dW
$$


We approximate this via Monte Carlo dropout by averaging multiple stochastic forward passes:

$$
\hat{y}^* = \frac{1}{N} \sum_{n=1}^N f(x^*; W_n)
$$
### 4.4 Uncertainty Quantification

Two forms of uncertainty are estimated:

1. **Epistemic (Model Uncertainty):**
$$
   U_{\text{epistemic}} = \mathrm{Var}(\hat{y}^*)
$$

2. **Aleatoric (Data Uncertainty):**
$$
   U_{\text{aleatoric}} = \mathbb{E}\!\left[(\hat{y}^* - \mathbb{E}[\hat{y}^*])^2\right]
$$

Together, they help distinguish between *what the model doesn’t know* and *what cannot be known* due to noise.

<!--![](semi_supervised_pes_gm_2025/figures/ssl_techniques.png){ width=50% }-->

## 5. Experimental Framework

<div style="text-align: center;">
  <img src="semi_supervised_pes_gm_2025/figures/proposed_framework.png" alt="SSL Framework" style="width: 70%; height: auto;">
  <div><strong>Fig. 2:</strong> Proposed SSL Framework Applied to AMI Data</div>
</div>

### Data Flow and Setup

- **Dataset Source:** Real AMI data from a U.S. utility (Duquesne Light Company).  
- **Feature Set:**  
  \( F = \{R_0, X_0, R_1, X_1, P, V_{\max}, V_{\min}, V_{\text{avg}}\} \)  
- **Data Split:** 70% development, 30% test; within development, labeled fractions vary from 5–80%.  
- **Models:**  
  - MLP (64–32 layers, ReLU activation)  
  - Label Spreading with kNN kernel  
  - 3-layer BNN using Gaussian priors, dropout rate 0.7, Adam optimizer  

<div style="text-align: center;">
  <img src="semi_supervised_pes_gm_2025/figures/feeder_topology.png" alt="Network Topology" style="width: 60%; height: auto;">
  <div><strong>Fig. 3:</strong> Distribution Feeder Topology</div>
</div>

<div style="text-align: center;">
  <img src="semi_supervised_pes_gm_2025/figures/data_split.png" alt="Data Split" style="width: 60%; height: auto;">
  <div><strong>Fig. 4:</strong> Training and Testing Data Partitions</div>
</div>

---

## 6. Results and Discussion

BNNs outperformed both self-training and label spreading across all labeled data ratios.  
When only **5% of the dataset** was labeled, BNNs already achieved **64.15% ± 0.14**, compared to 34.9% for self-training and 44.3% for label spreading.  
At **70% labeled data**, the BNN reached **99.06% ± 0.06 accuracy**.

<div style="text-align: center;">
  <img src="semi_supervised_pes_gm_2025/figures/accuracy_comparison.png" alt="Accuracy Comparison" style="width: 70%; height: auto;">
  <div><strong>Fig. 5:</strong> Comparison of SSL Algorithms with Uncertainty Estimation</div>
</div>

**Interpretation:**  
BNNs’ probabilistic nature allows them to express *how sure* they are about each decision. This prevents overfitting and enables informed decision-making when data are uncertain—crucial for utility operations.

---

## 7. Conclusion

This research presents a **semi-supervised learning framework enhanced with Bayesian uncertainty estimation** for phase identification in power distribution systems.  
By integrating *pseudo-labeling*, *graph-based label propagation*, and *Bayesian inference*, our framework achieves robust performance with minimal labeled data—**98% ± 0.08 accuracy**—and provides confidence metrics for each prediction.

This uncertainty-aware paradigm is a step toward **trustworthy, data-efficient, and intelligent smart grids**, where models not only predict but also *know when they might be wrong*.

2. **Proposed SSL Framework Applied to AMI Data**
<div style="text-align: center;">
  <img src="semi_supervised_pes_gm_2025/figures/proposed_framework.png" alt="SSL Framework" style="width: 60%; height: auto;">
</div>


3. **Distribution Feeder Topology**
<div style="text-align: center;">
  <img src="semi_supervised_pes_gm_2025/figures/feeder_topology.png" alt="Network Toplogy" style="width: 60%; height: auto;">
</div>


4. **Training and Testing Data Partitions**
<div style="text-align: center;">
  <img src="semi_supervised_pes_gm_2025/figures/data_split.png" alt="Data Split" style="width: 60%; height: auto;">
</div>


5. **Accuracy Comparison of SSL Methods**

<div style="text-align: center;">
  <img src="semi_supervised_pes_gm_2025/figures/accuracy_comparison.png" alt="Accuracy Comparison" style="width: 60%; height: auto;">
</div>


<!--
---
#### Presentation
<!--<iframe class="slides" src=""></iframe> -->
<!--
<iframe 
  class="slides" 
  src="semi_supervised_pes_gm_2025/docs/SSL_PESGM_poster.pdf" 
  width="100%" 
  height="400px" 
  style="border: none;">
</iframe>

<div style="text-align: center; margin-top: 10px;">
  <a href="semi_supervised_pes_gm_2025/docs/SSL_PESGM_poster.pdf" target="_blank" class="btn btn-primary">
    View Fullscreen Poster
  </a>
</div>
-->